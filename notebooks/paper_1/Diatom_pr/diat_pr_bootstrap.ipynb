{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap (Diatom Production Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import dill\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, name, inputs_names):\n",
    "\n",
    "    x = np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter))\n",
    "\n",
    "    inputs = []\n",
    "    \n",
    "    if inputs_names[-1] =='Day_of_year':   \n",
    "        for i in inputs_names[0:-1]:\n",
    "            inputs.append(dataset[i].to_numpy().flatten())       \n",
    "        inputs.append(np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y)))\n",
    "\n",
    "    else:        \n",
    "        for i in inputs_names:\n",
    "            inputs.append(dataset[i].to_numpy().flatten())\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    targets = np.ravel(dataset[name])\n",
    "    \n",
    "    indx = np.where(np.isfinite(targets) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,indx[0]]\n",
    "    targets = targets[indx[0]]\n",
    "\n",
    "    inputs = inputs.transpose()\n",
    "\n",
    "    return(inputs, targets, indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_preparation2(dataset,variable,indx):\n",
    "\n",
    "    variable_all = np.full((len(dataset.time_counter) * len(dataset.y) * len(dataset.x)),np.nan)\n",
    "    variable_all[indx[0]] = variable\n",
    "    variable_all = np.reshape(variable_all,(len(dataset.time_counter),len(dataset.y),len(dataset.x)))\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    array = xr.DataArray(variable_all,\n",
    "        coords = {'time_counter': dataset.time_counter,'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['time_counter','y','x'])\n",
    "        \n",
    "    return (array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, n_bins, drivers, spatial, inputs_names):\n",
    "\n",
    "    if spatial == []:\n",
    "        model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "            transformers=[('drivers', StandardScaler(), np.arange(0,len(drivers)))], remainder='passthrough'),\n",
    "            HistGradientBoostingRegressor(categorical_features=[len(drivers)])),\n",
    "            transformer=StandardScaler())\n",
    "\n",
    "    else:\n",
    "        model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "        transformers=[('drivers', StandardScaler(), np.arange(0,len(drivers))), \n",
    "            ('spatial', KBinsDiscretizer(n_bins=n_bins,encode='ordinal',strategy='quantile'), np.arange(inputs_names.index(spatial[0]),inputs_names.index(spatial[-1])+1))],\n",
    "            remainder='passthrough'),\n",
    "        HistGradientBoostingRegressor(categorical_features=np.arange(inputs_names.index(spatial[0]),len(inputs_names)))),\n",
    "        transformer=StandardScaler())\n",
    "    \n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(ax, corn, colour):\n",
    "\n",
    "    ax.plot([corn[2], corn[3], corn[3], corn[2], corn[2]], \n",
    "    [corn[0], corn[0], corn[1], corn[1], corn[0]], '-', color=colour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist (variable,name,boxnames):\n",
    "\n",
    "    fig, axs = plt.subplots(1,len(boxnames), figsize = (20,6), layout='constrained')\n",
    "\n",
    "    for j in range(0,len(boxnames)):\n",
    "\n",
    "        h = axs[j].hist(variable[:,j])\n",
    "        axs[j].set_title(boxnames[j])\n",
    "\n",
    "        axs[j].set_ylabel('Frequency')\n",
    "        fig.suptitle(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(dataset,inputs,targets,indx, dataset_test,inputs_test,indx_test, n_bins, drivers, spatial, inputs_names):\n",
    "\n",
    "    regr = regressor(inputs, targets, n_bins, drivers, spatial, inputs_names)\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "    predictions_all = datasets_preparation2(dataset,predictions,indx)\n",
    "    \n",
    "    predictions_test = regr.predict(inputs_test)\n",
    "    predictions_tests_all = datasets_preparation2(dataset_test,predictions_test,indx_test)\n",
    "\n",
    "    targets_mean = np.reshape(targets,(len(dataset.time_counter), len(indx[0]) // len(dataset.time_counter)))\n",
    "    targets_mean = np.mean(targets_mean,axis=1)\n",
    "\n",
    "    season = np.array(np.split(targets_mean,len(np.unique(dataset.time_counter.dt.year)),axis=0))\n",
    "    season = np.mean(season, axis=0)\n",
    "    \n",
    "    return(predictions,predictions_all,predictions_tests_all,season)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_train (dataset,targets_all,predictions_all,boxes,regions0,season):\n",
    "\n",
    "    r_train = np.full(len(boxes),np.nan)\n",
    "    rms_train = np.full(len(boxes),np.nan)\n",
    "    slope_train = np.full(len(boxes),np.nan)\n",
    "\n",
    "    r_train_season = np.full(len(boxes),np.nan)\n",
    "    slope_train_season = np.full(len(boxes),np.nan)\n",
    "\n",
    "    for i in range (0,len(boxes)):\n",
    "\n",
    "        targets = targets_all.where(regions0==i).mean(['y','x'])\n",
    "        predictions = predictions_all.where(regions0==i).mean(['y','x'])\n",
    "\n",
    "        r_train[i] = xr.corr(targets,predictions)\n",
    "        rms_train[i] = xs.rmse(targets,predictions,skipna=True) / np.mean(targets) * 100\n",
    "        slope_train[i] = xs.linslope(targets,predictions,skipna=True)\n",
    "\n",
    "        climatology = targets_all[:,boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]]\n",
    "        season_train = climatology.to_numpy()\n",
    "        season_train = np.reshape(season_train,(len(season),len(np.unique(dataset.time_counter.dt.year)),climatology.shape[1]*climatology.shape[2]),order='F')\n",
    "        season_train = np.nanmean(season_train,axis=(1,2))\n",
    "        season_train = np.tile(season_train,len(np.unique(dataset.time_counter.dt.year))) # Broadcasting season to all training years\n",
    "\n",
    "        r_train_season[i] = np.round(np.corrcoef(targets-season_train,predictions-season_train)[0][1],3)\n",
    "        m,_ = np.polyfit(targets-season_train,predictions-season_train, deg=1)\n",
    "        slope_train_season[i] = np.round(m,3)\n",
    "\n",
    "    return (r_train,rms_train,slope_train,r_train_season,slope_train_season)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_test (dataset_test,targets_test_all,predictions_test_all, boxes,regions0,season, dataset,targets_all):\n",
    "\n",
    "    r_test = np.full(len(boxes),np.nan)\n",
    "    rms_test = np.full(len(boxes),np.nan)\n",
    "    slope_test = np.full(len(boxes),np.nan)\n",
    "\n",
    "    r_test_season = np.full(len(boxes),np.nan)\n",
    "    slope_test_season = np.full(len(boxes),np.nan)\n",
    "\n",
    "    targets_sum = np.full((len(boxes), len(np.unique(dataset_test.time_counter.dt.year))), np.nan)\n",
    "    predictions_sum = np.full((len(boxes), len(np.unique(dataset_test.time_counter.dt.year))), np.nan)\n",
    "\n",
    "    rms_test_s = np.full(len(boxes),np.nan)\n",
    "\n",
    "    for i in range (0,len(boxes)):\n",
    "\n",
    "        targets = targets_test_all.where(regions0==i).mean(['y','x'])\n",
    "        predictions = predictions_test_all.where(regions0==i).mean(['y','x'])\n",
    "\n",
    "        r_test[i] = xr.corr(targets,predictions)\n",
    "        rms_test[i] = xs.rmse(targets,predictions,skipna=True) / np.mean(targets) * 100\n",
    "        slope_test[i] = xs.linslope(targets,predictions,skipna=True)\n",
    "\n",
    "        climatology = targets_all[:,boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]]\n",
    "        season_test = climatology.to_numpy()\n",
    "        season_test = np.reshape(season_test,(len(season),len(np.unique(dataset.time_counter.dt.year)),climatology.shape[1]*climatology.shape[2]),order='F')\n",
    "        season_test = np.nanmean(season_test,axis=(1,2))\n",
    "        season_test = np.tile(season_test,len(np.unique(dataset_test.time_counter.dt.year))) # Broadcasting season to all testing years\n",
    "\n",
    "        r_test_season[i] = np.round(np.corrcoef(targets-season_test,predictions-season_test)[0][1],3)\n",
    "        m,_ = np.polyfit(targets-season_test,predictions-season_test, deg=1)\n",
    "        slope_test_season[i] = np.round(m,3)\n",
    "\n",
    "        targets_sum[i] = (targets-season_test).groupby(targets.time_counter.dt.year).sum().values\n",
    "        predictions_sum[i] =  (predictions-season_test).groupby(predictions.time_counter.dt.year).sum().values\n",
    "\n",
    "        rms_test_s[i] = 0\n",
    "\n",
    "        for j in range (0, len(np.unique(dataset_test.time_counter.dt.year))):\n",
    "\n",
    "            rms_test_s[i] = rms_test_s[i] + (targets_sum[i,j] - predictions_sum[i,j])**2\n",
    "\n",
    "        rms_test_s[i] = np.sqrt(rms_test_s[i])/2\n",
    "\n",
    "    return (r_test,rms_test,slope_test,r_test_season,slope_test_season,rms_test_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Diatom_Production_Rate'\n",
    "units = '[mmol N m-2 s-1]'\n",
    "category = 'Production rates'\n",
    "\n",
    "filename = '/data/ibougoudis/MOAD/files/inputs/jan_mar.nc'\n",
    "\n",
    "drivers = ['Summation_of_solar_radiation', 'Mean_wind_speed']\n",
    "spatial = ['Latitude', 'Longitude']\n",
    "day_input = ['Day_of_year']\n",
    "\n",
    "inputs_names = drivers + spatial + day_input\n",
    "\n",
    "n_bins=255\n",
    "\n",
    "if filename[35:42] == 'jan_mar': # 75 days, 1st period\n",
    "    period = '(16 Jan - 31 Mar)'\n",
    "    id = '1'\n",
    "    months = ['January', 'February', 'March']\n",
    "\n",
    "elif filename[35:42] == 'jan_apr': # 120 days, 2nd period\n",
    "    period = '(01 Jan - 30 Apr)'\n",
    "    id = '2'\n",
    "    months = ['January', 'February', 'March', 'April']\n",
    "\n",
    "elif filename[35:42] == 'feb_apr': # 75 days, 3rd period\n",
    "    period = '(15 Feb - 30 Apr)'\n",
    "    id = '3'\n",
    "    months = ['February', 'March', 'April']\n",
    "\n",
    "elif filename[35:42] == 'apr_jun': # 76 days, 4th period\n",
    "    period = '(16 Apr - 30 Jun)'\n",
    "    id = '4'\n",
    "    months = ['April', 'May', 'June']\n",
    "\n",
    "elif filename[35:42] == 'may_sep': # 153 days, 5th period\n",
    "    period = '(01 May - 30 Sep)'\n",
    "    id = '5'\n",
    "    months = ['May', 'June', 'July', 'August', 'September']\n",
    "\n",
    "ds = xr.open_dataset(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 9))\n",
    "mycmap = cm.deep\n",
    "mycmap.set_bad('grey')\n",
    "ax.pcolormesh(ds[name][0], cmap=mycmap)\n",
    "sa_vi.set_aspect(ax)\n",
    "\n",
    "SoG_north = [650, 730, 100, 200]\n",
    "plot_box(ax, SoG_north, 'g')\n",
    "SoG_center = [450, 550, 200, 300]\n",
    "plot_box(ax, SoG_center, 'b')\n",
    "Fraser_plume = [380, 460, 260, 330]\n",
    "plot_box(ax, Fraser_plume, 'm')\n",
    "SoG_south = [320, 380, 280, 350]\n",
    "plot_box(ax, SoG_south, 'k')\n",
    "Haro_Boundary = [290, 350, 210, 280]\n",
    "plot_box(ax, Haro_Boundary, 'm')\n",
    "JdF_west = [250, 425, 25, 125]\n",
    "plot_box(ax, JdF_west, 'c')\n",
    "JdF_east = [200, 290, 150, 260]\n",
    "plot_box(ax, JdF_east, 'w')\n",
    "PS_all = [0, 200, 80, 320]\n",
    "plot_box(ax, PS_all, 'm')\n",
    "PS_main = [20, 150, 200, 280]\n",
    "plot_box(ax, PS_main, 'r')\n",
    "\n",
    "boxnames = ['SoG_north','SoG_center','Fraser_plume','SoG_south', 'Haro_Boundary', 'JdF_west', 'JdF_east', 'PS_all', 'PS_main']\n",
    "fig.legend(boxnames)\n",
    "\n",
    "boxes = [SoG_north,SoG_center,Fraser_plume,SoG_south,Haro_Boundary,JdF_west,JdF_east,PS_all,PS_main]\n",
    "\n",
    "regions0 = np.full((len(ds.y),len(ds.x)),np.nan)\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "    regions0[boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]] = i\n",
    "\n",
    "regions0 = xr.DataArray(regions0,dims = ['y','x'])\n",
    "\n",
    "# # Low resolution\n",
    "\n",
    "temp = []\n",
    "\n",
    "for i in boxes:\n",
    "    temp.append([x//5 for x in i])\n",
    "\n",
    "boxes = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low resolution\n",
    "\n",
    "ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "regions0 = regions0.isel(y=(np.arange(regions0.y[0], regions0.y[-1], 5)), \n",
    "    x=(np.arange(regions0.x[0], regions0.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "inputs,targets,indx = datasets_preparation(dataset, name, inputs_names)\n",
    "targets_all = datasets_preparation2(dataset,targets,indx)\n",
    "\n",
    "dataset_test = ds.sel(time_counter = slice('2021', '2024'))\n",
    "inputs_test,targets_test,indx_test = datasets_preparation(dataset_test, name, inputs_names)\n",
    "targets_test_all = datasets_preparation2(dataset_test,targets_test,indx_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_resamples = 100\n",
    "\n",
    "r_train = np.full((n_resamples+1, len(boxes)), np.nan)\n",
    "rms_train = np.full_like(r_train, np.nan)\n",
    "slope_train = np.full_like(r_train, np.nan)\n",
    "\n",
    "r_train_season = np.full_like(r_train, np.nan)\n",
    "slope_train_season = np.full_like(r_train, np.nan)\n",
    "\n",
    "r_test = np.full_like(r_train, np.nan)\n",
    "rms_test = np.full_like(r_train, np.nan)\n",
    "slope_test = np.full_like(r_train, np.nan)\n",
    "\n",
    "r_test_season = np.full_like(r_train, np.nan)\n",
    "slope_test_season = np.full_like(r_train, np.nan)\n",
    "\n",
    "rms_test_s = np.full_like(r_train, np.nan)\n",
    "\n",
    "# For the first (original) training session\n",
    "\n",
    "predictions,predictions_all,predictions_test_all,season = train_test(dataset,inputs,targets,indx, dataset_test,inputs_test,indx_test, n_bins,drivers,spatial,inputs_names)\n",
    "r_train[0],rms_train[0],slope_train[0],r_train_season[0],slope_train_season[0] = metrics_train(dataset,targets_all,predictions_all, boxes,regions0,season)\n",
    "r_test[0],rms_test[0],slope_test[0],r_test_season[0],slope_test_season[0],rms_test_s[0] = metrics_test(dataset_test,targets_test_all,predictions_test_all, \n",
    "    boxes,regions0,season, dataset,targets_all)\n",
    "\n",
    "targets0 = np.reshape(targets,(len(dataset.time_counter), len(indx[0]) // len(dataset.time_counter)))\n",
    "targets0 = np.array(np.split(targets0,len(np.unique(dataset.time_counter.dt.year)),axis=0))\n",
    "targets0 = np.transpose(targets0, (1,0,2))\n",
    "\n",
    "predictions0 = np.reshape(predictions,(len(dataset.time_counter), len(indx[0]) // len(dataset.time_counter)))\n",
    "predictions0 = np.array(np.split(predictions0,len(np.unique(dataset.time_counter.dt.year)),axis=0))\n",
    "predictions0 = np.transpose(predictions0, (1,0,2))\n",
    "\n",
    "errors = targets0 - predictions0\n",
    "\n",
    "for j in tqdm(range (0, n_resamples)):\n",
    "\n",
    "    temp = resample(errors.transpose(1,0,2))\n",
    "    errors_new = temp.transpose(1,0,2)\n",
    "    targets_new = np.ravel(errors_new) + predictions\n",
    "\n",
    "    targets_new_all = datasets_preparation2(dataset,targets_new,indx)\n",
    "    \n",
    "    _,predictions_all,predictions_test_all,season = train_test(dataset,inputs,targets_new,indx, dataset_test,inputs_test,indx_test, n_bins,drivers,spatial,inputs_names)\n",
    "    r_train[j+1],rms_train[j+1],slope_train[j+1],r_train_season[j+1],slope_train_season[j+1] = metrics_train(dataset,targets_new_all,predictions_all, boxes,regions0,season)\n",
    "    r_test[j+1],rms_test[j+1],slope_test[j+1],r_test_season[j+1],slope_test_season[j+1],rms_test_s[j+1] = metrics_test(dataset_test,targets_test_all,predictions_test_all, \n",
    "        boxes,regions0,season, dataset,targets_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(r_train,'Correlation Coefficient (Training, no seasonality)', boxnames)\n",
    "plot_hist(r_test_season,'Correlation Coefficient (Testing, no seasonality)', boxnames)\n",
    "plot_hist(rms_test_s,'Error (Testing)', boxnames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/ibougoudis/MOAD/files/results/' + name + '/bootstraps/' + name[0:4].lower() + '_pr_hist' + id + '_20_boot_100/'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(path + 'train_metrics.pkl', 'wb') as f:\n",
    "    dill.dump([r_train,rms_train,slope_train,r_train_season,slope_train_season], f)\n",
    "\n",
    "with open(path + 'test_metrics.pkl', 'wb') as f:\n",
    "    dill.dump([r_test,rms_test,slope_test,r_test_season,slope_test_season,rms_test_s], f)\n",
    "\n",
    "with open(path + 'readme.txt', 'w') as f:\n",
    "    f.write ('name: ' + name)\n",
    "    f.write('\\n')\n",
    "    f.write('period: ' + filename[35:42])\n",
    "    f.write ('\\n')\n",
    "    f.write ('input_features: ')\n",
    "    f.write (str([i for i in inputs_names]))\n",
    "    f.write ('\\n')\n",
    "    f.write('n_bins: ' + str(n_bins))\n",
    "    f.write ('\\n')\n",
    "    f.write('n_resamples: ')\n",
    "    f.write(str(n_resamples))\n",
    "    f.write ('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
