{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diatom concentration with functional regression based on the oceanographic boxes (spatial means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "\n",
    "from skfda.misc.hat_matrix import LocalLinearRegressionHatMatrix\n",
    "from skfda.preprocessing.smoothing import KernelSmoother\n",
    "\n",
    "from skfda.ml.regression import HistoricalLinearRegression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cmocean as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "\n",
    "def datasets_preparation(dataset, boxes, regions, name, inputs_names):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "\n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    inputs = []\n",
    "    for i in inputs_names:\n",
    "        inputs.append(dataset[i].to_numpy().reshape(*dataset[i].to_numpy().shape[:1],-1))\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.array(np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1))\n",
    "    targets = np.array(np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0))\n",
    "\n",
    "    # Transposing\n",
    "    inputs = np.transpose(inputs, (1,2,0,3))\n",
    "    targets = np.transpose(targets, (1,0,2))\n",
    "\n",
    "    indx = np.where(~np.isnan(targets[0]).any(axis=0))\n",
    "    inputs = inputs[:,:,:,indx[0]]\n",
    "    targets = targets[:,:,indx[0]]\n",
    "\n",
    "    regions = np.ravel(regions)\n",
    "    regions = regions[indx[0]]\n",
    "\n",
    "    regions_indiv_t = np.zeros((len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),len(boxes)))\n",
    "    regions_indiv_d = np.zeros((len(inputs_names),len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),len(boxes)))\n",
    "\n",
    "    for j in range (0,len(boxes)):\n",
    "\n",
    "        regions_indiv_d[:,:,:,j] = np.nanmean(np.where(regions==j, inputs, np.nan),axis=3)\n",
    "        regions_indiv_t[:,:,j] = np.nanmean(np.where(regions==j, targets, np.nan),axis=2)\n",
    "\n",
    "    inputs = regions_indiv_d\n",
    "    targets = regions_indiv_t\n",
    "\n",
    "    return(inputs, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, j, r_inputs, n_intervals, lag):\n",
    "\n",
    "    # Printing of the correlation coefficients\n",
    "    temp_inputs = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]), order='F')\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    temp_targets = np.reshape(targets, (targets.shape[0]*targets.shape[1]), order='F')\n",
    "\n",
    "    r_inputs[j] = np.round(r_regression(temp_inputs,temp_targets),2)\n",
    "\n",
    "    # Scaling the inputs\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), np.arange(0,len(inputs))))\n",
    "    temp_inputs = scaler_inputs.fit_transform(temp_inputs)\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    inputs = np.reshape(temp_inputs,(len(inputs),inputs.shape[1],inputs.shape[2]), order='F')   \n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp_targets = np.expand_dims(temp_targets,-1)\n",
    "    temp_targets = scaler_targets.fit_transform(temp_targets)\n",
    "    targets = temp_targets.reshape(targets.shape, order='F')\n",
    "\n",
    "    # Final transformations\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10))\n",
    "    kernel_estimator = LocalLinearRegressionHatMatrix(bandwidth=1)\n",
    "    smoother = KernelSmoother(kernel_estimator=kernel_estimator)\n",
    "    inputs = smoother.fit_transform(inputs)\n",
    "\n",
    "    model = HistoricalLinearRegression(n_intervals=n_intervals, lag=lag)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets,smoother,r_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets,smoother):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]), order='F')\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]), order='F')\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    inputs = smoother.transform(inputs)\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # Post-processing of predictions\n",
    "    predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.reshape(predictions, (targets.shape[0]*targets.shape[1]), order='F')\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape, order='F')\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(ax, corn, colour):\n",
    "\n",
    "    ax.plot([corn[2], corn[3], corn[3], corn[2], corn[2]], \n",
    "    [corn[0], corn[0], corn[1], corn[1], corn[0]], '-', color=colour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar (calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar(name, boxes):\n",
    "\n",
    "    names_1 = ['SWR, TP', '- TP', '- SWR', '+ WS', '+ LWR', '+ AT', '+ AP', '+ SH'] # 0,2,3\n",
    "    id_1 = [16, 9, 21, 18, 13, 22, 23, 24] \n",
    "\n",
    "    names_2 = ['SWR, LWR', '- LWR', '- SWR', '+ WS', '+ TP', '+ AT', '+ AP', '+ SH'] # 4,8\n",
    "    id_2 = [6, 9, 10, 11, 13, 3, 14, 12] \n",
    "\n",
    "    names_3 = ['SWR, AT', '- AT', '- SWR', '+ WS', '+ LWR', '+ TP', '+ AP', '+ SH'] # 5,6\n",
    "    id_3 = [7, 9, 25, 1, 3, 22, 26, 4] \n",
    "\n",
    "    names_4 = ['SWR', '+ AT', '+ WS', '+ LWR', 'SWR, WS, AT', '+ TP', '+ AP', '+ SH'] # 1,7\n",
    "    id_4 = [9, 7, 15, 6, 1, 16, 17, 20] \n",
    "\n",
    "    r_b = np.zeros((len(boxes), 8)) # 8 is the len of instances\n",
    "    rms_b = np.zeros((len(boxes), 8)) # 8 is the len of instances\n",
    "    names_all = []\n",
    "    \n",
    "    for i in range (0, len(boxes)):\n",
    "\n",
    "        for j in range (0, len(names_1)):\n",
    "\n",
    "            if i == 0 or i == 2 or i == 3: # SWR, TP          \n",
    "                path = '/data/ibougoudis/MOAD/files/results/' + name + '/bootstraps/' + name[0:4].lower() + '_func_reg2_boxes_s' + str(id_1[j]) +'_boot_100/'\n",
    "                names = names_1\n",
    "               \n",
    "            elif i == 4 or i == 8: # SWR, LWR\n",
    "                path = '/data/ibougoudis/MOAD/files/results/' + name + '/bootstraps/' + name[0:4].lower() + '_func_reg2_boxes_s' + str(id_2[j]) +'_boot_100/'\n",
    "                names = names_2\n",
    "\n",
    "            elif i == 5 or i == 6: # SWR, AT\n",
    "                path = '/data/ibougoudis/MOAD/files/results/' + name + '/bootstraps/' + name[0:4].lower() + '_func_reg2_boxes_s' + str(id_3[j]) +'_boot_100/'\n",
    "                names = names_3\n",
    "\n",
    "            elif i == 1 or i == 7: # SWR\n",
    "                path = '/data/ibougoudis/MOAD/files/results/' + name + '/bootstraps/' + name[0:4].lower() + '_func_reg2_boxes_s' + str(id_4[j]) +'_boot_100/'\n",
    "                names = names_4\n",
    "\n",
    "            with open(path + 'test_metrics.pkl', 'rb') as f:\n",
    "                temp = dill.load(f)\n",
    "                r_b[i,j] = np.mean(temp[3],axis=0)[i]\n",
    "                rms_b[i,j] = np.mean(temp[1],axis=0)[i]\n",
    "        names_all.append(names)\n",
    "\n",
    "    return (names_all, r_b, rms_b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Diatom'\n",
    "units = '[mmol m-2]'\n",
    "category = 'biomasses'\n",
    "\n",
    "filename = '/data/ibougoudis/MOAD/files/inputs/jan_apr.nc'\n",
    "inputs_names = ['Summation_of_solar_radiation', 'Summation_of_longwave_radiation', 'Mean_precipitation', 'Mean_pressure', 'Mean_air_temperature', 'Mean_specific_humidity', 'Mean_wind_speed']\n",
    "\n",
    "n_intervals = 4\n",
    "\n",
    "if filename[35:42] == 'jan_mar': # 75 days, 1st period\n",
    "    if n_intervals==3:\n",
    "        lags = [24.6, 49.3, 49.3, 24.6, 24.6, 24.6, 24.6, 24.6, 24.6]\n",
    "    elif n_intervals==4:\n",
    "        lags = [18.5, 37, 18.5, 18.5, 18.5, 37, 18.5, 18.5, 18.5] \n",
    "    period = '(16 Jan - 31 Mar)'\n",
    "    id = '1'\n",
    "\n",
    "elif filename[35:42] == 'jan_apr': # 120 days, 2nd period\n",
    "    if n_intervals==3:\n",
    "        lags = [39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6]\n",
    "    elif n_intervals==4:\n",
    "        lags = [29.75, 29.75, 29.75, 29.75, 29.75, 29.75, 29.75, 29.75, 29.75]\n",
    "    elif n_intervals==5:\n",
    "        lags = [47.6, 23.8, 23.8, 47.6, 47.6, 47.6, 47.6, 23.8, 95.2] \n",
    "    period = '(01 Jan - 30 Apr)'\n",
    "    id = '2'\n",
    "\n",
    "elif filename[35:42] == 'feb_apr': # 75 days, 3rd period\n",
    "    if n_intervals==3:\n",
    "        lags = [74, 49.3, 49.3, 74, 49.3, 49.3, 49.3, 24.6, 24.6] \n",
    "    elif n_intervals==4:\n",
    "        lags = [55.5, 55.5, 55.5, 37, 37, 55.5, 55.5, 37, 37] \n",
    "    period = '(15 Feb - 30 Apr)'\n",
    "    id = '3'\n",
    "\n",
    "ds = xr.open_dataset(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 9))\n",
    "mycmap = cm.cm.deep\n",
    "mycmap.set_bad('grey')\n",
    "ax.pcolormesh(ds[name][0], cmap=mycmap)\n",
    "sa_vi.set_aspect(ax)\n",
    "\n",
    "SoG_north = [650, 730, 100, 200]\n",
    "plot_box(ax, SoG_north, 'g')\n",
    "SoG_center = [450, 550, 200, 300]\n",
    "plot_box(ax, SoG_center, 'b')\n",
    "Fraser_plume = [380, 460, 260, 330]\n",
    "plot_box(ax, Fraser_plume, 'm')\n",
    "SoG_south = [320, 380, 280, 350]\n",
    "plot_box(ax, SoG_south, 'k')\n",
    "Haro_Boundary = [290, 350, 210, 280]\n",
    "plot_box(ax, Haro_Boundary, 'm')\n",
    "JdF_west = [250, 425, 25, 125]\n",
    "plot_box(ax, JdF_west, 'c')\n",
    "JdF_east = [200, 290, 150, 260]\n",
    "plot_box(ax, JdF_east, 'w')\n",
    "PS_all = [0, 200, 80, 320]\n",
    "plot_box(ax, PS_all, 'm')\n",
    "PS_main = [20, 150, 200, 280]\n",
    "plot_box(ax, PS_main, 'r')\n",
    "\n",
    "boxnames = ['GN','GC','FP','GS', 'HB', 'JdFW', 'JdFE', 'PSA', 'PSM']\n",
    "fig.legend(boxnames)\n",
    "\n",
    "boxes = [SoG_north,SoG_center,Fraser_plume,SoG_south,Haro_Boundary,JdF_west,JdF_east,PS_all,PS_main]\n",
    "\n",
    "regions0 = np.full((len(ds.y),len(ds.x)),np.nan)\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "    regions0[boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]] = i\n",
    "\n",
    "regions0 = xr.DataArray(regions0,dims = ['y','x'])\n",
    "\n",
    "# # Low resolution\n",
    "# temp = []\n",
    "\n",
    "# for i in boxes:\n",
    "#     temp.append([x//5 for x in i])\n",
    "\n",
    "# boxes = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low resolution\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# regions0 = regions0.isel(y=(np.arange(regions0.y[0], regions0.y[-1], 5)), \n",
    "#     x=(np.arange(regions0.x[0], regions0.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "labels = np.unique(dataset.time_counter.dt.strftime('%d %b'))\n",
    "indx_labels = np.argsort(pd.to_datetime(labels, format='%d %b'))\n",
    "labels = labels[indx_labels]\n",
    "\n",
    "r_inputs = np.zeros((len(boxnames), len(inputs_names)))\n",
    "\n",
    "inputs,targets = datasets_preparation(dataset,boxes,regions0,name,inputs_names)\n",
    "\n",
    "regr_all = []\n",
    "scaler_inputs_all = []\n",
    "scaler_targets_all = []\n",
    "smoother_all = []\n",
    "\n",
    "predictions = np.full(targets.shape,np.nan)\n",
    "\n",
    "for i in range (0,len(boxes)):\n",
    "\n",
    "    inputs2 = inputs[:,:,:,i] # inputs of the i cluster\n",
    "    targets2 = targets[:,:,i] # targets of the i cluster\n",
    "    regr, scaler_inputs, scaler_targets, smoother, r_inputs = regressor(inputs2,targets2,i,r_inputs,n_intervals,lags[i])\n",
    "\n",
    "    scaler_inputs_all.append(scaler_inputs)\n",
    "    scaler_targets_all.append(scaler_targets)\n",
    "    smoother_all.append(smoother)\n",
    "    regr_all.append(regr)\n",
    "\n",
    "    predictions[:,:,i] = scaling(regr_all[i],inputs2,scaler_inputs_all[i],targets2,scaler_targets_all[i],smoother_all[i]) # putting them in the right place\n",
    "\n",
    "print('Metrics between input features and '+name)\n",
    "temp = pd.DataFrame(r_inputs, index=boxnames, columns=inputs_names)\n",
    "display(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps - Triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(inputs_names)):\n",
    "\n",
    "    fig, axs = plt.subplots(1,len(boxes), figsize = (28,6), layout='constrained')\n",
    "\n",
    "    for j in range(0,len(boxes)):\n",
    "\n",
    "        temp = regr_all[j].coef_\n",
    "        coeff = temp.data_matrix\n",
    "        coeff = np.where(coeff==0,np.nan,coeff)\n",
    "\n",
    "        if j==0: # first box for this input feature\n",
    "\n",
    "            vmin = np.nanmin(coeff[0,:,:,i])\n",
    "            vmax = np.nanmax(coeff[0,:,:,i])\n",
    "\n",
    "        h = axs[j].imshow(coeff[0,:,:,i], cmap='bwr',aspect='auto', vmin=-np.maximum(np.abs(vmin),vmax), vmax=np.maximum(np.abs(vmin),vmax))\n",
    "\n",
    "        axs[j].set_ylim(axs[j].get_ylim()[::-1])\n",
    "        cbar = fig.colorbar(h)\n",
    "        axs[j].set_title(boxnames[j])\n",
    "        axs[j].set_xlabel('Day')\n",
    "        axs[j].set_ylabel('Day')\n",
    "        fig.suptitle(inputs_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps - Previous Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_names2 = ['SWR', 'LWR', 'TP', 'AP', 'AT', 'SH', 'WS']\n",
    "\n",
    "coef_mean = np.zeros((len(boxes),len(inputs_names)))\n",
    "coef_std = np.zeros((len(boxes),len(inputs_names)))\n",
    "coef_range = np.zeros((len(boxes),len(inputs_names)))\n",
    "coef_max = np.zeros((len(boxes),len(inputs_names)))\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize = (10,10), layout='constrained')\n",
    "\n",
    "for j in range(0,len(boxes)):\n",
    "\n",
    "    temp = regr_all[j].coef_\n",
    "    coeff = temp.data_matrix\n",
    "    coeff = np.where(coeff==0,np.nan,coeff)\n",
    "\n",
    "    for i in range (0, len(inputs_names)):\n",
    " \n",
    "        temp = np.zeros((len(labels)-1))\n",
    "\n",
    "        for k in range (0, len(labels) -1):\n",
    "            temp[k] = coeff[0,k,k+1,i]\n",
    "\n",
    "        coef_mean[j,i] = np.nanmean(temp) # Only the impact of the previous day\n",
    "        coef_std[j,i] = np.nanstd(temp) # Only the impact of the previous day\n",
    "        coef_range[j,i] = np.nanmax(temp) - np.nanmin(temp) # Only the impact of the previous day\n",
    "        coef_max[j,i] = np.maximum(np.abs(np.nanmin(temp)),np.nanmax(temp)) # Only the impact of the previous day\n",
    "\n",
    "axs[0,0].set_axis_off()\n",
    "\n",
    "h = axs[0,1].imshow(coef_std, cmap = cm.cm.ice, aspect='auto', vmin= coef_std.min(), vmax=coef_std.max())\n",
    "axs[0,1].set_title('Std')\n",
    "cbar = fig.colorbar(h)\n",
    "axs[0,1].set_ylim(axs[0,1].get_ylim()[::-1])\n",
    "axs[0,1].set_xticks(range(len(inputs_names)), labels=inputs_names2)\n",
    "axs[0,1].set_yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "h = axs[1,0].imshow(coef_range,cmap = cm.cm.ice, aspect='auto', vmin= coef_range.min(), vmax=coef_range.max())\n",
    "axs[1,0].set_title('Range')\n",
    "cbar = fig.colorbar(h)\n",
    "axs[1,0].set_ylim(axs[1,0].get_ylim()[::-1])\n",
    "axs[1,0].set_xticks(range(len(inputs_names)), labels=inputs_names2)\n",
    "axs[1,0].set_yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "h = axs[1,1].imshow(coef_max, cmap = cm.cm.ice, aspect='auto', vmin= coef_max.min(), vmax=coef_max.max())\n",
    "axs[1,1].set_title('Max Absolute Value')\n",
    "cbar = fig.colorbar(h)\n",
    "axs[1,1].set_ylim(axs[1,1].get_ylim()[::-1])\n",
    "axs[1,1].set_xticks(range(len(inputs_names)), labels=inputs_names2)\n",
    "axs[1,1].set_yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "fig.suptitle('Previous Day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps - All days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_mean = np.zeros((len(boxes),len(inputs_names)))\n",
    "coef_std = np.zeros((len(boxes),len(inputs_names)))\n",
    "coef_range = np.zeros((len(boxes),len(inputs_names)))\n",
    "coef_max = np.zeros((len(boxes),len(inputs_names)))\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize = (10,10), layout='constrained')\n",
    "\n",
    "for j in range(0,len(boxes)):\n",
    "\n",
    "    temp = regr_all[j].coef_\n",
    "    coeff = temp.data_matrix\n",
    "    coeff = np.where(coeff==0,np.nan,coeff)\n",
    "\n",
    "    for i in range (0, len(inputs_names)):\n",
    "        coef_mean[j,i] = np.nanmean(coeff[:,:,:,i])\n",
    "        coef_std[j,i] = np.nanstd(coeff[:,:,:,i])\n",
    "        coef_range[j,i] = np.nanmax(coeff[:,:,:,i]) - np.nanmin(coeff[:,:,:,i])\n",
    "        coef_max[j,i] = np.maximum(np.abs(np.nanmin(coeff[:,:,:,i])),np.nanmax(coeff[:,:,:,i]))\n",
    "\n",
    "axs[0,0].set_axis_off()\n",
    "\n",
    "h = axs[0,1].imshow(coef_std, cmap = cm.cm.ice, aspect='auto', vmin= coef_std.min(), vmax=coef_std.max())\n",
    "axs[0,1].set_title('Std')\n",
    "cbar = fig.colorbar(h)\n",
    "axs[0,1].set_ylim(axs[0,1].get_ylim()[::-1])\n",
    "axs[0,1].set_xticks(range(len(inputs_names)), labels=inputs_names2)\n",
    "axs[0,1].set_yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "h = axs[1,0].imshow(coef_range, cmap = cm.cm.ice, aspect='auto', vmin= coef_range.min(), vmax=coef_range.max())\n",
    "axs[1,0].set_title('Range')\n",
    "cbar = fig.colorbar(h)\n",
    "axs[1,0].set_ylim(axs[1,0].get_ylim()[::-1])\n",
    "axs[1,0].set_xticks(range(len(inputs_names)), labels=inputs_names2)\n",
    "axs[1,0].set_yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "h = axs[1,1].imshow(coef_max, cmap = cm.cm.ice, aspect='auto', vmin= coef_max.min(), vmax=coef_max.max())\n",
    "axs[1,1].set_title('Max Absolute Value')\n",
    "cbar = fig.colorbar(h)\n",
    "axs[1,1].set_ylim(axs[1,1].get_ylim()[::-1])\n",
    "axs[1,1].set_xticks(range(len(inputs_names)), labels=inputs_names2)\n",
    "axs[1,1].set_yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "fig.suptitle('All days')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar Plot (R Testing no Seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, r_b, rms_b = radar (name, boxes)\n",
    "\n",
    "labels = ['(a)','(b)','(c)','(d)','(e)','(f)','(g)','(h)','(i)']\n",
    "\n",
    "k,l = 0,0\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 6.5), layout='constrained', subplot_kw={'projection': 'polar'})\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "\n",
    "    theta = np.linspace(0, 2*np.pi, 9)\n",
    "    values = np.append(r_b[i,:], r_b[i,:][0])\n",
    "\n",
    "    ax[k,l].plot(theta, values,  marker='o')\n",
    "    ax[k,l].plot(np.linspace(0, 2*np.pi, 100), np.full(100, values[0]), ls = '--')\n",
    "    \n",
    "    ax[k,l].annotate(labels[i], (0.02, 1.2), xycoords='axes fraction', fontsize=14)\n",
    "\n",
    "    ax[k,l].set_theta_zero_location('N')\n",
    "    ax[k,l].set_rmax(np.max(values)+0.05)\n",
    "    ax[k,l].set_rmin(np.min(values)-0.1)\n",
    "    ax[k,l].set_rticks(np.round(np.linspace(np.round(np.min(values),1), np.round(np.max(values),1), 3), 2))\n",
    "\n",
    "    ax[k,l].set_rlabel_position(230) # The text\n",
    "    ax[k,l].tick_params(pad = 7)\n",
    "\n",
    "    ax[k,l].set_xticks(theta[:-1], names[i])\n",
    "    ax[k,l].set_title(boxnames[i], x=0.50, y=1.15)\n",
    "\n",
    "    l=l+1\n",
    "    if l==5:\n",
    "        l=0\n",
    "        k=k+1\n",
    "\n",
    "fig.suptitle('Importances of Input Features on DB (R Testing no Seasonality)')\n",
    "\n",
    "ax[k,l].remove()\n",
    "ax[k,l] = fig.add_subplot(2,5,10)\n",
    "\n",
    "ax[k,l].annotate('(j)', (0.00, 1.2), xycoords='axes fraction', fontsize=14)\n",
    "\n",
    "h = plt.imshow(coef_max, cmap = cm.cm.ice, aspect='auto', vmin= coef_max.min(), vmax=coef_max.max())\n",
    "plt.title('Max Absolute Value')\n",
    "cbar = plt.colorbar(h, pad=0)\n",
    "\n",
    "plt.ylim(plt.ylim()[::-1])\n",
    "plt.xticks(range(len(inputs_names)), labels=inputs_names2, rotation=30)\n",
    "plt.yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar Plot (RMS Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, r_b, rms_b = radar (name, boxes)\n",
    "\n",
    "labels = ['(a)','(b)','(c)','(d)','(e)','(f)','(g)','(h)','(i)']\n",
    "\n",
    "k,l = 0,0\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 6.5), layout='constrained', subplot_kw={'projection': 'polar'})\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "\n",
    "    theta = np.linspace(0, 2*np.pi, 9)\n",
    "    values = np.append(rms_b[i,:], rms_b[i,:][0])\n",
    "\n",
    "    ax[k,l].plot(theta, values,  marker='o')\n",
    "    ax[k,l].plot(np.linspace(0, 2*np.pi, 100), np.full(100, values[0]), ls = '--')\n",
    "    \n",
    "    ax[k,l].annotate(labels[i], (0.02, 1.2), xycoords='axes fraction', fontsize=14)\n",
    "\n",
    "    ax[k,l].set_theta_zero_location('N')\n",
    "    ax[k,l].set_rmax(np.max(values)+1)\n",
    "    # ax[k,l].set_rmin(np.min(values))\n",
    "    # ax[k,l].set_rticks(np.round(np.linspace(np.round(np.min(values),1), np.round(np.max(values),1), 4), 2))\n",
    "\n",
    "    ax[k,l].set_rlabel_position(230) # The text\n",
    "    ax[k,l].tick_params(pad = 7)\n",
    "\n",
    "    ax[k,l].set_xticks(theta[:-1], names[i])\n",
    "    ax[k,l].set_title(boxnames[i], x=0.50, y=1.15)\n",
    "\n",
    "    l=l+1\n",
    "    if l==5:\n",
    "        l=0\n",
    "        k=k+1\n",
    "\n",
    "fig.suptitle('Importances of Input Features on DB (RMS Testing)')\n",
    "\n",
    "ax[k,l].remove()\n",
    "ax[k,l] = fig.add_subplot(2,5,10)\n",
    "\n",
    "ax[k,l].annotate('(j)', (0.00, 1.2), xycoords='axes fraction', fontsize=14)\n",
    "\n",
    "h = plt.imshow(coef_max, cmap = cm.cm.ice, aspect='auto', vmin= coef_max.min(), vmax=coef_max.max())\n",
    "plt.title('Max Absolute Value')\n",
    "cbar = plt.colorbar(h, pad=0)\n",
    "\n",
    "plt.ylim(plt.ylim()[::-1])\n",
    "plt.xticks(range(len(inputs_names)), labels=inputs_names2, rotation=30)\n",
    "plt.yticks(range(len(boxnames)), labels=boxnames, rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/ibougoudis/MOAD/files/results/' + name + '/single_runs/' + name[0:4].lower() + '_func_reg' + id + '_boxes_s_df_im/'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "with lzma.open(path + 'regr_all.xz', 'wb') as f:   \n",
    "    dill.dump(regr_all, f)\n",
    "\n",
    "with open(path + 'r_inputs.pkl', 'wb') as f:\n",
    "    dill.dump(r_inputs, f)\n",
    "\n",
    "with open(path + 'importances.pkl', 'wb') as f:\n",
    "    dill.dump([coef_max], f)\n",
    "\n",
    "with open(path + 'metrics.pkl', 'wb') as f:\n",
    "    dill.dump([boxnames, inputs_names2, names, r_b, rms_b], f)\n",
    "\n",
    "with open(path + 'readme.txt', 'w') as f:\n",
    "    f.write ('name: ' + name)\n",
    "    f.write('\\n')\n",
    "    f.write('period: ' + filename[35:42])\n",
    "    f.write ('\\n')\n",
    "    f.write ('input_features: ')\n",
    "    f.write (str([i for i in inputs_names]))\n",
    "    f.write ('\\n')\n",
    "    f.write('n_intervals: ' + str(n_intervals))\n",
    "    f.write ('\\n')\n",
    "    f.write('lags: ')\n",
    "    f.write (str([i for i in lags]))\n",
    "    f.write ('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
