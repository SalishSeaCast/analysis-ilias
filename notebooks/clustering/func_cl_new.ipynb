{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from skfda.ml.clustering import KMeans\n",
    "from skfda.representation.grid import FDataGrid\n",
    "\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drivers Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drivers_preparation(dataset2):\n",
    "\n",
    "    indx = np.where((dataset2.time_counter.dt.month==2) & (dataset2.time_counter.dt.day==29))\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_wind_speed'].to_numpy().reshape(*dataset2['Mean_wind_speed'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1)\n",
    "        ])\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset2.time_counter.dt.year)),axis=1)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(inputs[0]).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((MinMaxScaler(), [0,1,2]))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2])) \n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs2 = FDataGrid(inputs, np.arange(0,len(inputs[0])))\n",
    "\n",
    "    return(inputs2,indx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_preparation(dataset, name):\n",
    "\n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    targets = targets.transpose()\n",
    "    targets2 = FDataGrid(targets,np.arange(0,len(targets[0])))\n",
    "\n",
    "    return(targets2,indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(name,clusters,unique,cluster_mean,counts,ind_cluster):\n",
    "\n",
    "    if name == 'drivers':\n",
    "        temp = np.vstack((counts,cluster_mean.transpose()))\n",
    "        temp = temp.reshape(4,len(unique))\n",
    "        temp = pd.DataFrame(temp.transpose(),columns=['counts','Summation of solar radiation', 'Mean wind speed', 'Mean Temperature'],index=unique+1)\n",
    "    else:\n",
    "        temp = np.concatenate((counts,cluster_mean))\n",
    "        temp = temp.reshape(2,len(unique))\n",
    "        temp = pd.DataFrame(temp.transpose(),columns=['counts','mean'],index=unique+1)\n",
    "    temp.index.name = 'Cluster'\n",
    "\n",
    "    fig, ax = plt.subplots(figsize =(5,9))\n",
    "    \n",
    "    cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "    cmap.set_bad('gray')\n",
    "    clus = clusters.plot(ax=ax, cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "    cbar = fig.colorbar(clus, ticks = unique+0.5) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    cbar.set_label('Clusters [count]')\n",
    "    ax.set_title('Functional Clustering for '+ name + ' (2007-2024)')\n",
    "    sa_vi.set_aspect(ax)\n",
    "    plt.show()\n",
    "\n",
    "    display(temp.transpose())\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "    axs[0, 0].plot(ind_cluster[0])\n",
    "    axs[0, 0].set_title('Cluster 1')\n",
    "\n",
    "    axs[0, 1].plot(ind_cluster[1])\n",
    "    axs[0, 1].set_title('Cluster 2')\n",
    "\n",
    "    axs[1, 0].plot(ind_cluster[2])\n",
    "    axs[1, 0].set_title('Cluster 3')\n",
    "\n",
    "    axs[1, 1].plot(ind_cluster[3])\n",
    "    axs[1, 1].set_title('Cluster 4')\n",
    "\n",
    "    axs[2, 0].plot(ind_cluster[4])\n",
    "    axs[2, 0].set_title('Cluster 5')\n",
    "    \n",
    "    axs[2, 1].plot(ind_cluster[5])\n",
    "    axs[2, 1].set_title('Cluster 6')\n",
    "\n",
    "    if name == 'drivers':\n",
    "        fig.legend(('Summation_of_solar_radiation','Mean_wind_speed','Mean_air_temperature'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(dataset,quant,indx,name):\n",
    "\n",
    "    # Training\n",
    "    kmeans = KMeans(n_clusters=6)\n",
    "    clusters = kmeans.fit_predict(quant)\n",
    "\n",
    "    # Sorting so that cluster 1 has the minimum mean target value, 6 the maximum\n",
    "\n",
    "        # Finding the mean of each cluster\n",
    "    if name == 'drivers':\n",
    "        cluster_mean_all = np.mean(kmeans.cluster_centers_.data_matrix,axis=1)\n",
    "        cluster_mean = cluster_mean_all[:,0]  # Sorted based on the first input\n",
    "    else:\n",
    "        cluster_mean = np.squeeze(np.mean(kmeans.cluster_centers_.data_matrix,axis=1))\n",
    "\n",
    "        # The index to sort the clusters\n",
    "    indx3 = np.argsort(np.argsort(cluster_mean)) # For the complete map we need the double np.argsort\n",
    "\n",
    "        # Sorting\n",
    "    for j in np.arange(0,len(np.unique(clusters))):\n",
    "        clusters = xr.where(kmeans.labels_==j, indx3[j], clusters)\n",
    "\n",
    "    unique, counts = np.unique(clusters, return_counts=True)\n",
    "    \n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "    clusters2 = xr.DataArray(clusters,dims = ['y','x'])\n",
    "\n",
    "    # Obtaining & sorting the individual clusters\n",
    "    if name == 'drivers':\n",
    "        ind_cluster = kmeans.cluster_centers_.data_matrix[np.argsort(indx3)]\n",
    "    else:\n",
    "        ind_cluster = kmeans.cluster_centers_.data_matrix[np.argsort(indx3)]\n",
    "\n",
    "    # Sorting the mean values\n",
    "    if name == 'drivers':\n",
    "        cluster_mean = cluster_mean_all[np.argsort(cluster_mean)]\n",
    "    else:\n",
    "        cluster_mean = cluster_mean[np.argsort(cluster_mean)]\n",
    "\n",
    "    return(clusters2,unique,cluster_mean,counts,ind_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(\n",
    "    y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(\n",
    "    y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2024'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2024'))\n",
    "\n",
    "# id = 0 # For drivers\n",
    "\n",
    "id = 1 # For targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drivers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if id == 0:\n",
    "\n",
    "    drivers,indx = drivers_preparation(dataset2)\n",
    "    clusters_all,unique, clusters_mean,counts, ind_clusters = clustering(dataset2,drivers,indx,'drivers')\n",
    "    summary('drivers',clusters,unique,clusters_mean,counts,ind_clusters)\n",
    "\n",
    "    ind_clusters = ind_clusters.transpose(1,2,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Years (Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if id == 1:\n",
    "\n",
    "    name = 'Diatom'\n",
    "    targets,indx = targets_preparation(dataset,name)\n",
    "    clusters_all, unique, clusters_mean, counts, ind_clusters = clustering(dataset,targets,indx, name)\n",
    "    summary(name,clusters,unique,clusters_mean,counts,ind_clusters)\n",
    "\n",
    "    ind_clusters = ind_clusters.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Years (based on the all years clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_clusters2 = np.zeros((ind_clusters.shape[0],len(np.unique(ds.time_counter.dt.year)),len(np.unique(clusters_all))-1))\n",
    "years = np.unique(ds.time_counter.dt.year)\n",
    "\n",
    "for i in tqdm(range(0, len(years))):\n",
    "\n",
    "    dataset = ds.sel(time_counter = slice(str(years[i]), str(years[i])))\n",
    "    dataset2 = ds2.sel(time_counter = slice(str(years[i]), str(years[i])))\n",
    "\n",
    "    # # Drivers\n",
    "    # drivers, indx = drivers_preparation(dataset2)\n",
    "    # drivers = drivers.data_matrix.transpose(2,1,0)\n",
    "    # clusters2 = np.ravel(clusters_all)[indx]\n",
    "    # for j in range (0,len(np.unique(clusters2))):\n",
    "    \n",
    "    #     temp = xr.where(clusters2==j, drivers, np.nan)\n",
    "    #     ind_clusters2[:,i,j] = np.nanmean(temp,axis=1)\n",
    "\n",
    "    # Targets\n",
    "    targets, indx = targets_preparation(dataset,name)\n",
    "    targets = np.squeeze(targets.data_matrix).transpose()\n",
    "    clusters2 = np.ravel(clusters_all)[indx]\n",
    "    for j in range (0,len(np.unique(clusters2))):\n",
    "    \n",
    "        temp = xr.where(clusters2==j, targets, np.nan)\n",
    "        ind_clusters2[:,i,j] = np.nanmean(temp,axis=1)\n",
    "\n",
    "clusters_mean2 = np.round(np.mean(ind_clusters2,axis=0),10)\n",
    "clusters_mean2 = np.append(clusters_mean2,np.expand_dims(clusters_mean,0),axis=0)\n",
    "\n",
    "years2 = np.append(years,'2007-2024')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = dataset[name].mean('time_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(np.unique(clusters_all))-1):\n",
    "\n",
    "    temp = pd.DataFrame(clusters_mean2[:,i].transpose(),columns=['mean'],index=years2)\n",
    "    temp.index.name = 'Year'\n",
    "    print ('Cluster '+ str(i+1))\n",
    "    display(temp.transpose())\n",
    "\n",
    "    k=0\n",
    "    l=0\n",
    "\n",
    "    fig, ax = plt.subplots(5, 4, figsize=(10, 15))\n",
    "\n",
    "    for j in np.arange (0,len(years)):\n",
    "\n",
    "        ax[k, l].plot(ind_clusters2[:,j,i])\n",
    "        ax[k, l].set_title(str(years[j]))\n",
    "\n",
    "        l=l+1\n",
    "\n",
    "        if l==4:\n",
    "            l=0\n",
    "            k=k+1\n",
    "\n",
    "    ax[4,2].axis('off')\n",
    "    ax[4,3].plot(ind_clusters[:,0,i])\n",
    "    ax[4,3].set_title('2007-2024')\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    fig.suptitle('Cluster '+ str(i+1))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_per_year.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.unique(ds.time_counter.dt.year)\n",
    "\n",
    "clusters_per_year =  np.zeros((len(np.unique(ds.time_counter.dt.year)),dataset[name].shape[1],dataset[name].shape[2]))\n",
    "counts_per_year = np.zeros((len(np.unique(ds.time_counter.dt.year)),dataset[name].shape[1],dataset[name].shape[2]))\n",
    "\n",
    "for i in tqdm(range(0, len(years))):\n",
    "\n",
    "    dataset = ds.sel(time_counter = slice(str(years[i]), str(years[i])))\n",
    "    dataset2 = ds2.sel(time_counter = slice(str(years[i]), str(years[i])))\n",
    "\n",
    "    targets,indx = targets_preparation(dataset,name)\n",
    "    clusters, unique, clusters_mean, counts, ind_clusters = clustering(dataset,targets,indx, name)\n",
    "    clusters_per_year[i,:,:] = clusters\n",
    "\n",
    "a = xr.DataArray(clusters_per_year,dims = ['years','y','x'])\n",
    "b = xr.DataArray(clusters_all,dims = ['y','x'])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 4, figsize=(10, 15))\n",
    "\n",
    "cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "cmap.set_bad('gray')\n",
    "\n",
    "k=0\n",
    "l=0\n",
    "\n",
    "for j in np.arange (0,len(years)):\n",
    "\n",
    "    temp = np.concatenate((counts,clusters_mean))\n",
    "    temp = temp.reshape(2,len(unique))\n",
    "    temp = pd.DataFrame(temp.transpose(),columns=['counts','mean'],index=unique+1)\n",
    "    temp.index.name = 'Cluster'\n",
    "\n",
    "    display(temp.transpose())\n",
    "\n",
    "    clus = a[j].plot(ax=ax[k,l], cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "\n",
    "    cbar = fig.colorbar(clus, ticks=unique+0.5, fraction=0.08, pad=0.08) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    # cbar.set_label('Clusters [count]')\n",
    "    ax[k,l].set_title(str(years[j]))\n",
    "\n",
    "    sa_vi.set_aspect(ax[k,l])\n",
    "\n",
    "    l=l+1\n",
    "\n",
    "    if l==4:\n",
    "        l=0\n",
    "        k=k+1\n",
    "\n",
    "ax[4,2].axis('off')\n",
    "\n",
    "clus = b.plot(ax=ax[4,3], cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "cbar = fig.colorbar(clus, ticks=unique+0.5, fraction=0.08, pad=0.08) \n",
    "cbar.set_ticklabels(unique+1)\n",
    "# cbar.set_label('Clusters [count]')\n",
    "ax[4,3].set_title('2007-2024')\n",
    "sa_vi.set_aspect(ax[4,3])\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "fig.suptitle('Functional Clustering for ' + str(name))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
