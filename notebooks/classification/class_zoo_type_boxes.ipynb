{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdd7bab",
   "metadata": {},
   "source": [
    "# Classification of Zooplankton Type (Boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a192916",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cb83d",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, classes, inputs_names, regions):\n",
    "\n",
    "    x = np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter))\n",
    "    \n",
    "    inputs = []\n",
    "    \n",
    "    for i in inputs_names:\n",
    "        inputs.append(dataset[i].to_numpy().flatten())  \n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    targets = np.ravel(classes)\n",
    "\n",
    "    regions = np.tile(np.ravel(regions), len(np.unique(dataset.time_counter)))   \n",
    "     \n",
    "    indx = np.where(np.isfinite(targets) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,indx[0]]\n",
    "    targets = targets[indx[0]]\n",
    "    regions = regions[indx[0]]\n",
    "\n",
    "    inputs = inputs.transpose()\n",
    "\n",
    "    return(inputs, targets, indx, regions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f17373",
   "metadata": {},
   "source": [
    "## Plotting (Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(ax, corn, colour):\n",
    "\n",
    "    ax.plot([corn[2], corn[3], corn[3], corn[2], corn[2]], \n",
    "    [corn[0], corn[0], corn[1], corn[1], corn[0]], '-', color=colour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce9336",
   "metadata": {},
   "source": [
    "## New Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/clustering/feb_apr_yearly_clustering.nc')\n",
    "\n",
    "z1 = xr.where(ds['Z1'] <5, 0, ds['Z1'])\n",
    "z2 = xr.where(ds['Z2'] <5, 0, ds['Z2'])\n",
    "\n",
    "classes = z1\n",
    "classes = xr.where((z1==5) & (z2==0), 1, classes) # z1 high\n",
    "classes = xr.where((z1==5) & (z2==5), 3, classes) # z1 and z2 high\n",
    "classes = xr.where((z2==5) & (z1==0), 2, classes) # z2 high\n",
    "\n",
    "# Low resolution\n",
    "\n",
    "# classes = classes.isel(y=(np.arange(classes.y[0], classes.y[-1], 5)), \n",
    "#     x=(np.arange(classes.x[0], classes.x[-1], 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e5cbb",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdaeb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/data/ibougoudis/MOAD/files/inputs/feb_apr.nc'\n",
    "\n",
    "# drivers =  ['Summation_of_solar_radiation', 'Summation_of_longwave_radiation', 'Mean_precipitation', 'Mean_pressure', 'Mean_air_temperature', 'Mean_specific_humidity', 'Mean_wind_speed']\n",
    "\n",
    "drivers =  ['Summation_of_solar_radiation', 'Summation_of_longwave_radiation', 'Mean_precipitation', 'Mean_pressure', 'Mean_air_temperature', 'Mean_specific_humidity', 'Mean_wind_speed']\n",
    "spatial = ['Latitude', 'Longitude']\n",
    "day_input = ['Day_of_year']\n",
    "\n",
    "inputs_names = drivers + spatial + day_input\n",
    "\n",
    "n_bins = 255\n",
    "\n",
    "if filename[35:42] == 'jan_mar': # 75 days, 1st period\n",
    "    period = '(16 Jan - 31 Mar)'\n",
    "    id = '1'\n",
    "    months = ['January', 'February', 'March']\n",
    "\n",
    "elif filename[35:42] == 'jan_apr': # 120 days, 2nd period\n",
    "    period = '(01 Jan - 30 Apr)'\n",
    "    id = '2'\n",
    "    months = ['January', 'February', 'March', 'April']\n",
    "\n",
    "elif filename[35:42] == 'feb_apr': # 75 days, 3rd period\n",
    "    period = '(15 Feb - 30 Apr)'\n",
    "    id = '3'\n",
    "    months = ['February', 'March', 'April']\n",
    "\n",
    "elif filename[35:42] == 'apr_jun': # 76 days, 4th period\n",
    "    period = '(16 Apr - 30 Jun)'\n",
    "    id = '4'\n",
    "    months = ['April', 'May', 'June']\n",
    "\n",
    "elif filename[35:42] == 'may_sep': # 153 days, 5th period\n",
    "    period = '(01 May - 30 Sep)'\n",
    "    id = '5'\n",
    "    months = ['May', 'June', 'July', 'August', 'September']\n",
    "   \n",
    "ds = xr.open_dataset(filename)\n",
    "ds0 = ds # For the regional plot\n",
    "\n",
    "# Low resolution\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00885827",
   "metadata": {},
   "source": [
    "## Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy = xr.open_dataset('/home/sallen/MEOPAR/grid/bathymetry_202108.nc')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 9))\n",
    "mycmap = cm.deep\n",
    "mycmap.set_bad('grey')\n",
    "ax.pcolormesh(bathy.Bathymetry, cmap=mycmap)\n",
    "sa_vi.set_aspect(ax)\n",
    "\n",
    "SoG_north = [650, 730, 100, 200]\n",
    "plot_box(ax, SoG_north, 'g')\n",
    "SoG_center = [450, 550, 200, 300]\n",
    "plot_box(ax, SoG_center, 'b')\n",
    "Fraser_plume = [380, 460, 260, 330]\n",
    "plot_box(ax, Fraser_plume, 'm')\n",
    "SoG_south = [320, 380, 280, 350]\n",
    "plot_box(ax, SoG_south, 'k')\n",
    "Haro_Boundary = [290, 350, 210, 280]\n",
    "plot_box(ax, Haro_Boundary, 'm')\n",
    "JdF_west = [250, 425, 25, 125]\n",
    "plot_box(ax, JdF_west, 'c')\n",
    "JdF_east = [200, 290, 150, 260]\n",
    "plot_box(ax, JdF_east, 'w')\n",
    "PS_all = [0, 200, 80, 320]\n",
    "plot_box(ax, PS_all, 'm')\n",
    "PS_main = [20, 150, 200, 280]\n",
    "plot_box(ax, PS_main, 'r')\n",
    "\n",
    "boxnames = ['GN','GC','FP','GS', 'HB', 'JdFW', 'JdFE', 'PSA', 'PSM']\n",
    "fig.legend(boxnames)\n",
    "\n",
    "boxes = [SoG_north,SoG_center,Fraser_plume,SoG_south,Haro_Boundary,JdF_west,JdF_east,PS_all,PS_main]\n",
    "\n",
    "regions0 = np.full((len(ds0.y),len(ds0.x)),np.nan)\n",
    "for i in range (0, len(boxes)):\n",
    "    regions0[boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]] = i\n",
    "\n",
    "regions0 = xr.DataArray(regions0,dims = ['y','x'])\n",
    "\n",
    "# Low resolution\n",
    "\n",
    "# temp = []\n",
    "# for i in boxes:\n",
    "\n",
    "#     temp.append([x//5 for x in i])\n",
    "\n",
    "# boxes = temp\n",
    "\n",
    "# regions0 = regions0.isel(y=(np.arange(regions0.y[0], regions0.y[-1], 5)), \n",
    "#     x=(np.arange(regions0.x[0], regions0.x[-1], 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6a854",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "classes2 = classes.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "labels = np.unique(dataset.time_counter.dt.strftime('%d %b'))\n",
    "indx_labels = np.argsort(pd.to_datetime(labels, format='%d %b'))\n",
    "labels = labels[indx_labels]\n",
    "\n",
    "inputs, targets, indx, regions = datasets_preparation(dataset, classes2, inputs_names, regions0)\n",
    "\n",
    "clf_all = []\n",
    "predictions = np.full(targets.shape,np.nan) # size of targets without nans\n",
    "\n",
    "if spatial == []:\n",
    "    model = make_pipeline(ColumnTransformer(\n",
    "        transformers=[('drivers', StandardScaler(), np.arange(0,len(drivers)))], remainder='passthrough'),\n",
    "        HistGradientBoostingClassifier(categorical_features=[len(drivers)]))\n",
    "\n",
    "else:\n",
    "    model = make_pipeline(ColumnTransformer(\n",
    "    transformers=[('drivers', StandardScaler(), np.arange(0,len(drivers))), \n",
    "        ('spatial', KBinsDiscretizer(n_bins=n_bins,encode='ordinal',strategy='quantile'), np.arange(inputs_names.index(spatial[0]),inputs_names.index(spatial[-1])+1))],\n",
    "        remainder='passthrough'),\n",
    "    HistGradientBoostingClassifier(categorical_features=np.arange(inputs_names.index(spatial[0]),len(inputs_names))))\n",
    "\n",
    "for i in tqdm(range (0, len(boxes))):\n",
    "\n",
    "    indx2 = np.where(regions==i)\n",
    "    inputs2 = inputs[indx2[0],:]\n",
    "    targets2 = targets[indx2[0]]\n",
    "\n",
    "    clf = model.fit(inputs2,targets2)\n",
    "    predictions[indx2[0]] = clf.predict(inputs2)\n",
    "\n",
    "    clf_all.append(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7dfa9",
   "metadata": {},
   "source": [
    "## Training Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_names = ['no zooplankton', 'high Z1', 'high Z2', 'high Z1 & Z2']\n",
    "print('Classification Report (Training)')\n",
    "print('\\n')\n",
    "\n",
    "metrics_all = []\n",
    "cm_all = []\n",
    "\n",
    "for i in range (0, len(boxnames)):\n",
    "\n",
    "    print (boxnames[i])\n",
    "\n",
    "    indx2 = np.where(regions==i)\n",
    "    predictions2 = predictions[indx2[0]]\n",
    "    targets2 = targets[indx2[0]]\n",
    "\n",
    "    print(classification_report(targets2, predictions2, target_names=targets_names))\n",
    "    metrics = precision_recall_fscore_support(targets2, predictions2, labels = [0,1,2,3])\n",
    "    metrics_all.append(np.array(metrics))\n",
    "\n",
    "    cm = confusion_matrix(targets2, predictions2, labels = [0,1,2,3])\n",
    "    cm_all.append(cm)\n",
    "    ConfusionMatrixDisplay.from_predictions(targets2, predictions2, display_labels=targets_names, colorbar=False)\n",
    "    plt.title('Confusion Matrix for ' + boxnames[i])\n",
    "    plt.show()\n",
    "\n",
    "metrics_all = np.array(metrics_all)\n",
    "cm_all = np.array(cm_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7072c9",
   "metadata": {},
   "source": [
    "## Feature Importance (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ca5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_importance_all = []\n",
    "# recall_importance_all = []\n",
    "\n",
    "# inputs_names2 = ['SWR', 'LWR', 'TP', 'AP', 'AT', 'SH', 'WS', 'Lat', 'Lon', 'Day']\n",
    "\n",
    "# for i in tqdm(range (0, len(boxes))):\n",
    "\n",
    "#     indx2 = np.where(regions==i)\n",
    "#     inputs2 = inputs[indx2[0],:]\n",
    "#     targets2 = targets[indx2[0]]\n",
    "\n",
    "#     importances_all = permutation_importance(clf_all[i], inputs2, targets2, n_repeats=5, scoring=('accuracy','recall_macro'))\n",
    "\n",
    "#     accuracy_importance = pd.Series(importances_all['accuracy'].importances_mean)\n",
    "#     accuracy_importance_all.append(accuracy_importance)\n",
    "\n",
    "#     recall_importance = pd.Series(importances_all['recall_macro'].importances_mean)\n",
    "#     recall_importance_all.append(recall_importance)\n",
    "\n",
    "# accuracy_importance_all = np.array(accuracy_importance_all)\n",
    "\n",
    "# plt.pcolormesh(np.transpose(accuracy_importance_all), cmap='cividis')\n",
    "# plt.yticks(ticks=np.arange(0.5,len(inputs_names2)), labels=inputs_names2)\n",
    "# plt.xticks(ticks=np.arange(0.5,len(boxnames)), labels=boxnames)\n",
    "# plt.show()\n",
    "\n",
    "# recall_importance_all = np.array(recall_importance_all)\n",
    "# plt.pcolormesh(np.transpose(recall_importance_all), cmap='cividis')\n",
    "# plt.yticks(ticks=np.arange(0.5,len(inputs_names2)), labels=inputs_names2)\n",
    "# plt.xticks(ticks=np.arange(0.5,len(boxnames)), labels=boxnames)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4b847",
   "metadata": {},
   "source": [
    "## Testing Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "classes2 = classes.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "inputs_test, targets_test, indx_test, regions = datasets_preparation(dataset, classes2, inputs_names, regions0)\n",
    "\n",
    "predictions_test = np.full(targets_test.shape,np.nan) # size of targets without nans\n",
    "\n",
    "for i in range(0, len(boxes)):\n",
    "\n",
    "    indx2 = np.where(regions==i)\n",
    "    inputs2 = inputs_test[indx2[0],:]\n",
    "    targets2 = targets_test[indx2[0]]\n",
    "\n",
    "    predictions2 = clf_all[i].predict(inputs2)\n",
    "    predictions_test[indx2[0]] = predictions2\n",
    "\n",
    "    print('Classification Report (Testing)')\n",
    "    print('\\n')\n",
    "\n",
    "    metrics_all_test = []\n",
    "    cm_all_test = []\n",
    "\n",
    "    print (boxnames[i])\n",
    "\n",
    "    print(classification_report(targets2, predictions2, target_names=targets_names))\n",
    "    metrics = precision_recall_fscore_support(targets2, predictions2, labels = [0,1,2,3])\n",
    "    metrics_all_test.append(np.array(metrics))\n",
    "\n",
    "    cm = confusion_matrix(targets2, predictions2, labels = [0,1,2,3])\n",
    "    cm_all_test.append(cm)\n",
    "    ConfusionMatrixDisplay.from_predictions(targets2, predictions2, display_labels=targets_names, colorbar=False)\n",
    "    plt.title('Confusion Matrix for ' + boxnames[i])\n",
    "    plt.show()\n",
    "\n",
    "    metrics_all_test = np.array(metrics_all_test)\n",
    "    cm_all_test = np.array(cm_all_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e11d0",
   "metadata": {},
   "source": [
    "## Feature Importance (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac755989",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_importance_all = []\n",
    "recall_importance_all = []\n",
    "\n",
    "inputs_names2 = ['SWR', 'LWR', 'TP', 'AP', 'AT', 'SH', 'WS', 'Lat', 'Lon', 'Day']\n",
    "\n",
    "for i in tqdm(range (0, len(boxes))):\n",
    "\n",
    "    indx2 = np.where(regions==i)\n",
    "    inputs2 = inputs_test[indx2[0],:]\n",
    "    targets2 = targets_test[indx2[0]]\n",
    "\n",
    "    importances_all = permutation_importance(clf_all[i], inputs2, targets2, n_repeats=5, scoring=('accuracy','recall_macro'))\n",
    "\n",
    "    accuracy_importance = pd.Series(importances_all['accuracy'].importances_mean)\n",
    "    accuracy_importance_all.append(accuracy_importance)\n",
    "\n",
    "    recall_importance = pd.Series(importances_all['recall_macro'].importances_mean)\n",
    "    recall_importance_all.append(recall_importance)\n",
    "\n",
    "accuracy_importance_all = np.array(accuracy_importance_all)\n",
    "\n",
    "plt.pcolormesh(np.transpose(accuracy_importance_all), cmap='cividis')\n",
    "plt.yticks(ticks=np.arange(0.5,len(inputs_names2)), labels=inputs_names2)\n",
    "plt.xticks(ticks=np.arange(0.5,len(boxnames)), labels=boxnames)\n",
    "plt.show()\n",
    "\n",
    "recall_importance_all = np.array(recall_importance_all)\n",
    "plt.pcolormesh(np.transpose(recall_importance_all), cmap='cividis')\n",
    "plt.yticks(ticks=np.arange(0.5,len(inputs_names2)), labels=inputs_names2)\n",
    "plt.xticks(ticks=np.arange(0.5,len(boxnames)), labels=boxnames)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825ac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
