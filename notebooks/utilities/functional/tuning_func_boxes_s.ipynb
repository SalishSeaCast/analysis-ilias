{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the optimal parameters for Historical Linear Regression algorithm (spatial means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.clustering import KMeans\n",
    "\n",
    "from skfda.misc.hat_matrix import NadarayaWatsonHatMatrix, LocalLinearRegressionHatMatrix, KNeighborsHatMatrix\n",
    "from skfda.preprocessing.smoothing import KernelSmoother\n",
    "\n",
    "from skfda.ml.regression import HistoricalLinearRegression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, regions, name, inputs_names):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    inputs = []\n",
    "    \n",
    "    for i in inputs_names:\n",
    "        inputs.append(dataset2[i].to_numpy().reshape(*dataset2[i].to_numpy().shape[:1],-1))\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    regions = np.tile(np.ravel(regions), len(dataset.time_counter))\n",
    "    regions = regions[indx[0]]\n",
    "\n",
    "    return(inputs, targets, indx, regions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, j, lag):\n",
    "\n",
    "    temp_inputs = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    temp_targets = np.ravel(targets)\n",
    "\n",
    "    # Scaling the inputs\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), np.arange(0,len(inputs))))\n",
    "    temp_inputs = scaler_inputs.fit_transform(temp_inputs)\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    inputs = np.reshape(temp_inputs,(len(inputs),inputs.shape[1],inputs.shape[2]))   \n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp_targets = np.expand_dims(temp_targets,-1)\n",
    "    temp_targets = scaler_targets.fit_transform(temp_targets)\n",
    "    targets = temp_targets.reshape(targets.shape)\n",
    "\n",
    "    # Final transformations\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10))\n",
    "    kernel_estimator = LocalLinearRegressionHatMatrix(bandwidth=1)\n",
    "    smoother = KernelSmoother(kernel_estimator=kernel_estimator)\n",
    "    inputs = smoother.fit_transform(inputs)\n",
    "\n",
    "    model = HistoricalLinearRegression(n_intervals=3, lag=lag)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets,smoother)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets,smoother):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    inputs = smoother.transform(inputs)\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # Post-processing of predictions\n",
    "    predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_training(dataset,dataset2,boxes,regions0,name,inputs_names):\n",
    "\n",
    "    regions_indiv_t = np.zeros((len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),len(boxes)))\n",
    "    regions_indiv_d = np.zeros((len(inputs_names),len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),len(boxes)))\n",
    "\n",
    "    ds = dataset\n",
    "    ds2 = dataset2\n",
    "\n",
    "    for i in range(0, len(np.unique(ds.time_counter.dt.year))):\n",
    "\n",
    "        dataset = ds.sel(time_counter = slice(str(np.unique(ds.time_counter.dt.year)[i]), str(np.unique(ds.time_counter.dt.year)[i])))\n",
    "        dataset2 = ds2.sel(time_counter = slice(str(np.unique(ds2.time_counter.dt.year)[i]), str(np.unique(ds2.time_counter.dt.year)[i])))\n",
    "\n",
    "        inputs, targets, indx, _ = datasets_preparation(dataset, dataset2, regions0, name, inputs_names)\n",
    "\n",
    "        regions1 = np.ravel(regions0)[indx]\n",
    "\n",
    "        for j in range (0,len(boxes)):\n",
    "\n",
    "            temp = xr.where(regions1==j, inputs, np.nan)\n",
    "            regions_indiv_d[:,:,i,j] = np.nanmean(temp,axis=2)\n",
    "\n",
    "            temp = xr.where(regions1==j, targets, np.nan)\n",
    "            regions_indiv_t[:,i,j] = np.nanmean(temp,axis=1)\n",
    "\n",
    "    return(regions_indiv_d,regions_indiv_t,indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(ax, corn, colour):\n",
    "\n",
    "    ax.plot([corn[2], corn[3], corn[3], corn[2], corn[2]], \n",
    "    [corn[0], corn[0], corn[1], corn[1], corn[0]], '-', color=colour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Flagellate'\n",
    "units = '[mmol m-2]'\n",
    "category = 'Concentrations'\n",
    "\n",
    "if name == 'Diatom':\n",
    "    inputs_names = ['Summation_of_solar_radiation','Mean_wind_speed','Mean_air_temperature']\n",
    "else:\n",
    "    inputs_names = ['Summation_of_solar_radiation','Mean_air_temperature','Mean_pressure', 'Mean_precipitation', 'Mean_specific_humidity']\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 9))\n",
    "mycmap = cm.deep\n",
    "mycmap.set_bad('grey')\n",
    "ax.pcolormesh(ds['Diatom'][0], cmap=mycmap)\n",
    "sa_vi.set_aspect(ax)\n",
    "\n",
    "SoG_north = [650, 730, 100, 200]\n",
    "plot_box(ax, SoG_north, 'g')\n",
    "SoG_center = [450, 550, 200, 300]\n",
    "plot_box(ax, SoG_center, 'b')\n",
    "Fraser_plume = [380, 460, 260, 330]\n",
    "plot_box(ax, Fraser_plume, 'm')\n",
    "SoG_south = [320, 380, 280, 350]\n",
    "plot_box(ax, SoG_south, 'k')\n",
    "Haro_Boundary = [290, 350, 210, 280]\n",
    "plot_box(ax, Haro_Boundary, 'm')\n",
    "JdF_west = [250, 425, 25, 125]\n",
    "plot_box(ax, JdF_west, 'c')\n",
    "JdF_east = [200, 290, 150, 260]\n",
    "plot_box(ax, JdF_east, 'w')\n",
    "PS_all = [0, 200, 80, 320]\n",
    "plot_box(ax, PS_all, 'm')\n",
    "PS_main = [20, 150, 200, 280]\n",
    "plot_box(ax, PS_main, 'r')\n",
    "\n",
    "boxnames = ['SoG_north','SoG_center','Fraser_plume','SoG_south', 'Haro_Boundary', 'JdF_west', 'JdF_east', 'PS_all', 'PS_main']\n",
    "fig.legend(boxnames)\n",
    "\n",
    "boxes = [SoG_north,SoG_center,Fraser_plume,SoG_south,Haro_Boundary,JdF_west,JdF_east,PS_all,PS_main]\n",
    "\n",
    "regions0 = np.full((len(ds.y),len(ds.x)),np.nan)\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "    regions0[boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]] = i\n",
    "\n",
    "regions0 = xr.DataArray(regions0,dims = ['y','x'])\n",
    "\n",
    "# # Low resolution\n",
    "# temp = []\n",
    "\n",
    "# for i in boxes:\n",
    "#     temp.append([x//5 for x in i])\n",
    "\n",
    "# boxes = temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low resolution\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "#     x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "# regions0 = regions0.isel(y=(np.arange(regions0.y[0], regions0.y[-1], 5)), \n",
    "#     x=(np.arange(regions0.x[0], regions0.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "r_inputs = np.zeros((len(boxnames), len(inputs_names)))\n",
    "\n",
    "inputs,targets,indx = pre_training(dataset,dataset2,boxes,regions0,name,inputs_names)\n",
    "\n",
    "# Testing\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "inputs_test,targets_test,indx_test = pre_training(dataset,dataset2,boxes,regions0,name,inputs_names)\n",
    "\n",
    "# Lags\n",
    "lags = [24.6,49.3,74]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = np.zeros((len(boxes),len(lags))) \n",
    "rms_train = np.zeros((len(boxes),len(lags))) \n",
    "slope_train = np.zeros((len(boxes),len(lags))) \n",
    "\n",
    "r_test = np.zeros((len(boxes),len(lags))) \n",
    "rms_test = np.zeros((len(boxes),len(lags))) \n",
    "slope_test = np.zeros((len(boxes),len(lags))) \n",
    "\n",
    "for i in range(0,len(boxes)):\n",
    "\n",
    "    for j in range(0, len(lags)):\n",
    "\n",
    "        inputs2 = inputs[:,:,:,i] # inputs of the i cluster\n",
    "        targets2 = targets[:,:,i] # targets of the i cluster\n",
    "        regr, scaler_inputs,scaler_targets,smoother = regressor(inputs2,targets2,i,lags[j])\n",
    "\n",
    "        predictions= scaling(regr,inputs2,scaler_inputs,targets2,scaler_targets,smoother) # putting them in the right place\n",
    "\n",
    "        r_train[i,j] = np.corrcoef(np.ravel(targets2),np.ravel(predictions))[0][1]\n",
    "        rms_train[i,j] = rmse(np.ravel(targets2),np.ravel(predictions))\n",
    "        m,_ = np.polyfit(np.ravel(targets2), np.ravel(predictions), deg=1)\n",
    "        slope_train[i,j]= np.round(m,3)\n",
    "\n",
    "        inputs2 = inputs_test[:,:,:,i] # inputs of the i cluster\n",
    "        targets2 = targets_test[:,:,i] # targets of the i cluster\n",
    "\n",
    "        predictions_test =  scaling(regr,inputs2,scaler_inputs,targets2,scaler_targets,smoother) # putting them in the right place\n",
    "\n",
    "        r_test[i,j] = np.corrcoef(np.ravel(targets2),np.ravel(predictions_test))[0][1]\n",
    "        rms_test[i,j] = rmse(np.ravel(targets2),np.ravel(predictions_test))\n",
    "        m,_ = np.polyfit(np.ravel(targets2), np.ravel(predictions_test), deg=1)\n",
    "        slope_test[i,j]= np.round(m,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(boxes)):\n",
    "\n",
    "    print('The best correlation coefficient for training for region ' +boxnames[i]+ ' is with lag: ' +str(lags[np.argmax(r_train[i])]))\n",
    "    print('The best root mean square error for training for region ' +boxnames[i]+ ' is with lag: ' +str(lags[np.argmin(rms_train[i])]))\n",
    "\n",
    "    print('The best correlation coefficient for testing for region ' +boxnames[i]+ ' is with lag: ' +str(lags[np.argmax(r_test[i])]))\n",
    "    print('The best root mean square error for testing for region ' +boxnames[i]+ ' is with lag: ' +str(lags[np.argmin(rms_test[i])]))\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
