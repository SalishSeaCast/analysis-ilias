{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.representation.basis import FourierBasis\n",
    "from skfda.misc.hat_matrix import NadarayaWatsonHatMatrix\n",
    "from skfda.ml.regression import KernelRegression\n",
    "from skfda.ml.regression import FPLSRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_preparation(dataset, dataset2, name):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    day = np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y))\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation_Short'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation_Short'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_wind_speed'].to_numpy().reshape(*dataset2['Mean_wind_speed'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Summation_of_solar_radiation_Long'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation_Long'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_pressure'].to_numpy().reshape(*dataset2['Mean_pressure'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_precipitation'].to_numpy().reshape(*dataset2['Mean_precipitation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_specific_humidity'].to_numpy().reshape(*dataset2['Mean_specific_humidity'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Latitude'].to_numpy().reshape(*dataset2['Latitude'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Longitude'].to_numpy().reshape(*dataset2['Longitude'].to_numpy().shape[:1],-1),\n",
    "        ])\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Grouping all the years (amount of days for one year * amount of grid boxes)\n",
    "    inputs = np.concatenate(inputs,axis=2)\n",
    "    targets = np.concatenate(targets,axis=1)\n",
    "\n",
    "    x = np.tile(dataset.x, len(np.unique(dataset.time_counter.dt.year))*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(np.unique(dataset.time_counter.dt.year)))\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    return(inputs, targets, indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_train(inputs,targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((MinMaxScaler(), [0,1,2,3,4,5,6,7,8]))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))   \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp = np.ravel(targets)\n",
    "    temp = np.expand_dims(temp,-1)\n",
    "    temp = scaler_targets.fit_transform(temp)\n",
    "    targets = temp.reshape(targets.shape)\n",
    "\n",
    "    return(inputs,scaler_inputs,targets,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_test(regr,inputs,scaler_inputs,targets,scaler_targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # # Post-processing of predictions\n",
    "    # predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    # predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Training with all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs0, targets0, table):\n",
    "\n",
    "    inputs,scaler_inputs,targets,scaler_targets = scaling_train(inputs0,targets0)\n",
    "\n",
    "    # Final transformations\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    model = FPLSRegression(35)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    predictions = scaling_test(regr,inputs0,scaler_inputs,targets0,scaler_targets)\n",
    "    \n",
    "    table[0,0] = np.round(np.corrcoef(np.ravel(predictions),np.ravel(targets0))[0][1],3)\n",
    "    table[1,0] = rmse(np.ravel(predictions),np.ravel(targets0))\n",
    "    m,_ = np.polyfit(np.ravel(targets0), np.ravel(predictions), deg=1)\n",
    "    table[2,0] = np.round(m,3)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Training with 75%, testing with 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor2 (inputs, targets, table):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.transpose(inputs,(1,0,2)), targets, test_size=0.25)\n",
    "\n",
    "    X_train = np.transpose(X_train,(1,0,2))\n",
    "    X_test = np.transpose(X_test,(1,0,2))\n",
    "    \n",
    "    X_train,scaler_inputs,y_train,scaler_targets = scaling_train(X_train,y_train)\n",
    "\n",
    "    # Final transformations\n",
    "    y_train = y_train.trans\n",
    "    X_train = FDataGrid(data_matrix=X_train, grid_points=np.arange(0,len(y_train[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    model = FPLSRegression(35)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    predictions = regr.predict(X_train)\n",
    "\n",
    "    table[0,2] = np.round(np.corrcoef(y_train,predictions)[0][1],3)\n",
    "    table[1,2] = rmse(y_train,predictions)\n",
    "    m,_ = np.polyfit(np.ravel(y_train),np.ravel(predictions), deg=1)\n",
    "    table[2,2] = np.round(m,3)\n",
    "    \n",
    "    predictions = scaling_test(regr,X_test,scaler_inputs,y_test,scaler_targets)\n",
    "\n",
    "    table[0,3] = np.round(np.corrcoef(y_test,predictions)[0][1],3)\n",
    "    table[1,3] = rmse(y_test,predictions)\n",
    "    m,_ = np.polyfit(np.ravel(y_test),np.ravel(predictions), deg=1)\n",
    "    table[2,3] = np.round(m,3)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (4 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor3 (inputs0, targets0, table):\n",
    "\n",
    "    inputs,scaler_inputs,targets,scaler_targets = scaling_train(inputs0,targets0)\n",
    "\n",
    "    # Final transformations\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    model = FPLSRegression(35)\n",
    "    regr = model\n",
    "\n",
    "    kf = KFold(n_splits=4,shuffle=True)\n",
    "    predictions = cross_val_predict(regr, inputs, targets, cv=kf)\n",
    "    scores = cross_validate(regr, inputs, targets, cv=4, scoring=('r2', 'neg_root_mean_squared_error'), return_train_score=True)\n",
    "\n",
    "    table[0,5:9] =  np.round(np.sqrt(np.abs(scores['train_r2'])),3)\n",
    "    table[1,5:9] =  np.abs(scores['train_neg_root_mean_squared_error'])\n",
    "\n",
    "    table[0,9:13] =  np.round(np.sqrt(np.abs(scores['test_r2'])),3)\n",
    "    table[1,9:13] =  np.abs(scores['test_neg_root_mean_squared_error'])\n",
    "\n",
    "    table[0,13] = np.round(np.corrcoef(np.ravel(predictions),np.ravel(targets))[0][1],3)\n",
    "    table[1,13] = rmse(np.ravel(predictions),np.ravel(targets))\n",
    "    m,_ = np.polyfit(np.ravel(targets), np.ravel(predictions), deg=1)\n",
    "    table[2,13] = np.round(m,3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (2021-2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (ds, ds2, regr, scaler_inputs, scaler_targets, name, table, i):\n",
    "\n",
    "    dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "    dataset2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "    inputs, targets, _ = datasets_preparation(dataset, dataset2, name)\n",
    "\n",
    "    predictions = scaling_test(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "\n",
    "    table[0,i] = np.round(np.corrcoef(np.ravel(predictions),np.ravel(targets))[0][1],3)\n",
    "    table[1,i] = rmse(np.ravel(predictions),np.ravel(targets))\n",
    "    m,_ = np.polyfit(np.ravel(targets), np.ravel(predictions), deg=1)\n",
    "    table[2,i] = np.round(m,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing(table,criteria,categories,metric):\n",
    "\n",
    "    temp = pd.DataFrame(table.transpose(),columns=criteria,index=categories)\n",
    "    print(metric)\n",
    "    display(temp)\n",
    "    print ('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name,table):\n",
    "\n",
    "    ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "    ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "    ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "        x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "    ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "        x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    " \n",
    "    dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "    dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "    inputs, targets, _ = datasets_preparation(dataset, dataset2, name)\n",
    "\n",
    "    # regr,scaler_inputs,scaler_targets = regressor(inputs, targets, table)\n",
    "    # evaluation(ds, ds2, regr, scaler_inputs, scaler_targets, name, table, 1)\n",
    "\n",
    "    # regr2,scaler_inputs,scaler_targets = regressor2(inputs, targets, table)\n",
    "    # evaluation(ds, ds2, regr2, scaler_inputs, scaler_targets, name, table, 4)\n",
    "\n",
    "    regressor3(inputs, targets, table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = ['r','rms','slope']\n",
    "categories = ['training with 100%', 'testing', 'training with 75%', 'testing with 25%', 'testing', '1st fold train','2nd fold train', '3rd fold train','4th fold train',\n",
    "    '1st fold test', '2nd fold test', '3rd fold test', '4th fold test', 'overall cross-val']\n",
    "\n",
    "diat = np.zeros((len(criteria),len(categories)))\n",
    "flag = np.zeros((len(criteria),len(categories)))\n",
    "diat_pr = np.zeros((len(criteria),len(categories)))\n",
    "flag_pr = np.zeros((len(criteria),len(categories)))\n",
    "\n",
    "training('Diatom',diat)\n",
    "# training('Flagellate',flag)\n",
    "# training('Diatom_Production_Rate',diat_pr)\n",
    "# training('Flagellate_Production_Rate',flag_pr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing (Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing(diat,criteria, categories,'Diatom')\n",
    "printing(flag,criteria, categories,'Flagellate')\n",
    "printing(diat_pr,criteria, categories,'Diatom production rate')\n",
    "printing(flag_pr,criteria, categories, 'Flagellate production rate')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
