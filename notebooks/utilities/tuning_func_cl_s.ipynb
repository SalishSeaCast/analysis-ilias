{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the optimal parameters for Historical Linear Regression algorithm (spatial means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.clustering import KMeans\n",
    "\n",
    "from skfda.ml.regression import FPLSRegression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, clusters, name):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_pressure'].to_numpy().reshape(*dataset2['Mean_pressure'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_precipitation'].to_numpy().reshape(*dataset2['Mean_precipitation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_specific_humidity'].to_numpy().reshape(*dataset2['Mean_specific_humidity'].to_numpy().shape[:1],-1),\n",
    "        ])\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    clusters = np.tile(np.ravel(clusters), len(dataset.time_counter))\n",
    "    clusters = clusters[indx[0]]\n",
    "\n",
    "    return(inputs, targets, indx, clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Finalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(dataset,quant,indx,name):\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    clusters = kmeans.fit_predict(quant)\n",
    "\n",
    "    # Sorting so that cluster 1 has the minimum mean target value, 6 the maximum\n",
    "\n",
    "        # Finding the mean of each cluster\n",
    "    if name == 'inputs':\n",
    "        cluster_mean_all = np.mean(kmeans.cluster_centers_.data_matrix,axis=1)\n",
    "        cluster_mean = cluster_mean_all[:,0]  # Sorted based on the first input\n",
    "    else:\n",
    "        cluster_mean = np.squeeze(np.mean(kmeans.cluster_centers_.data_matrix,axis=1))\n",
    "\n",
    "        # The index to sort the clusters\n",
    "    indx3 = np.argsort(np.argsort(cluster_mean)) # For the complete map we need the double np.argsort\n",
    "\n",
    "        # Sorting\n",
    "    for j in np.arange(0,len(np.unique(clusters))):\n",
    "        clusters = xr.where(kmeans.labels_==j, indx3[j], clusters)\n",
    "\n",
    "    unique, _ = np.unique(clusters, return_counts=True)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "    clusters2 = xr.DataArray(clusters,dims = ['y','x'])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize =(5,9))\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "    cmap.set_bad('gray')\n",
    "    clus = clusters2.plot(ax=ax, cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "\n",
    "    cbar = fig.colorbar(clus, ticks = unique+0.5) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    cbar.set_label('Clusters [count]')\n",
    "    ax.set_title('Functional Clustering for '+ name + ' (2007-2020)')\n",
    "\n",
    "    sa_vi.set_aspect(ax)\n",
    "    plt.show()\n",
    "\n",
    "    return(clusters,n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_target(dataset, name):\n",
    "\n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    targets = targets.transpose()\n",
    "    targets2 = FDataGrid(targets)\n",
    "\n",
    "    clusters, n_clusters = clustering(dataset,targets2,indx,name)\n",
    "\n",
    "    return(clusters,0,n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (Drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_drivers(dataset,dataset2,name):\n",
    "\n",
    "    indx = np.where((dataset2.time_counter.dt.month==2) & (dataset2.time_counter.dt.day==29))\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_pressure'].to_numpy().reshape(*dataset2['Mean_pressure'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_precipitation'].to_numpy().reshape(*dataset2['Mean_precipitation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_specific_humidity'].to_numpy().reshape(*dataset2['Mean_specific_humidity'].to_numpy().shape[:1],-1),\n",
    "        ])\n",
    "    \n",
    "    targets = dataset['Diatom'].to_numpy().reshape(*dataset['Diatom'].to_numpy().shape[:1],-1)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset2.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880))) # Target goes down to 100m\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), [0,1,2,3,4]))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2])) \n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs2 = FDataGrid(inputs, np.arange(0,len(inputs[0])))\n",
    "\n",
    "    clusters, n_clusters = clustering(dataset2,inputs2,indx,name)\n",
    "\n",
    "    return(clusters, 1, n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets,name,i,lag):\n",
    "\n",
    "    temp_inputs = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    temp_targets = np.ravel(targets)\n",
    "\n",
    "    # Scaling the inputs\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), [0,1,2,3,4]))\n",
    "    temp_inputs = scaler_inputs.fit_transform(temp_inputs)\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    inputs = np.reshape(temp_inputs,(len(inputs),inputs.shape[1],inputs.shape[2]))   \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp_targets = np.expand_dims(temp_targets,-1)\n",
    "    temp_targets = scaler_targets.fit_transform(temp_targets)\n",
    "    targets = temp_targets.reshape(targets.shape)\n",
    "\n",
    "     # Final transformations\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    ## Smoothing\n",
    "    from skfda.representation.basis import VectorValuedBasis,FourierBasis\n",
    "\n",
    "    # x_basis = np.tile(FourierBasis(domain_range=(0,74),n_basis=10),5)\n",
    "    # basis = VectorValuedBasis(x_basis)\n",
    "    # inputs = inputs.to_basis(basis)\n",
    "    \n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10))\n",
    "\n",
    "    from skfda.ml.regression import HistoricalLinearRegression,LinearRegression,FPCARegression\n",
    "\n",
    "    model = HistoricalLinearRegression(n_intervals=3,lag=lag)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_training(dataset,dataset2,n_clusters,clusters0,name):\n",
    "\n",
    "    np.warnings.filterwarnings('ignore') # For the nan mean warning\n",
    "\n",
    "    clusters_indiv_t = np.zeros((len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),n_clusters))\n",
    "    clusters_indiv_d = np.zeros((5,len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),n_clusters))\n",
    "\n",
    "    ds = dataset\n",
    "    ds2 = dataset2\n",
    "\n",
    "    for i in range(0, len(np.unique(ds.time_counter.dt.year))):\n",
    "\n",
    "        dataset = ds.sel(time_counter = slice(str(np.unique(ds.time_counter.dt.year)[i]), str(np.unique(ds.time_counter.dt.year)[i])))\n",
    "        dataset2 = ds2.sel(time_counter = slice(str(np.unique(ds2.time_counter.dt.year)[i]), str(np.unique(ds2.time_counter.dt.year)[i])))\n",
    "\n",
    "        inputs, targets, indx, _ = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "        inputs1 = np.squeeze(inputs)\n",
    "        targets1 = np.squeeze(targets)\n",
    "        clusters1 = np.ravel(clusters0)[indx]\n",
    "\n",
    "        for j in range (0,n_clusters):\n",
    "\n",
    "            temp = xr.where(clusters1==j, inputs1, np.nan)\n",
    "            clusters_indiv_d[:,:,i,j] = np.nanmean(temp,axis=2)\n",
    "\n",
    "            temp = xr.where(clusters1==j, targets1, np.nan)\n",
    "            clusters_indiv_t[:,i,j] = np.nanmean(temp,axis=1)\n",
    "\n",
    "    return(clusters_indiv_d,clusters_indiv_t,indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # Post-processing of predictions\n",
    "    predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates,mean_targets,mean_predictions,category,units,region):\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "    ticks = [0]\n",
    "    \n",
    "    fig, _ = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(mean_targets)\n",
    "    mean_predictions = np.ma.array(mean_predictions)\n",
    "\n",
    "    for year in years[:-1]:\n",
    "        ticks.append((np.where(dates.year==year)[0][-1]+1))\n",
    "        mean_targets[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "        mean_predictions[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "\n",
    "    plt.plot(mean_targets, label = 'targets')\n",
    "    plt.plot(mean_predictions, label = 'predictions')\n",
    "    plt.xlabel('Years')\n",
    "    plt.xticks(ticks,years)\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region)\n",
    "    plt.legend()\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Flagellate'\n",
    "units = '[mmol m-2]'\n",
    "category = 'Concentrations'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "#     x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "dataset_test = ds.sel(time_counter = slice('2021', '2024'))\n",
    "dataset2_test = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "# Selecting the clustering input (drivers or target)\n",
    "clusters0, id, n_clusters = func_clust_target(dataset, name)\n",
    "# clusters0, id, n_clusters = func_clust_drivers(dataset,dataset2,'inputs')\n",
    "\n",
    "inputs,targets,indx = pre_training(dataset,dataset2,n_clusters,clusters0,name)\n",
    "\n",
    "inputs_test,targets_test,indx = pre_training(dataset_test, dataset2_test, n_clusters, clusters0, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = np.zeros((n_clusters,targets.shape[0]))\n",
    "rms_train = np.zeros((n_clusters,targets.shape[0]))\n",
    "slope_train = np.zeros((n_clusters,targets.shape[0]))\n",
    "\n",
    "r_test = np.zeros((n_clusters,targets.shape[0]))\n",
    "rms_test = np.zeros((n_clusters,targets.shape[0]))\n",
    "slope_test = np.zeros((n_clusters,targets.shape[0]))\n",
    "\n",
    "for i in range (0,n_clusters):\n",
    "\n",
    "    inputs2 = inputs[:,:,:,i] # inputs of the i cluster\n",
    "    targets2 = targets[:,:,i] # targets of the i cluster\n",
    "\n",
    "    inputs2_test = inputs_test[:,:,:,i] # inputs of the i cluster\n",
    "    targets2_test = targets_test[:,:,i] # targets of the i cluster\n",
    "\n",
    "    for lag in tqdm(range(1,76)):\n",
    "\n",
    "        regr, scaler_inputs,scaler_targets = regressor(inputs2,targets2,name,i,lag)\n",
    "\n",
    "        predictions = scaling(regr,inputs2,scaler_inputs,targets2,scaler_targets) # putting them in the right place\n",
    "\n",
    "        r_train[i,lag-1] = np.corrcoef(np.ravel(targets2),np.ravel(predictions))[0][1]\n",
    "        rms_train[i,lag-1] = rmse(np.ravel(targets2),np.ravel(predictions))\n",
    "        m,_ = np.polyfit(np.ravel(targets2), np.ravel(predictions), deg=1)\n",
    "        slope_train[i,lag-1]= np.round(m,3)\n",
    "\n",
    "        predictions = scaling(regr,inputs2_test,scaler_inputs,targets2_test,scaler_targets)\n",
    "\n",
    "        r_test[i,lag-1] = np.corrcoef(np.ravel(targets2_test),np.ravel(predictions))[0][1]\n",
    "        rms_test[i,lag-1] = rmse(np.ravel(targets2_test),np.ravel(predictions))\n",
    "        m,_ = np.polyfit(np.ravel(targets2_test), np.ravel(predictions), deg=1)\n",
    "        slope_test[i,lag-1] = np.round(m,3)\n",
    "\n",
    "    print('The best correlation coefficient for training for cluster ' +str(i+1)+ ' is with number of interval: ' +str(np.argmax(r_train[i])+1))\n",
    "    print('The best root mean square error for training for cluster ' +str(i+1)+ ' is with number of interval: ' +str(np.argmin(rms_train[i])+1))\n",
    "\n",
    "    print('The best correlation coefficient for testing for cluster ' +str(i+1)+ ' is with number of interval: ' +str(np.argmax(r_test[i])+1))\n",
    "    print('The best root mean square error for testing for cluster ' +str(i+1)+ ' is with number of interval: ' +str(np.argmin(rms_test[i])+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
