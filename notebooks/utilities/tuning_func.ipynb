{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the optimal parameters for Historical Linear Regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.clustering import KMeans\n",
    "\n",
    "from skfda.ml.regression import FPLSRegression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, name):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    day = np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y)) # if needed\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_wind_speed'].to_numpy().reshape(*dataset2['Mean_wind_speed'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Latitude'].to_numpy().reshape(*dataset2['Latitude'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Longitude'].to_numpy().reshape(*dataset2['Longitude'].to_numpy().shape[:1],-1),\n",
    "        ])\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Grouping all the years (amount of days for one year * amount of grid boxes)\n",
    "    inputs = np.concatenate(inputs,axis=2)\n",
    "    targets = np.concatenate(targets,axis=1)\n",
    "\n",
    "    x = np.tile(dataset.x, len(np.unique(dataset.time_counter.dt.year))*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(np.unique(dataset.time_counter.dt.year)))\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "\n",
    "    return(inputs, targets, indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, name,i):\n",
    "\n",
    "    temp_inputs = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    temp_targets = np.ravel(targets)\n",
    "\n",
    "    # Scaling the inputs\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), [0,1,2,3,4]))\n",
    "    temp_inputs = scaler_inputs.fit_transform(temp_inputs)\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    inputs = np.reshape(temp_inputs,(len(inputs),inputs.shape[1],inputs.shape[2]))   \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp_targets = np.expand_dims(temp_targets,-1)\n",
    "    temp_targets = scaler_targets.fit_transform(temp_targets)\n",
    "    targets = temp_targets.reshape(targets.shape)\n",
    "\n",
    "     # Final transformations\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    ## Smoothing\n",
    "    from skfda.representation.basis import VectorValuedBasis,FourierBasis\n",
    "\n",
    "    # x_basis = np.tile(FourierBasis(domain_range=(0,74),n_basis=10),5)\n",
    "    # basis = VectorValuedBasis(x_basis)\n",
    "    # inputs = inputs.to_basis(basis)\n",
    "    \n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10))\n",
    "\n",
    "    from skfda.ml.regression import HistoricalLinearRegression,LinearRegression,FPCARegression\n",
    "\n",
    "    model = HistoricalLinearRegression(n_intervals=i,lag=25)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # Post-processing of predictions\n",
    "    predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates,mean_targets,mean_predictions,category,units,region):\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "    ticks = [0]\n",
    "    \n",
    "    fig, _ = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(mean_targets)\n",
    "    mean_predictions = np.ma.array(mean_predictions)\n",
    "\n",
    "    for year in years[:-1]:\n",
    "        ticks.append((np.where(dates.year==year)[0][-1]+1))\n",
    "        mean_targets[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "        mean_predictions[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "\n",
    "    plt.plot(mean_targets, label = 'targets')\n",
    "    plt.plot(mean_predictions, label = 'predictions')\n",
    "    plt.xlabel('Years')\n",
    "    plt.xticks(ticks,years)\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region)\n",
    "    plt.legend()\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Diatom'\n",
    "units = '[mmol m-2]'\n",
    "category = 'Concentrations'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "dataset_test = ds.sel(time_counter = slice('2021', '2024'))\n",
    "dataset2_test = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "inputs, targets, indx = datasets_preparation(dataset, dataset2, name)\n",
    "\n",
    "inputs_test, targets_test, indx = datasets_preparation(dataset_test, dataset2_test, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [38:47<00:00, 2327.55s/it]\n"
     ]
    }
   ],
   "source": [
    "r_train = []\n",
    "rms_train = []\n",
    "slope_train = []\n",
    "\n",
    "r_test = []\n",
    "rms_test = []\n",
    "slope_test = []\n",
    "\n",
    "for i in tqdm(range(5,6)):\n",
    "\n",
    "    regr,scaler_inputs,scaler_targets = regressor(inputs, targets, name, i)\n",
    "\n",
    "    # predictions = scaling(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "\n",
    "    # r_train.append(np.corrcoef(np.ravel(targets),np.ravel(predictions))[0][1])\n",
    "    # rms_train.append(rmse(np.ravel(targets),np.ravel(predictions)))\n",
    "    # m,_ = np.polyfit(np.ravel(targets), np.ravel(predictions), deg=1)\n",
    "    # slope_train.append(np.round(m,3))\n",
    "\n",
    "    predictions = scaling(regr,inputs_test,scaler_inputs,targets_test,scaler_targets)\n",
    "\n",
    "    test = np.reshape(targets_test,(75,4,1838))\n",
    "    test2 = np.reshape(predictions,(75,4,1838))\n",
    "    a = np.mean(test,axis=2)\n",
    "    b = np.mean(test2,axis=2)\n",
    "    np.corrcoef(np.ravel(a),np.ravel(b))[0][1]\n",
    "\n",
    "    r_test.append(np.corrcoef(np.ravel(a),np.ravel(b))[0][1])\n",
    "    rms_test.append(rmse(np.ravel(a),np.ravel(b)))\n",
    "    m,_ = np.polyfit(np.ravel(a),np.ravel(b), deg=1)\n",
    "    slope_test.append(np.round(m,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe best correlation coefficient for training is with number of components: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(r_train\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr_train\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe best root mean square error for training is with number of components: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(rms_train\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmin\u001b[39m(rms_train))\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "print('The best correlation coefficient for training is with number of components: ' +str(r_train.index(max(r_train))+1))\n",
    "print('The best root mean square error for training is with number of components: ' +str(rms_train.index(min(rms_train))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best correlation coefficient for testing is with number of components: ' +str(r_test.index(max(r_test))+1))\n",
    "print('The best root mean square error for testing is with number of components: ' +str(rms_test.index(min(rms_test))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05059774095135961]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9181224502617207]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04222890183061277]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9432235070504909]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
