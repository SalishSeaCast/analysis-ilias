{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.representation.basis import FourierBasis\n",
    "from skfda.misc.hat_matrix import NadarayaWatsonHatMatrix\n",
    "from skfda.ml.regression import KernelRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_preparation(ds, ds2, ref, name):\n",
    "    \n",
    "    years = np.unique(ds.time_counter.dt.year)\n",
    "    test = []\n",
    "    test2 = []\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        dataset = ds.sel(time_counter=str(year))\n",
    "        dataset2 = ds2.sel(time_counter=str(year))\n",
    "\n",
    "        y = np.tile(ref.y, len(ref.time_counter)*len(ref.x))\n",
    "        x = np.tile(np.repeat(ref.x, len(ref.y)), len(ref.time_counter))\n",
    "\n",
    "        test.append(np.stack([\n",
    "            dataset2['Summation_of_solar_radiation'].to_numpy(),\n",
    "            dataset2['Mean_wind_speed'].to_numpy(),\n",
    "            dataset2['Mean_air_temperature'].to_numpy(),\n",
    "            y.reshape(dataset2['Summation_of_solar_radiation'].to_numpy().shape),\n",
    "            x.reshape(dataset2['Summation_of_solar_radiation'].to_numpy().shape)\n",
    "            ]))\n",
    "        \n",
    "        test2.append(dataset[name].to_numpy())\n",
    "\n",
    "    # Grouping all the years\n",
    "    inputs = np.concatenate(test,axis=2)\n",
    "    targets = np.concatenate(test2,axis=1)\n",
    "\n",
    "    y = np.tile(ref.y, len(np.unique(ds.time_counter.dt.year))*len(ref.x))\n",
    "    x = np.tile(np.repeat(ref.x, len(ref.y)), len(np.unique(ds.time_counter.dt.year)))\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    return(inputs, targets, indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_train(inputs,targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((MinMaxScaler(), [0,1,2]), remainder=KBinsDiscretizer(n_bins=155,encode='ordinal',strategy='uniform'))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "    \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = MinMaxScaler()\n",
    "    temp = np.ravel(targets)\n",
    "    temp = np.expand_dims(temp,-1)\n",
    "    temp = scaler_targets.fit_transform(temp)\n",
    "    targets = temp.reshape(targets.shape)\n",
    "    targets = targets.transpose()\n",
    "\n",
    "    return(inputs,scaler_inputs,targets,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_test(regr,inputs,scaler_inputs,targets,scaler_targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # # Post-processing of predictions\n",
    "    # predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    # predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Training with all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, table):\n",
    "\n",
    "    inputs,scaler_inputs,targets,scaler_targets = scaling_train(inputs,targets)\n",
    "\n",
    "    # Final transformations\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    kernel_estimator = NadarayaWatsonHatMatrix(bandwidth=1)\n",
    "    model = KernelRegression(kernel_estimator=kernel_estimator)\n",
    "    regr = model.fit(inputs, targets)\n",
    "\n",
    "    predictions = scaling_test(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "    \n",
    "    table[0,0] = np.round(np.corrcoef(predictions,targets)[0][1],3)\n",
    "    table[1,0] = rmse(predictions,targets)\n",
    "    m,_ = np.polyfit(targets, predictions, deg=1)\n",
    "    table[2,0] = np.round(m,3)\n",
    "\n",
    "    return(regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Training with 75%, testing with 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor2 (inputs, targets, table):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.25)\n",
    "    \n",
    "    X_train,scaler_inputs,y_train,scaler_targets = scaling_train(X_train,y_train)\n",
    "\n",
    "    # Final transformations\n",
    "    X_train = FDataGrid(data_matrix=X_train, grid_points=np.arange(0,len(y_train[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    kernel_estimator = NadarayaWatsonHatMatrix(bandwidth=1)\n",
    "    regr = KernelRegression(kernel_estimator=kernel_estimator)\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    predictions = scaling_test(regr,X_train,scaler_inputs,y_train,scaler_targets)\n",
    "\n",
    "    table[0,2] = np.round(np.corrcoef(y_train,predictions)[0][1],3)\n",
    "    table[1,2] = rmse(y_train,predictions)\n",
    "    m,_ = np.polyfit(y_train,predictions, deg=1)\n",
    "    table[2,2] = np.round(m,3)\n",
    "    \n",
    "    predictions = scaling_test(regr,X_test,scaler_inputs,y_test,scaler_targets)\n",
    "\n",
    "    table[0,3] = np.round(np.corrcoef(y_test,predictions)[0][1],3)\n",
    "    table[1,3] = rmse(y_test,predictions)\n",
    "    m,_ = np.polyfit(y_test,predictions, deg=1)\n",
    "    table[2,3] = np.round(m,3)\n",
    "\n",
    "    return(regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (4 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor3 (inputs, targets, table):\n",
    "\n",
    "    inputs,scaler_inputs,targets,scaler_targets = scaling_train(inputs,targets)\n",
    "\n",
    "    # Final transformations\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    kernel_estimator = NadarayaWatsonHatMatrix(bandwidth=1)\n",
    "    regr = KernelRegression(kernel_estimator=kernel_estimator)\n",
    "\n",
    "    predictions = scaling_test(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "    scores = cross_validate(regr, inputs, targets, cv=4, scoring=('r2', 'neg_root_mean_squared_error'), return_train_score=True)\n",
    "\n",
    "    table[0,5:9] =  np.round(np.sqrt(np.abs(scores['train_r2'])),3)\n",
    "    table[1,5:9] =  np.abs(scores['train_neg_root_mean_squared_error'])\n",
    "\n",
    "    table[0,9:13] =  np.round(np.sqrt(np.abs(scores['test_r2'])),3)\n",
    "    table[1,9:13] =  np.abs(scores['test_neg_root_mean_squared_error'])\n",
    "\n",
    "    table[0,13] = np.round(np.corrcoef(predictions,targets)[0][1],3)\n",
    "    table[1,13] = rmse(predictions,targets)\n",
    "    m,_ = np.polyfit(targets, predictions, deg=1)\n",
    "    table[2,13] = np.round(m,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (2021-2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (ds, ds2, regr, scaler_inputs, scaler_targets, name, table, i):\n",
    "\n",
    "    dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "    dataset2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "    inputs, targets, _ = datasets_preparation(dataset, dataset2, name)\n",
    "\n",
    "    predictions = scaling_test(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "\n",
    "    table[0,i] = np.round(np.corrcoef(predictions,targets)[0][1],3)\n",
    "    table[1,i] = rmse(predictions,targets)\n",
    "    m,_ = np.polyfit(targets, predictions, deg=1)\n",
    "    table[2,i] = np.round(m,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing(table,criteria,categories,metric):\n",
    "\n",
    "    temp = pd.DataFrame(table.transpose(),columns=criteria,index=categories)\n",
    "    print(metric)\n",
    "    display(temp)\n",
    "    print ('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of datasets (run only once!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "ref = ds.sel(time_counter = slice('2007', '2007'))\n",
    "\n",
    "ds = ds.stack(z=('x','y'))\n",
    "ds2 = ds2.stack(z=('x','y'))\n",
    "\n",
    "indx = ~((ds.time_counter.dt.month==2) & (ds.time_counter.dt.day==29))\n",
    "ds = ds.sel(time_counter=indx)\n",
    "ds2 = ds2.sel(time_counter=indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name,table):\n",
    " \n",
    "    dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "    dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "    inputs, targets, _ = datasets_preparation(dataset, dataset2, name)\n",
    "\n",
    "    regr = regressor(inputs, targets, table)\n",
    "    evaluation(ds,ds2,regr,name,table,1)\n",
    "\n",
    "    regr2 = regressor2(inputs, targets, table)\n",
    "    evaluation(ds,ds2,regr2,name,table,4)\n",
    "\n",
    "    regressor3(inputs, targets, table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = ['r','rms','slope']\n",
    "categories = ['training with 100%', 'testing', 'training with 75%', 'testing with 25%', 'testing', '1st fold train','2nd fold train', '3rd fold train','4th fold train',\n",
    "    '1st fold test', '2nd fold test', '3rd fold test', '4th fold test', 'overall cross-val']\n",
    "\n",
    "diat = np.zeros((len(criteria),len(categories)))\n",
    "flag = np.zeros((len(criteria),len(categories)))\n",
    "diat_pr = np.zeros((len(criteria),len(categories)))\n",
    "flag_pr = np.zeros((len(criteria),len(categories)))\n",
    "\n",
    "training('Diatom',diat)\n",
    "training('Flagellate',flag)\n",
    "training('Diatom_Production_Rate',diat_pr)\n",
    "training('Flagellate_Production_Rate',flag_pr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing (Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing(diat,criteria, categories,'Diatom')\n",
    "printing(flag,criteria, categories,'Flagellate')\n",
    "printing(diat_pr,criteria, categories,'Diatom production rate')\n",
    "printing(flag_pr,criteria, categories, 'Flagellate production rate')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
