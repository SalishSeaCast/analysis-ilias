{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Flagellate concentration with functional clustering and regression (daily means - removed seasonality before training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.clustering import KMeans\n",
    "\n",
    "from skfda.ml.regression import FPLSRegression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, clusters, name):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_wind_speed'].to_numpy().reshape(*dataset2['Mean_wind_speed'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1)\n",
    "        ])\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    clusters = np.tile(np.ravel(clusters), len(dataset.time_counter))\n",
    "    clusters = clusters[indx[0]]\n",
    "\n",
    "    return(inputs, targets, indx, clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Finalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(dataset,quant,indx,name):\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    clusters = kmeans.fit_predict(quant)\n",
    "\n",
    "    # Sorting so that cluster 1 has the minimum mean target value, 6 the maximum\n",
    "\n",
    "        # Finding the mean of each cluster\n",
    "    if name == 'inputs':\n",
    "        cluster_mean_all = np.mean(kmeans.cluster_centers_.data_matrix,axis=1)\n",
    "        cluster_mean = cluster_mean_all[:,0]  # Sorted based on the first input\n",
    "    else:\n",
    "        cluster_mean = np.squeeze(np.mean(kmeans.cluster_centers_.data_matrix,axis=1))\n",
    "\n",
    "        # The index to sort the clusters\n",
    "    indx3 = np.argsort(np.argsort(cluster_mean)) # For the complete map we need the double np.argsort\n",
    "\n",
    "        # Sorting\n",
    "    for j in np.arange(0,len(np.unique(clusters))):\n",
    "        clusters = xr.where(kmeans.labels_==j, indx3[j], clusters)\n",
    "\n",
    "    unique, _ = np.unique(clusters, return_counts=True)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "    clusters2 = xr.DataArray(clusters,dims = ['y','x'])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize =(5,9))\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "    cmap.set_bad('gray')\n",
    "    clus = clusters2.plot(ax=ax, cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "\n",
    "    cbar = fig.colorbar(clus, ticks = unique+0.5) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    cbar.set_label('Clusters [count]')\n",
    "    ax.set_title('Functional Clustering for '+ name + ' (2007-2020)')\n",
    "\n",
    "    sa_vi.set_aspect(ax)\n",
    "    plt.show()\n",
    "\n",
    "    return(clusters,n_clusters)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_target(dataset, name):\n",
    "\n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    targets = targets.transpose()\n",
    "    targets2 = FDataGrid(targets)\n",
    "\n",
    "    clusters, n_clusters = clustering(dataset,targets2,indx,name)\n",
    "\n",
    "    return(clusters,0,n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (Drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_drivers(dataset,dataset2,name):\n",
    "\n",
    "    indx = np.where((dataset2.time_counter.dt.month==2) & (dataset2.time_counter.dt.day==29))\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_wind_speed'].to_numpy().reshape(*dataset2['Mean_wind_speed'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1)\n",
    "        ])\n",
    "    \n",
    "    targets = dataset['Diatom'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset2.time_counter.dt.year)),axis=1)\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880))) # Target goes down to 100m\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((MinMaxScaler(), [0,1,2]))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2])) \n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs2 = FDataGrid(inputs, np.arange(0,len(inputs[0])))\n",
    "\n",
    "    clusters, n_clusters = clustering(dataset2,inputs2,indx,name)\n",
    "\n",
    "    return(clusters, 1, n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_creation(path, variable, name):\n",
    "\n",
    "    temp = variable.to_dataset(name=name)\n",
    "    temp.to_netcdf(path = path + 'targets_predictions.nc', mode='a', encoding={name:{\"zlib\": True, \"complevel\": 9}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, name, cluster):\n",
    "\n",
    "# Printing of the correlation coefficients\n",
    "    temp_inputs = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    temp_targets = np.ravel(targets)\n",
    "\n",
    "    r = np.round(r_regression(temp_inputs,temp_targets),2)\n",
    "    dict = {'Summation_of_solar_radiation':r[0], 'Mean_wind_speed':r[1], 'Mean_air_temperature':r[2]}\n",
    "    \n",
    "    print('The correlation coefficients between each input and ' + name +  ' for cluster ' + str(cluster+1) + ' are: ' +str(dict))\n",
    "\n",
    "    # Scaling the inputs\n",
    "    scaler_inputs = make_column_transformer((MinMaxScaler(), [0,1,2]))\n",
    "    temp_inputs = scaler_inputs.fit_transform(temp_inputs)\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    inputs = np.reshape(temp_inputs,(len(inputs),inputs.shape[1],inputs.shape[2]))   \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp_targets = np.expand_dims(temp_targets,-1)\n",
    "    temp_targets = scaler_targets.fit_transform(temp_targets)\n",
    "    targets = temp_targets.reshape(targets.shape)\n",
    "\n",
    "    # Final transformations\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    ## Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10)\n",
    "\n",
    "    model = FPLSRegression(n_components=14)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality(n_clusters,targets,season_id):  \n",
    "\n",
    "    # Season id 0 is one seasonality, 1 is one for each cluster\n",
    "    \n",
    "    if season_id == 0:\n",
    "\n",
    "        season = np.mean(targets,axis=(1,2))\n",
    "        plt.plot(season)\n",
    "        plt.suptitle('Long-term seasonality (2007-2020)')\n",
    "\n",
    "        season_train = np.tile(season,14) # Broadcasting season to all training years\n",
    "        season_train = np.reshape(season_train,(75,14),order='F')\n",
    "\n",
    "        season_test = np.tile(season,4) # Broadcasting season to all testing years\n",
    "        season_test = np.reshape(season_test,(75,4),order='F')\n",
    "            \n",
    "    else:\n",
    "\n",
    "        season = np.mean(targets,axis=1)\n",
    "\n",
    "        for i in range (0, n_clusters):\n",
    "\n",
    "            plt.plot(season[:,i])\n",
    "            plt.legend(('Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5','Cluster 6'))\n",
    "            plt.suptitle('Long-term seasonality (2007-2020)')\n",
    "\n",
    "        season_train = np.tile(season,14) # Broadcasting season to all training years\n",
    "        season_train = np.reshape(season_train,targets.shape)\n",
    "\n",
    "        season_test = np.tile(season,4) # Broadcasting season to all testing years\n",
    "        season_test = np.reshape(season_test,(75,4,6))\n",
    "    \n",
    "    return(season_train,season_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # # Post-processing of predictions\n",
    "    # predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    # predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(dates,targets, predictions, name):\n",
    "\n",
    "    indx = pd.DatetimeIndex(dates[0:75]) # From the first year\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, predictions, deg=1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter = ax.scatter(targets,predictions, s = 10, c= indx.month)\n",
    "\n",
    "    lims = [np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "        np.max([ax.get_xlim(), ax.get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax.axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax.set_xlabel('targets')\n",
    "    ax.set_ylabel('predictions')\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=['February','March','April'])\n",
    "\n",
    "    ax.plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_training(dataset,dataset2,n_clusters,clusters0,name):\n",
    "\n",
    "    np.warnings.filterwarnings('ignore') # For the nan mean warning\n",
    "\n",
    "    clusters_indiv_t = np.zeros((len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),n_clusters))\n",
    "    clusters_indiv_d = np.zeros((3,len(np.unique(dataset.time_counter.dt.dayofyear))-1,len(np.unique(dataset.time_counter.dt.year)),n_clusters))\n",
    "\n",
    "    ds = dataset\n",
    "    ds2 = dataset2\n",
    "\n",
    "    for i in range(0, len(np.unique(ds.time_counter.dt.year))):\n",
    "\n",
    "        dataset = ds.sel(time_counter = slice(str(np.unique(ds.time_counter.dt.year)[i]), str(np.unique(ds.time_counter.dt.year)[i])))\n",
    "        dataset2 = ds2.sel(time_counter = slice(str(np.unique(ds2.time_counter.dt.year)[i]), str(np.unique(ds2.time_counter.dt.year)[i])))\n",
    "\n",
    "        inputs, targets, indx, _ = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "        inputs1 = np.squeeze(inputs)\n",
    "        targets1 = np.squeeze(targets)\n",
    "        clusters1 = np.ravel(clusters0)[indx]\n",
    "\n",
    "        for j in range (0,n_clusters):\n",
    "\n",
    "            temp = xr.where(clusters1==j, inputs1, np.nan)\n",
    "            clusters_indiv_d[:,:,i,j] = np.nanmean(temp,axis=2)\n",
    "\n",
    "            temp = xr.where(clusters1==j, targets1, np.nan)\n",
    "            clusters_indiv_t[:,i,j] = np.nanmean(temp,axis=1)\n",
    "\n",
    "    return(clusters_indiv_d,clusters_indiv_t,indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(years,n_clusters,targets,predictions,r_train,rms_train,slope_train,category,units,region):\n",
    "\n",
    "    ticks = []\n",
    "    for i in range (0,targets.shape[0]*targets.shape[1],targets.shape[0]):\n",
    "        ticks.append(i)\n",
    "\n",
    "    targets = np.reshape(targets,(targets.shape[0]*targets.shape[1],targets.shape[2]), order = 'F')\n",
    "    predictions = np.reshape(predictions,(predictions.shape[0]*predictions.shape[1],predictions.shape[2]), order = 'F')\n",
    "    \n",
    "    targets_masked = np.ma.array(targets)\n",
    "    predictions_masked = np.ma.array(predictions)\n",
    "\n",
    "    targets_masked[ticks] = np.ma.masked\n",
    "    predictions_masked[ticks] = np.ma.masked\n",
    "\n",
    "    for i in range (0,n_clusters):\n",
    "\n",
    "        fig, _ = plt.subplots(figsize=(19,5))\n",
    "\n",
    "        if region == 'Salish Sea (Training)':\n",
    "            temp = pd.DataFrame(np.vstack((r_train[i],rms_train[i],slope_train[i])).transpose(),index=['Cluster '+str(i+1)],columns=['r_train','rms_train','slope_train'])\n",
    "            display(temp)\n",
    "\n",
    "        plt.plot(targets_masked[:,i], label = 'targets')\n",
    "        plt.plot(predictions_masked[:,i], label = 'predictions')\n",
    "        plt.xlabel('Years')\n",
    "        plt.xticks(ticks,years)\n",
    "        plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region + ' (Cluster '+str(i+1)+ ')')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training(n_clusters,targets,predictions):\n",
    "\n",
    "    r_train = np.full(n_clusters,np.nan)\n",
    "    rms_train = np.full(n_clusters,np.nan)\n",
    "    slope_train = np.full(n_clusters,np.nan)\n",
    "\n",
    "    for i in range (0,n_clusters):\n",
    "\n",
    "        r_train[i] = np.round(np.corrcoef(targets[:,:,i],predictions[:,:,i])[0][1],3)\n",
    "        rms_train[i] = rmse(targets[:,:,i],predictions[:,:,i])\n",
    "        m,_ = np.polyfit(np.ravel(targets[:,:,i]),np.ravel(predictions[:,:,i]), deg=1)\n",
    "        slope_train[i] = np.round(m,3)\n",
    "\n",
    "    return(r_train,rms_train,slope_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (dates,name,years,targets,predictions,id):\n",
    "\n",
    "    # For every year\n",
    "    r_years = np.array([])\n",
    "    rms_years = np.array([])\n",
    "    slope_years = np.array([])\n",
    "\n",
    "    ticks = []\n",
    "    for i in range (0,targets.shape[0]*targets.shape[1],targets.shape[0]):\n",
    "        ticks.append(i)\n",
    "        \n",
    "    for i in range (0, len(years)):\n",
    "\n",
    "        r_year = np.round(np.corrcoef(np.ravel(targets[:,i]), np.ravel(predictions[:,i]))[0][1],3)\n",
    "        rms_year = np.round(rmse(np.ravel(targets[:,i]), np.ravel(predictions[:,i])),5)\n",
    "        m,_ = np.polyfit(np.ravel(targets[:,i]), np.ravel(predictions[:,i]), deg=1)\n",
    "        slope_year = np.round(m,3)\n",
    "        \n",
    "        print ('The correlation coefficient during testing for year '+ str(years[i]) + ' is: ' + str(r_year))\n",
    "        print ('The rmse during testing for year '+ str(years[i]) + ' is: ' + str(rms_year))\n",
    "        print ('The slope of the best fitting line during testing for year '+ str(years[i]) + ' is: ' + str(slope_year))\n",
    "\n",
    "        # _ = scatter_plot(dates, targets[:,i], predictions[:,i], name + ' for '+ str(years[i]) + ' (Cluster ' + str(id) + ')') \n",
    "\n",
    "        r_years = np.append(r_years,r_year)\n",
    "        rms_years = np.append(rms_years,rms_year)\n",
    "        slope_years = np.append(slope_years,slope_year)\n",
    "\n",
    "    return(r_years, rms_years, slope_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_clusters(years,n_clusters,targets):\n",
    "\n",
    "    ticks = []\n",
    "    for i in range (0,targets.shape[0]*targets.shape[1],targets.shape[0]):\n",
    "        ticks.append(i)\n",
    "\n",
    "    targets = np.reshape(targets,(1050,6), order = 'F')\n",
    "    \n",
    "    targets_masked = np.ma.array(targets)\n",
    "\n",
    "    targets_masked[ticks] = np.ma.masked\n",
    "\n",
    "    for i in range (0,n_clusters):\n",
    "\n",
    "        fig, _ = plt.subplots(figsize=(19,5))\n",
    "\n",
    "        plt.plot(targets_masked[:,i])\n",
    "        plt.xlabel('Years')\n",
    "        plt.xticks(ticks,years)\n",
    "        plt.suptitle('Cluster '+str(i+1))\n",
    "\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Diatom'\n",
    "units = '[mmol m-2]'\n",
    "category = 'Concentrations'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "# Selecting the clustering input (drivers or target)\n",
    "clusters0, id, n_clusters = func_clust_target(dataset, name)\n",
    "# clusters0, id, n_clusters = func_clust_drivers(dataset,dataset2,'inputs')\n",
    "\n",
    "inputs,targets,indx = pre_training(dataset,dataset2,n_clusters,clusters0,name)\n",
    "\n",
    "plotting_clusters(years,n_clusters,targets)\n",
    "\n",
    "regr_all = []\n",
    "scaler_inputs_all = []\n",
    "scaler_targets_all = []\n",
    "\n",
    "predictions = np.full(targets.shape,np.nan)\n",
    "season_train, season_test = seasonality(n_clusters,targets,1)  # removing the seasonality for each cluster before training\n",
    "\n",
    "for i in range (0,n_clusters):\n",
    "\n",
    "    inputs2 = inputs[:,:,:,i] # inputs of the i cluster \n",
    "    targets2 = targets[:,:,i] - season_train[:,:,i] # targets of the i cluster    \n",
    "    regr, scaler_inputs,scaler_targets = regressor(inputs2,targets[:,:,i],name,i)\n",
    "\n",
    "    scaler_inputs_all.append(scaler_inputs)\n",
    "    scaler_targets_all.append(scaler_targets)\n",
    "    regr_all.append(regr)\n",
    "\n",
    "    predictions[:,:,i] = scaling(regr_all[i],inputs2,scaler_inputs_all[i],targets2,scaler_targets_all[i]) # putting them in the right place\n",
    "\n",
    "r_train,rms_train,slope_train = post_training(n_clusters,targets,predictions)\n",
    "\n",
    "plotting_mean_values(years,n_clusters,targets,predictions,r_train,rms_train,slope_train,category,units,'Salish Sea (Training)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "indx = ~((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "dates = dates[indx]\n",
    "\n",
    "inputs,targets,indx = pre_training(dataset,dataset2,n_clusters,clusters0,name)\n",
    "\n",
    "predictions = np.full(targets.shape,np.nan)\n",
    "\n",
    "for i in range (0,n_clusters):\n",
    "\n",
    "    inputs2 = inputs[:,:,:,i] # inputs of the i cluster\n",
    "    targets2 = targets[:,:,i] - season_test[:,:,i] # targets of the i cluster\n",
    "\n",
    "    predictions[:,:,i] = scaling(regr_all[i],inputs2,scaler_inputs_all[i],targets2,scaler_targets_all[i]) # putting them in the right place\n",
    "\n",
    "    r_test, rms_test, slope_test = evaluation(dates,name,years,targets[:,:,i],predictions[:,:,i],i)\n",
    "\n",
    "plotting_mean_values(years,n_clusters,targets,predictions,_,_,_,category,units,'Salish Sea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if id == 0:\n",
    "#     path = '/data/ibougoudis/MOAD/files/results/' + name + '/func_reg_cl_d2_target_b1/'\n",
    "# else:\n",
    "#     path = '/data/ibougoudis/MOAD/files/results/' + name + 'func_reg_cl_d2_drivers_b1/'\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# with lzma.open(path + 'regr_all.xz', 'wb') as f:\n",
    "    \n",
    "#     dill.dump(regr_all, f)\n",
    "\n",
    "# with open(path + 'metrics.pkl', 'wb') as f:\n",
    "#     dill.dump([r_train,rms_train,slope_train,dates_season,season], f)\n",
    "\n",
    "# file_creation(path, targets_all, 'Targets')\n",
    "# file_creation(path, predictions_all, 'Predictions')\n",
    "# file_creation(path, (targets_all-predictions_all), 'Targets - Predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
