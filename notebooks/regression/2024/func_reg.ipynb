{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diatom concentration with functional regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Stacking y and x from the dataarray, then working per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "from skfda.ml.regression import KNeighborsRegressor\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.representation.basis import FourierBasis\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "import cmocean.cm as cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(ds, ds2, ref, name):\n",
    "    \n",
    "    years = np.unique(ds.time_counter.dt.year)\n",
    "    test = []\n",
    "    test2 = []\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        dataset = ds.sel(time_counter=str(year))\n",
    "        dataset2 = ds2.sel(time_counter=str(year))\n",
    "\n",
    "        y = np.tile(ref.y, len(ref.time_counter)*len(ref.x))\n",
    "        x = np.tile(np.repeat(ref.x, len(ref.y)), len(ref.time_counter))\n",
    "\n",
    "        test.append(np.stack([\n",
    "            dataset2['Summation_of_solar_radiation'].to_numpy(),\n",
    "            dataset2['Mean_wind_speed'].to_numpy(),\n",
    "            dataset2['Mean_air_temperature'].to_numpy(),\n",
    "            y.reshape(dataset2['Summation_of_solar_radiation'].to_numpy().shape),\n",
    "            x.reshape(dataset2['Summation_of_solar_radiation'].to_numpy().shape)\n",
    "            ]))\n",
    "        \n",
    "        test2.append(dataset[name].to_numpy())\n",
    "\n",
    "    # Grouping all the years\n",
    "    inputs = np.concatenate(test,axis=2)\n",
    "    targets = np.concatenate(test2,axis=1)\n",
    "\n",
    "    y = np.tile(ref.y, len(np.unique(ds.time_counter.dt.year))*len(ref.x))\n",
    "    x = np.tile(np.repeat(ref.x, len(ref.y)), len(np.unique(ds.time_counter.dt.year)))\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    return(inputs, targets, indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the data arrays\n",
    "def datasets_preparation2(variable, name, units, dataset, ref, indx):\n",
    "\n",
    "    # Creating the maps size (with nans)\n",
    "    variable_all = np.full((len(dataset.time_counter), len(ref.y) * len(ref.x)),np.nan)\n",
    "    variable_all[:,indx[0]] = variable\n",
    "    variable_all = np.reshape(variable_all,(len(dataset.time_counter),len(ref.y),len(ref.x)), order = 'F')\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    array = xr.DataArray(variable_all,\n",
    "        coords = {'time_counter': dataset.time_counter,'y': ref.y, 'x': ref.x},\n",
    "        dims = ['time_counter','y','x'],\n",
    "        attrs=dict(description= name,\n",
    "        units=units))\n",
    "        \n",
    "    return (array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_creation(path, variable, name):\n",
    "\n",
    "    temp = variable.to_dataset(name=name)\n",
    "    temp.to_netcdf(path = path + 'targets_predictions.nc', mode='a', encoding={name:{\"zlib\": True, \"complevel\": 9}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets):\n",
    "\n",
    "    from skfda.misc.hat_matrix import NadarayaWatsonHatMatrix\n",
    "    from skfda.misc.hat_matrix import KNeighborsHatMatrix\n",
    "    from skfda.misc.hat_matrix import LocalLinearRegressionHatMatrix\n",
    "    from skfda.ml.regression import KernelRegression\n",
    "    \n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((MinMaxScaler(), [0,1,2]), remainder=KBinsDiscretizer(n_bins=155,encode='ordinal',strategy='uniform'))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "    \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = MinMaxScaler()\n",
    "    temp = np.ravel(targets)\n",
    "    temp = np.expand_dims(temp,-1)\n",
    "    temp = scaler_targets.fit_transform(temp)\n",
    "    targets = temp.reshape(targets.shape)\n",
    "\n",
    "    # Final transformations\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    # targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "    kernel_estimator = NadarayaWatsonHatMatrix(bandwidth=1)\n",
    "    regr = KernelRegression(kernel_estimator=kernel_estimator)\n",
    "    regr.fit(inputs, targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets):\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # # Post-processing of predictions\n",
    "    # predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    # predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(inputs,predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(targets, predictions, name):\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, predictions, deg=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(5,10), layout='constrained')\n",
    "\n",
    "    ax[0].scatter(targets,predictions, alpha = 0.2, s = 10)\n",
    "\n",
    "    lims = [np.min([ax[0].get_xlim(), ax[0].get_ylim()]),\n",
    "        np.max([ax[0].get_xlim(), ax[0].get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[0].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[0].set_xlabel('targets')\n",
    "    ax[0].set_ylabel('predictions')\n",
    "    ax[0].set_xlim(lims)\n",
    "    ax[0].set_ylim(lims)\n",
    "    ax[0].set_aspect('equal')\n",
    "\n",
    "    ax[0].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    h = ax[1].hist2d(targets,predictions, bins=100, cmap='jet', \n",
    "        range=[lims,lims], cmin=0.1, norm='log')\n",
    "    \n",
    "    ax[1].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[1].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[1].set_xlabel('targets')\n",
    "    ax[1].set_ylabel('predictions')\n",
    "    ax[1].set_aspect('equal')\n",
    "\n",
    "    fig.colorbar(h[3],ax=ax[1], location='bottom')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality (dates,targets):\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    targets2 = xr.DataArray(targets,\n",
    "        coords = {'time_counter':dates},\n",
    "        dims = 'time_counter')\n",
    "    \n",
    "    test = targets2.groupby('time_counter.dayofyear').mean('time_counter')\n",
    "    test = test.drop_isel(dayofyear=14) # Removing 29 Feb\n",
    "    season = np.tile(test,len(np.unique(dates.year)))\n",
    "\n",
    "    return(season)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_criteria(dates, variable, year_variable, title):\n",
    "    \n",
    "    indx = pd.DatetimeIndex(dates)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter= ax.scatter(dates,variable, marker='.', c=indx.month)\n",
    "    plt.xticks(rotation=70)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=['February','March','April'])\n",
    "    ax.plot(dates[(indx.month == 3) & (indx.day == 15)], year_variable,color='red',marker='*')\n",
    "    fig.suptitle(title + ' (15 Feb - 30 Apr)')\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates, mean_targets, mean_predictions, units, category, region):\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "    ticks = [0]\n",
    "    \n",
    "    fig, _ = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(mean_targets)\n",
    "    mean_predictions = np.ma.array(mean_predictions)\n",
    "\n",
    "    for year in years[:-1]:\n",
    "        ticks.append((np.where(dates.year==year)[0][-1]+1))\n",
    "        mean_targets[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "        mean_predictions[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "\n",
    "    plt.plot(mean_targets, label = 'targets')\n",
    "    plt.plot(mean_predictions, label = 'predictions')\n",
    "    plt.xlabel('Years')\n",
    "    plt.xticks(ticks,years)\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region)\n",
    "    plt.legend()\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_maps(targets, predictions, name, units):\n",
    "\n",
    "    fig, ax = plt.subplots(2,2, figsize = (10,15), layout='tight')\n",
    "\n",
    "    cmap = plt.get_cmap('cubehelix')\n",
    "    cmap.set_bad('gray')\n",
    "\n",
    "    targets.plot(ax=ax[0,0], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    predictions.plot(ax=ax[0,1], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    (targets-predictions).plot(ax=ax[1,0], cmap=cmap, cbar_kwargs={'label': name + ' ' + units})\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "        bottom=0.1, \n",
    "        right=0.95, \n",
    "        top=0.95, \n",
    "        wspace=0.35, \n",
    "        hspace=0.35)\n",
    "\n",
    "    sa_vi.set_aspect(ax[0,0])\n",
    "    sa_vi.set_aspect(ax[0,1])\n",
    "    sa_vi.set_aspect(ax[1,0])\n",
    "\n",
    "    ax[0,0].title.set_text('Targets')\n",
    "    ax[0,1].title.set_text('Predictions')\n",
    "    ax[1,0].title.set_text('Targets-Predictions')\n",
    "    ax[1,1].axis('off')\n",
    "\n",
    "    fig.suptitle(name + ' '+ str(targets.time_counter.dt.date.values))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(ax, corn, colour):\n",
    "\n",
    "    ax.plot([corn[2], corn[3], corn[3], corn[2], corn[2]], \n",
    "    [corn[0], corn[0], corn[1], corn[1], corn[0]], '-', color=colour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Regional analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_regional(metric,box,years,category):\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    for i in range (0,len(box)):\n",
    "        ax.plot(years,metric[:,i],marker= '*', label=box[i])\n",
    "    plt.suptitle(category+ ' (Regional analysis)')\n",
    "    plt.legend()\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (regr,ds,ds2,name,units,ref,scaler_inputs,scaler_targets):\n",
    "\n",
    "    years = np.unique(ds.time_counter.dt.year)\n",
    "\n",
    "    # For every year\n",
    "    r_years = np.array([])\n",
    "    rms_years = np.array([])\n",
    "    slope_years = np.array([])\n",
    "\n",
    "    # For all testing years \n",
    "    targets_all = []\n",
    "    predictions_all = []\n",
    "\n",
    "    for year in (years):\n",
    "\n",
    "        dataset = ds.sel(time_counter=str(year))\n",
    "        dataset2 = ds2.sel(time_counter=str(year))\n",
    "    \n",
    "        inputs, targets, indx = datasets_preparation(dataset, dataset2, ref, name)\n",
    "\n",
    "        inputs,predictions = scaling(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "\n",
    "        # Calculating the annual time-series\n",
    "        m_year = scatter_plot(np.ravel(targets), np.ravel(predictions), name + ' for '+ str(year)) \n",
    "        r_year = np.corrcoef(np.ravel(targets), np.ravel(predictions))[0][1]\n",
    "        rms_year = rmse(np.ravel(targets), np.ravel(predictions))\n",
    "        \n",
    "        r_years = np.append(r_years,r_year)\n",
    "        rms_years = np.append(rms_years,rms_year)\n",
    "        slope_years = np.append(slope_years,m_year)\n",
    "\n",
    "        # Daily arrays\n",
    "        targets_all.append (datasets_preparation2(targets, name + ' _targets', units, dataset, ref, indx))\n",
    "        predictions_all.append(datasets_preparation2(predictions, name + ' _predictions', units, dataset, ref, indx))  \n",
    "\n",
    "    # Daily arrays\n",
    "    targets_all = xr.concat(targets_all, dim='time_counter')\n",
    "    predictions_all = xr.concat(predictions_all, dim='time_counter') \n",
    "    \n",
    "    return(r_years, rms_years, slope_years, targets_all, predictions_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of datasets (run only once!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Flagellate'\n",
    "units = '[mmol m-2]'\n",
    "category = 'Concentrations'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "ref = ds.sel(time_counter = slice('2007', '2007'))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "quant_train = dataset[name] # Keeping it for the regional seasonalities\n",
    "\n",
    "ds = ds.stack(z=('x','y'))\n",
    "ds2 = ds2.stack(z=('x','y'))\n",
    "\n",
    "indx = ~((ds.time_counter.dt.month==2) & (ds.time_counter.dt.day==29))\n",
    "ds = ds.sel(time_counter=indx)\n",
    "ds2 = ds2.sel(time_counter=indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "inputs, targets, indx = datasets_preparation(dataset, dataset2, ref, name)\n",
    "\n",
    "regr,scaler_inputs,scaler_targets = regressor(inputs, targets)\n",
    "\n",
    "inputs,predictions = scaling(regr,inputs,scaler_inputs,targets,scaler_targets)\n",
    "\n",
    "print ('The correlation coefficient during training is: ' + str(np.round(np.corrcoef(np.ravel(predictions),np.ravel(targets))[0][1],3)))\n",
    "print ('The rmse during training is: ' + str(rmse(np.ravel(predictions),np.ravel(targets))))\n",
    "m,_ = np.polyfit(np.ravel(predictions),np.ravel(targets), deg=1)\n",
    "print('The slope of the best fitting line during training is: '+str(np.round(m,3)))\n",
    "\n",
    "# Daily mean plot\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "indx2 = ~((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "dates = dates[indx2]\n",
    "\n",
    "targets = np.split(targets,len(np.unique(dates.year)),axis=1)\n",
    "targets = np.ravel(targets)\n",
    "targets = np.reshape(targets,(len(dates),int(len(indx[0])/len(np.unique(dates.year)))))\n",
    "targets = np.mean(targets,axis=1)\n",
    "\n",
    "predictions = np.split(predictions,len(np.unique(dates.year)),axis=1)\n",
    "predictions = np.ravel(predictions)\n",
    "predictions = np.reshape(predictions,(len(dates),int(len(indx[0])/len(np.unique(dates.year)))))\n",
    "predictions = np.mean(predictions,axis=1)\n",
    "\n",
    "plotting_mean_values(dates, targets, predictions, units, category, 'Salish Sea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for splitting into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(np.transpose(inputs,axes=(2,0,1)), targets.transpose(), test_size=0.33)\n",
    "# inputs = np.transpose(X_train,axes=(1,2,0))\n",
    "# targets = y_train.transpose()\n",
    "\n",
    "# scaler_inputs = []\n",
    "\n",
    "# # Scaling the inputs\n",
    "# for j in range (0, len(inputs)):\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     temp = np.ravel(inputs[j])\n",
    "#     temp = np.expand_dims(temp,-1)\n",
    "#     temp = scaler.fit_transform(temp)\n",
    "#     inputs[j] = temp.reshape(inputs[j].shape)\n",
    "#     scaler_inputs.append(scaler)\n",
    "\n",
    "# inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "\n",
    "# targets0 = targets\n",
    "\n",
    "# # Scaling the targets\n",
    "# scaler_targets = StandardScaler()\n",
    "# temp = np.ravel(targets)\n",
    "# temp = np.expand_dims(temp,-1)\n",
    "# temp = scaler_targets.fit_transform(temp)\n",
    "# targets = temp.reshape(targets.shape)\n",
    "# targets = targets.transpose()\n",
    "\n",
    "# # Final transformations\n",
    "# inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "# targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "# # Smoothing\n",
    "# # targets = targets.to_basis(FourierBasis(n_basis=5))\n",
    "\n",
    "# regr = KNeighborsRegressor(n_neighbors=10,n_jobs=4)\n",
    "# regr.fit(inputs, targets)\n",
    "\n",
    "# predictions = regr.predict(inputs)\n",
    "\n",
    "# # Post-processing of predictions\n",
    "# predictions = np.array(predictions.to_grid(np.arange(0,len(targets0))).data_matrix)\n",
    "# predictions = np.squeeze(predictions,2)\n",
    "# # Scaling the predictions\n",
    "# temp = np.ravel(predictions)\n",
    "# temp = np.expand_dims(temp,axis=-1)\n",
    "# temp = scaler_targets.inverse_transform(temp)\n",
    "# predictions = temp.reshape(predictions.shape)\n",
    "# predictions = predictions.transpose()\n",
    "\n",
    "# print ('The correlation coefficient during training is: ' + str(np.round(np.corrcoef(np.ravel(predictions),np.ravel(targets0))[0][1],3)))\n",
    "# print ('The rmse during training is: ' + str(rmse(np.ravel(predictions),np.ravel(targets0))))\n",
    "# m,_ = np.polyfit(np.ravel(predictions),np.ravel(targets0), deg=1)\n",
    "# print('The slope of the best fitting line during training is: '+str(np.round(m,3)))\n",
    "\n",
    "# inputs = np.transpose(X_test,axes=(1,2,0))\n",
    "# targets0 = y_test.transpose()\n",
    "\n",
    "# # Scaling the inputs\n",
    "# for j in range (0, len(inputs)):\n",
    "\n",
    "#     temp = np.ravel(inputs[j])\n",
    "#     temp = np.expand_dims(temp,axis=-1)\n",
    "#     temp = scaler_inputs[j].transform(temp)\n",
    "#     inputs[j] = temp.reshape(inputs[j].shape)\n",
    "\n",
    "# inputs = np.transpose(inputs,axes=(2,1,0))    \n",
    "\n",
    "# inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets0)))\n",
    "\n",
    "# predictions = regr.predict(inputs)\n",
    "\n",
    "# # Post-processing of predictions\n",
    "# predictions = np.array(predictions.to_grid(np.arange(0,len(targets0))).data_matrix)\n",
    "# predictions = np.squeeze(predictions,2)\n",
    "# # Scaling the predictions\n",
    "# temp = np.ravel(predictions)\n",
    "# temp = np.expand_dims(temp,axis=-1)\n",
    "# temp = scaler_targets.inverse_transform(temp)\n",
    "# predictions = temp.reshape(predictions.shape)\n",
    "# predictions = predictions.transpose()\n",
    "\n",
    "# print ('The correlation coefficient during testing is: ' + str(np.round(np.corrcoef(np.ravel(predictions),np.ravel(targets0))[0][1],3)))\n",
    "# print ('The rmse during testing is: ' + str(rmse(np.ravel(predictions),np.ravel(targets0))))\n",
    "# m,_ = np.polyfit(np.ravel(predictions),np.ravel(targets0), deg=1)\n",
    "# print('The slope of the best fitting line during testing is: '+str(np.round(m,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = seasonality(dates,targets)\n",
    "\n",
    "plt.plot(season[75:150])\n",
    "plt.suptitle('Long-term seasonality (2007-2020)')\n",
    "\n",
    "plotting_mean_values(dates, targets-season, predictions-season, units, category, 'Salish Sea (removed seasonality)')\n",
    "\n",
    "dates_season = dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "\n",
    "r_years, rms_years, slope_years, targets_all, predictions_all = evaluation(regr,dataset,dataset2,name,units,ref,scaler_inputs,scaler_targets)\n",
    "\n",
    "r_days = xr.corr(targets_all,predictions_all, dim=['x','y'])\n",
    "rms_days = xs.rmse(targets_all,predictions_all, dim=['x','y'], skipna=True)\n",
    "slope_days = xs.linslope(targets_all,predictions_all, dim=['x','y'], skipna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_criteria(dates, r_days, r_years, 'Correlation Coefficients')\n",
    "plotting_criteria(dates, rms_days, rms_years, 'Root Mean Square Errors')\n",
    "plotting_criteria(dates, slope_days, slope_years, 'Slopes of the best fitting line')\n",
    "\n",
    "# Daily maps\n",
    "maps = random.sample(sorted(np.arange(0,len(targets_all.time_counter))),10)\n",
    "for i in maps:\n",
    "\n",
    "    idx = np.isfinite(np.ravel(targets_all[i]))\n",
    "    scatter_plot(np.ravel(targets_all[i])[idx], np.ravel(predictions_all[i])[idx], name + ' '+ str(targets_all[i].time_counter.dt.date.values))\n",
    "\n",
    "    plotting_maps(targets_all[i], predictions_all[i], name, units)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 9))\n",
    "mycmap = cm.deep\n",
    "mycmap.set_bad('grey')\n",
    "ax.pcolormesh(ds[name][0], cmap=mycmap)\n",
    "sa_vi.set_aspect(ax)\n",
    "\n",
    "SoG_north = [650, 730, 100, 200]\n",
    "plot_box(ax, SoG_north, 'g')\n",
    "SoG_center = [450, 550, 200, 300]\n",
    "plot_box(ax, SoG_center, 'b')\n",
    "Fraser_plume = [380, 460, 260, 330]\n",
    "plot_box(ax, Fraser_plume, 'm')\n",
    "SoG_south = [320, 380, 280, 350]\n",
    "plot_box(ax, SoG_south, 'k')\n",
    "Haro_Boundary = [290, 350, 210, 280]\n",
    "plot_box(ax, Haro_Boundary, 'm')\n",
    "JdF_west = [250, 425, 25, 125]\n",
    "plot_box(ax, JdF_west, 'c')\n",
    "JdF_east = [200, 290, 150, 260]\n",
    "plot_box(ax, JdF_east, 'w')\n",
    "PS_main = [20, 150, 200, 280]\n",
    "plot_box(ax, PS_main, 'r')\n",
    "PS_all = [0, 200, 80, 320]\n",
    "plot_box(ax, PS_all, 'm')\n",
    "\n",
    "boxnames = ['SoG_north','SoG_center','Fraser_plume','SoG_south', 'Haro_Boundary', 'JdF_west', 'JdF_east', 'PS_main', 'PS_all']\n",
    "fig.legend(boxnames)\n",
    "\n",
    "SS_all = [0, 895, 0, 395]\n",
    "boxes = [SS_all,SoG_north,SoG_center,Fraser_plume,SoG_south,Haro_Boundary,JdF_west,JdF_east,PS_main, PS_all]\n",
    "boxnames.insert(0,'SS_all')\n",
    "\n",
    "# Low resolution\n",
    "temp = []\n",
    "for i in boxes:\n",
    "\n",
    "    temp.append([x//5 for x in i])\n",
    "\n",
    "boxes = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "r = np.zeros((4,len(boxnames)))\n",
    "rms = np.zeros((4,len(boxnames)))\n",
    "\n",
    "r_season = np.zeros((4,len(boxnames)))\n",
    "rms_season = np.zeros((4,len(boxnames)))\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "\n",
    "    targets=targets_all[:,boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]]\n",
    "    predictions=predictions_all[:,boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]]\n",
    "   \n",
    "    mean_targets = targets.mean(dim=['x','y'], skipna=True)\n",
    "    mean_predictions = predictions.mean(dim=['x','y'], skipna=True)\n",
    "    plotting_mean_values(dates, mean_targets, mean_predictions, units, category, boxnames[i])\n",
    "\n",
    "    climatology = quant_train[:,boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]]\n",
    "    mean_targets_clim = climatology.mean(dim=['x','y'], skipna=True)\n",
    "    mean_targets_clim = mean_targets_clim.drop_sel(time_counter=['2008-02-29','2012-02-29','2016-02-29','2020-02-29']) # Removing 29 of Feb\n",
    "    season = seasonality(dates_season,mean_targets_clim)\n",
    "\n",
    "    plotting_mean_values(dates, mean_targets-season[np.where(dates_season.year==2017)[0][0]:], \n",
    "        mean_predictions-season[np.where(dates_season.year==2017)[0][0]:], units, category, boxnames[i]+' (removed seasonality)')\n",
    "\n",
    "    targets_annual = np.ravel(mean_targets.groupby('time_counter.year'))\n",
    "    predictions_annual = np.ravel(mean_predictions.groupby('time_counter.year'))\n",
    "\n",
    "    targets_annual_season = np.ravel((mean_targets-season[np.where(dates_season.year==2017)[0][0]:]).groupby('time_counter.year'))\n",
    "    predictions_annual_season = np.ravel((mean_predictions-season[np.where(dates_season.year==2017)[0][0]:]).groupby('time_counter.year'))\n",
    "    \n",
    "    years = []\n",
    "    for j in range(1,8,2):\n",
    "\n",
    "        years.append(targets_annual[j-1])\n",
    "\n",
    "        r[len(years)-1,i] = np.round(np.corrcoef(np.ravel(targets_annual[j]),np.ravel(predictions_annual[j]))[0][1],3)\n",
    "        rms[len(years)-1,i] = rmse(np.ravel(targets_annual[j]),np.ravel(predictions_annual[j]))\n",
    "\n",
    "        r_season[len(years)-1,i] = np.round(np.corrcoef(np.ravel(targets_annual_season[j]),np.ravel(predictions_annual_season[j]))[0][1],3)\n",
    "        rms_season[len(years)-1,i] = rmse(np.ravel(targets_annual_season[j]),np.ravel(predictions_annual_season[j]))\n",
    "\n",
    "plotting_regional(r,boxnames,years, 'Correlation coefficients')\n",
    "plotting_regional(rms,boxnames,years, 'Root mean square errors')\n",
    "\n",
    "plotting_regional(r_season,boxnames,years, 'Correlation coefficients (removed seasonality)')\n",
    "plotting_regional(rms_season,boxnames,years, 'Root mean square errors (removed seasonality)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets_all.stack(z=('x','y')).dropna('z').to_numpy()\n",
    "predictions = predictions_all.stack(z=('x','y')).dropna('z').to_numpy()\n",
    "\n",
    "targets = targets.transpose()\n",
    "predictions = predictions.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(targets[1001].transpose(), label = \"targets\")\n",
    "plt.plot(predictions[1001].transpose(), label = \"predictions\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/data/ibougoudis/MOAD/files/results/' + name + '/func_reg/'\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# with lzma.open(path + 'regr.xz', 'wb') as f:\n",
    "#     dill.dump(regr, f)\n",
    "\n",
    "# file_creation(path, targets_all, 'Targets')\n",
    "# file_creation(path, predictions_all, 'Predictions')\n",
    "# file_creation(path, (targets_all-predictions_all), 'Targets - Predictions')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
