{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diatom concentration with functional clustering and a Histogram-based Gradient Boosting Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.clustering import KMeans\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, clusters, name):\n",
    "    \n",
    "    inputs = np.stack([\n",
    "        np.ravel(dataset2['Summation_of_solar_radiation']),\n",
    "        np.ravel(dataset2['Mean_air_temperature']),\n",
    "        np.ravel(dataset2['Mean_precipitation']),\n",
    "        np.ravel(dataset2['Mean_pressure']),\n",
    "        np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y)),\n",
    "        ])\n",
    "    \n",
    "    x = np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter))\n",
    "\n",
    "    targets = np.ravel(dataset[name])\n",
    "    \n",
    "    indx = np.where(np.isfinite(targets) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,indx[0]]\n",
    "    targets = targets[indx[0]]\n",
    "\n",
    "    clusters = np.tile(np.ravel(clusters), len(dataset.time_counter))\n",
    "    clusters = clusters[indx[0]]\n",
    "\n",
    "    inputs = inputs.transpose()\n",
    "\n",
    "    return(inputs, targets, indx, clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Finalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(dataset,quant,indx,name):\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    clusters = kmeans.fit_predict(quant)\n",
    "\n",
    "    unique, _ = np.unique(clusters, return_counts=True)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "    clusters2 = xr.DataArray(clusters,dims = ['y','x'])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize =(5,9))\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "    cmap.set_bad('gray')\n",
    "    clus = clusters2.plot(ax=ax, cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "\n",
    "    cbar = fig.colorbar(clus, ticks = unique+0.5) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    cbar.set_label('Clusters [count]')\n",
    "    ax.set_title('Functional Clustering for '+ name + ' (2007-2020)')\n",
    "\n",
    "    sa_vi.set_aspect(ax)\n",
    "    plt.show()\n",
    "\n",
    "    return(clusters)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_target(dataset, name):\n",
    "\n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "    \n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    targets = np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    # Means\n",
    "    targets = np.nanmean(targets,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    targets = targets[:,indx[0]]\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    targets = targets.transpose()\n",
    "    targets2 = FDataGrid(targets)\n",
    "\n",
    "    clusters = clustering(dataset,targets2,indx,name)\n",
    "\n",
    "    return(clusters,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (Drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_drivers(dataset,dataset2,name):\n",
    "\n",
    "    indx = np.where((dataset2.time_counter.dt.month==2) & (dataset2.time_counter.dt.day==29))\n",
    "\n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    inputs = np.stack([\n",
    "        dataset2['Summation_of_solar_radiation'].to_numpy().reshape(*dataset2['Summation_of_solar_radiation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_air_temperature'].to_numpy().reshape(*dataset2['Mean_air_temperature'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_precipitation'].to_numpy().reshape(*dataset2['Mean_precipitation'].to_numpy().shape[:1],-1),\n",
    "        dataset2['Mean_pressure'].to_numpy().reshape(*dataset2['Mean_pressure'].to_numpy().shape[:1],-1)\n",
    "        ])\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.split(inputs,len(np.unique(dataset2.time_counter.dt.year)),axis=1)\n",
    "\n",
    "    # Means\n",
    "    inputs = np.nanmean(inputs,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets).any(axis=0))& (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,indx[0]]\n",
    "\n",
    "    # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), [0,1,2,3]))\n",
    "    temp = scaler_inputs.fit_transform(temp)\n",
    "    temp = temp.transpose()\n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2])) \n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs2 = FDataGrid(inputs, np.arange(0,len(inputs[0])))\n",
    "\n",
    "    clusters = clustering(dataset2,inputs2,indx,'inputs')\n",
    "\n",
    "    return(clusters, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the data arrays\n",
    "def datasets_preparation2(variable, name, units, dataset):\n",
    "\n",
    "    # Obtaining the daily indexes\n",
    "    temp = np.reshape(np.ravel(dataset['Temperature_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x)))\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(temp).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "\n",
    "    variable_all = np.full((len(dataset.time_counter), len(dataset.y) * len(dataset.x)),np.nan)\n",
    "    variable_all[:,indx[0]] = variable\n",
    "    variable_all = np.reshape(variable_all,(len(dataset.time_counter),len(dataset.y),len(dataset.x)))\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    array = xr.DataArray(variable_all,\n",
    "        coords = {'time_counter': dataset.time_counter,'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['time_counter','y','x'],\n",
    "        attrs=dict(description= name,\n",
    "        units=units))\n",
    "        \n",
    "    return (array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_creation(path, variable, name):\n",
    "\n",
    "    temp = variable.to_dataset(name=name)\n",
    "    temp.to_netcdf(path = path + 'targets_predictions.nc', mode='a', encoding={name:{\"zlib\": True, \"complevel\": 9}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, clusters, name):\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "        transformers=[('drivers', StandardScaler(), [0,1,2,3])],remainder='passthrough'),\n",
    "        HistGradientBoostingRegressor(categorical_features=[4])),\n",
    "        transformer=StandardScaler())\n",
    "    \n",
    "    regr_all = []\n",
    "    predictions = np.full(len(targets),np.nan) # size of targets without nans\n",
    "    \n",
    "    for i in range (0,len(np.unique(clusters))):\n",
    "        indx = np.where(clusters==i) # indexes of the i cluster\n",
    "        inputs2 = inputs[indx[0]] # inputs of the i cluster\n",
    "        targets2 = targets[indx[0]]\n",
    "\n",
    "        regr = BaggingRegressor(model, n_estimators=12, n_jobs=4).fit(inputs2,targets2)\n",
    "        predictions[indx[0]] = regr.predict(inputs2) # putting them in the right place\n",
    "        regr_all.append(regr)\n",
    "    \n",
    "        r = np.round(r_regression(inputs2,targets2),2)\n",
    "        dict = {'Summation_of_solar_radiation': r[0], 'Mean_air_temperature': r[1], 'Mean_precipitation': r[2], 'Mean_pressure': r[3], 'Day_of_the_year': r[4]}\n",
    "\n",
    "        print('The correlation coefficients between each input and ' + name +  ' for cluster ' + str(i+1) + ' are: ' +str(dict))\n",
    "\n",
    "    return(regr_all,predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(targets, predictions, name):\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, predictions, deg=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(5,10), layout='constrained')\n",
    "\n",
    "    ax[0].scatter(targets,predictions, alpha = 0.2, s = 10)\n",
    "\n",
    "    lims = [np.min([ax[0].get_xlim(), ax[0].get_ylim()]),\n",
    "        np.max([ax[0].get_xlim(), ax[0].get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[0].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[0].set_xlabel('targets')\n",
    "    ax[0].set_ylabel('predictions')\n",
    "    ax[0].set_xlim(lims)\n",
    "    ax[0].set_ylim(lims)\n",
    "    ax[0].set_aspect('equal')\n",
    "\n",
    "    ax[0].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    h = ax[1].hist2d(targets,predictions, bins=100, cmap='jet', \n",
    "        range=[lims,lims], cmin=0.1, norm='log')\n",
    "    \n",
    "    ax[1].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[1].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[1].set_xlabel('targets')\n",
    "    ax[1].set_ylabel('predictions')\n",
    "    ax[1].set_aspect('equal')\n",
    "\n",
    "    fig.colorbar(h[3],ax=ax[1], location='bottom')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality (dates,targets):\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    targets2 = xr.DataArray(targets,\n",
    "        coords = {'time_counter':dates},\n",
    "        dims = 'time_counter')\n",
    "    \n",
    "    test = targets2.groupby('time_counter.dayofyear').mean('time_counter')\n",
    "    test0 = test.drop_isel(dayofyear=14) # Removing 29 Feb\n",
    "    test2 = np.tile(test0,len(np.unique(dates.year)))\n",
    "    indx2 = np.where((dates.month==2) & (dates.day==29)) # Finding where  29 Feb exists\n",
    "\n",
    "    test3 = np.insert(test2,indx2[0][0],test[14]) # 2008\n",
    "    test3 = np.insert(test3,indx2[0][1],test[14]) # 2012\n",
    "    test3 = np.insert(test3,indx2[0][2],test[14]) # 2016\n",
    "    season = np.insert(test3,indx2[0][3],test[14]) # 2020\n",
    "\n",
    "    return(season)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_criteria(dates, variable, year_variable, title):\n",
    "    \n",
    "    indx = pd.DatetimeIndex(dates)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter= ax.scatter(dates,variable, marker='.', c=indx.month)\n",
    "    plt.xticks(rotation=70)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=['February','March','April'])\n",
    "    ax.plot(dates[(indx.month == 3) & (indx.day == 15)], year_variable,color='red',marker='*')\n",
    "    fig.suptitle(title + ' (15 Feb - 30 Apr)')\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates, mean_targets, mean_predictions, units, category, region):\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "    ticks = [0]\n",
    "    \n",
    "    fig, _ = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(mean_targets)\n",
    "    mean_predictions = np.ma.array(mean_predictions)\n",
    "\n",
    "    for year in years[:-1]:\n",
    "        ticks.append((np.where(dates.year==year)[0][-1]+1))\n",
    "        mean_targets[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "        mean_predictions[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "\n",
    "    plt.plot(mean_targets, label = 'targets')\n",
    "    plt.plot(mean_predictions, label = 'predictions')\n",
    "    plt.xlabel('Years')\n",
    "    plt.xticks(ticks,years)\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_maps(targets, predictions, name, units):\n",
    "\n",
    "    fig, ax = plt.subplots(2,2, figsize = (10,15), layout='tight')\n",
    "\n",
    "    cmap = plt.get_cmap('cubehelix')\n",
    "    cmap.set_bad('gray')\n",
    "\n",
    "    targets.plot(ax=ax[0,0], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    predictions.plot(ax=ax[0,1], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    (targets-predictions).plot(ax=ax[1,0], cmap=cmap, cbar_kwargs={'label': name + ' ' + units})\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "        bottom=0.1, \n",
    "        right=0.95, \n",
    "        top=0.95, \n",
    "        wspace=0.35, \n",
    "        hspace=0.35)\n",
    "\n",
    "    sa_vi.set_aspect(ax[0,0])\n",
    "    sa_vi.set_aspect(ax[0,1])\n",
    "    sa_vi.set_aspect(ax[1,0])\n",
    "\n",
    "    ax[0,0].title.set_text('Targets')\n",
    "    ax[0,1].title.set_text('Predictions')\n",
    "    ax[1,0].title.set_text('Targets-Predictions')\n",
    "    ax[1,1].axis('off')\n",
    "\n",
    "    fig.suptitle(name + ' '+ str(targets.time_counter.dt.date.values))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(dates,dataset,clusters,indx,targets,predictions,units,category):\n",
    "\n",
    "    r_train = np.zeros(len(np.unique(clusters)))\n",
    "    rms_train = np.zeros(len(np.unique(clusters)))\n",
    "    slope_train = np.zeros(len(np.unique(clusters)))\n",
    "\n",
    "    targets_mean = np.zeros((len(np.unique(clusters)),len(dataset.time_counter)))\n",
    "    predictions_mean = np.zeros((len(np.unique(clusters)),len(dataset.time_counter)))\n",
    "\n",
    "    for i in range (0,len(np.unique(clusters))):\n",
    "        indx2 = np.where(clusters==i) # indexes of the j cluster\n",
    "        targets2 = targets[indx2[0]] # inputs of the j cluster\n",
    "        predictions2 = predictions[indx2[0]] # putting them in the right place\n",
    "\n",
    "        r_train[i] = np.round(np.corrcoef(predictions2,targets2)[0][1],3)\n",
    "        rms_train[i] = rmse(predictions2,targets2)\n",
    "        m,_ = np.polyfit(targets2, predictions2, deg=1)\n",
    "        slope_train[i] = np.round(m,3)\n",
    "\n",
    "        temp = pd.DataFrame(np.vstack((r_train[i],rms_train[i],slope_train[i])).transpose(),index=['Cluster '+str(i+1)],columns=['r_train','rms_train','slope_train'])\n",
    "        display(temp)\n",
    "\n",
    "        # for the daily mean plot\n",
    "        targets_mean_temp = np.reshape(targets2,(len(dataset.time_counter), int(len(indx2[0]) / len(dataset.time_counter))))\n",
    "        predictions_mean_temp = np.reshape(predictions2,(len(dataset.time_counter), int(len(indx2[0]) / len(dataset.time_counter))))\n",
    "        targets_mean[i] = np.mean(targets_mean_temp,axis=1)\n",
    "        predictions_mean[i] = np.mean(predictions_mean_temp,axis=1)\n",
    "\n",
    "        plotting_mean_values(dates, targets_mean[i], predictions_mean[i], units, category, 'Salish Sea Cluster '+str(i+1))\n",
    "\n",
    "    return(r_train,rms_train,slope_train,targets_mean,predictions_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (regr_all, clusters0, ds, ds2, name, units):\n",
    "\n",
    "    years = np.unique(ds.time_counter.dt.year)\n",
    "\n",
    "    # For every year\n",
    "    r_years = np.array([])\n",
    "    rms_years = np.array([])\n",
    "    slope_years = np.array([])\n",
    "\n",
    "    # The data arrays \n",
    "    targets_all = []\n",
    "    predictions_all = []\n",
    "\n",
    "    for year in (years):\n",
    "\n",
    "        dataset = ds.sel(time_counter=str(year))\n",
    "        dataset2 = ds2.sel(time_counter=str(year))\n",
    "\n",
    "        inputs, targets, indx, clusters  = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "        # Predictions for each regressor\n",
    "        predictions = np.full(len(targets),np.nan) # size of a year without nans\n",
    "        for i in range (0,len(np.unique(clusters))):\n",
    "            indx2 = np.where(clusters==i) # indexes of the j cluster\n",
    "            inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "            predictions[indx2[0]] = regr_all[i].predict(inputs2) # putting them in the right place\n",
    "\n",
    "        # Calculating the annual time-series\n",
    "        m_year = scatter_plot(targets, predictions, name + ' for '+ str(year)) \n",
    "        r_year = np.round(np.corrcoef(targets, predictions)[0][1],3)\n",
    "        rms_year = rmse(targets, predictions)\n",
    "        \n",
    "        r_years = np.append(r_years,r_year)\n",
    "        rms_years = np.append(rms_years,rms_year)\n",
    "        slope_years = np.append(slope_years,m_year)\n",
    "\n",
    "        # Daily arrays\n",
    "        targets = np.reshape(targets,(len(dataset.time_counter), int(len(indx[0]) / len(dataset.time_counter))))\n",
    "        predictions = np.reshape(predictions,(len(dataset.time_counter), int(len(indx[0]) / len(dataset.time_counter))))\n",
    "        targets_all.append (datasets_preparation2(targets, name + ' _targets', units, dataset))\n",
    "        predictions_all.append(datasets_preparation2(predictions, name + ' _predictions', units, dataset))   \n",
    "\n",
    "    # Daily arrays\n",
    "    targets_all = xr.concat(targets_all, dim='time_counter')\n",
    "    predictions_all = xr.concat(predictions_all, dim='time_counter')\n",
    "    \n",
    "    return(r_years, rms_years, slope_years, targets_all, predictions_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Flagellate_Production_Rate'\n",
    "units = '[mmol N m-2 s-1]'\n",
    "category = 'Production rates'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "\n",
    "# Selecting the clustering input (drivers or target)\n",
    "clusters0, id = func_clust_target(dataset, name)\n",
    "# clusters0, id = func_clust_drivers(dataset,dataset2,name)\n",
    "\n",
    "inputs, targets, indx, clusters = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "regr_all,predictions = regressor(inputs, targets, clusters, name)\n",
    "\n",
    "r_train,rms_train,slope_train,targets_mean,predictions_mean = post_processing(dates,dataset,clusters,indx,targets,predictions,units,category)\n",
    "\n",
    "season = np.zeros((len(np.unique(clusters)),len(dataset.time_counter)))\n",
    "\n",
    "for i in range (0,len(np.unique(clusters))):\n",
    "    season[i] = seasonality(dates,targets_mean[i])\n",
    "\n",
    "plt.plot(season[:,0:75].transpose())\n",
    "plt.legend(('Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5','Cluster 6'))\n",
    "plt.suptitle('Long-term seasonalities (2007-2020)')\n",
    "\n",
    "for i in range (0,len(np.unique(clusters))):\n",
    "\n",
    "    plotting_mean_values(dates, targets_mean[i]-season[i], predictions_mean[i]-season[i], units, category, 'Salish Sea (removed seasonality) (Cluster) '+str(i+1))\n",
    "\n",
    "quant_train = dataset[name] # Keeping it for the regional seasonalities\n",
    "dates_season = pd.DatetimeIndex(quant_train['time_counter'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(time_counter = slice('2021', '2024'))\n",
    "ds2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "dates = pd.DatetimeIndex(ds['time_counter'].values)\n",
    "\n",
    "r_years, rms_years, slope_years, targets_all, predictions_all = evaluation(regr_all, clusters0, ds ,ds2, name, units)\n",
    "\n",
    "r_days = xr.corr(targets_all,predictions_all, dim=['x','y'])\n",
    "rms_days = xs.rmse(targets_all,predictions_all, dim=['x','y'], skipna=True)\n",
    "slope_days = xs.linslope(targets_all,predictions_all, dim=['x','y'], skipna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_criteria(dates, r_days, r_years, 'Correlation Coefficients')\n",
    "plotting_criteria(dates, rms_days, rms_years, 'Root Mean Square Errors')\n",
    "plotting_criteria(dates, slope_days, slope_years, 'Slopes of the best fitting line')\n",
    "\n",
    "# Daily maps\n",
    "maps = random.sample(sorted(np.arange(0,len(targets_all.time_counter))),10)\n",
    "for i in maps:\n",
    "\n",
    "    idx = np.isfinite(np.ravel(targets_all[i]))\n",
    "    scatter_plot(np.ravel(targets_all[i])[idx], np.ravel(predictions_all[i])[idx], name + ' '+ str(targets_all[i].time_counter.dt.date.values))\n",
    "\n",
    "    plotting_maps(targets_all[i], predictions_all[i], name, units)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter= slice('2021', '2024'))\n",
    "dataset2 = ds2.sel(time_counter= slice('2021', '2024'))\n",
    "\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "targets_mean = np.zeros((len(np.unique(clusters)),len(dataset.time_counter)))\n",
    "predictions_mean = np.zeros((len(np.unique(clusters)),len(dataset.time_counter)))\n",
    "\n",
    "r_test = np.zeros((len(np.unique(clusters)),len(np.unique(dataset.time_counter.dt.year))))\n",
    "rms_test = np.zeros((len(np.unique(clusters)),len(np.unique(dataset.time_counter.dt.year))))\n",
    "slope_test = np.zeros((len(np.unique(clusters)),len(np.unique(dataset.time_counter.dt.year))))\n",
    "\n",
    "inputs, targets, indx, clusters  = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "predictions = np.full(len(targets),np.nan) # size of a year without nans\n",
    "for i in range (0,len(np.unique(clusters))):\n",
    "    indx2 = np.where(clusters==i) # indexes of the j cluster\n",
    "    inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "    targets2 = targets[indx2[0]] # targets of the j cluster\n",
    "    predictions[indx2[0]] = regr_all[i].predict(inputs2) # putting them in the right place\n",
    "    predictions2 = predictions[indx2[0]]\n",
    "\n",
    "    targets_mean_temp = np.reshape(targets2,(len(dataset.time_counter), int(len(indx2[0]) / len(dataset.time_counter))))\n",
    "    predictions_mean_temp = np.reshape(predictions[indx2[0]],(len(dataset.time_counter), int(len(indx2[0]) / len(dataset.time_counter))))\n",
    "    targets_mean[i] = np.mean(targets_mean_temp,axis=1)\n",
    "    predictions_mean[i] = np.mean(predictions_mean_temp,axis=1)\n",
    "\n",
    "    for year in range (0, len(years)):\n",
    "        r_test[i,year] = np.round(np.corrcoef(predictions_mean[i,np.where(dates.year==years[year])],targets_mean[i,np.where(dates.year==years[year])])[0][1],3)\n",
    "        rms_test[i,year] = rmse(predictions_mean[i,np.where(dates.year==years[year])],targets_mean[i,np.where(dates.year==years[year])])\n",
    "        m,_ = np.polyfit(np.squeeze(predictions_mean[i,np.where(dates.year==years[year])]),np.squeeze(targets_mean[i,np.where(dates.year==years[year])]), deg=1)\n",
    "        slope_test[i,year] = np.round(m,3)\n",
    "\n",
    "    temp = pd.DataFrame(np.vstack((r_test[i],rms_test[i],slope_test[i])).transpose(),index=[np.unique(dataset.time_counter.dt.year)],columns=['r_test','rms_test','slope_test'])\n",
    "    display(temp)\n",
    "\n",
    "    plotting_mean_values(dates, targets_mean[i], predictions_mean[i], units, category, 'Salish Sea Cluster '+str(i+1))\n",
    "    plotting_mean_values(dates, targets_mean[i]-season[i,np.where(dates_season.year==2017)[0][0]:], predictions_mean[i]-season[i,np.where(dates_season.year==2017)[0][0]:], \n",
    "        units, category, 'Salish Sea (removed seasonality) Cluster '+str(i+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if id == 0:\n",
    "#     path = '/data/ibougoudis/MOAD/files/results/' + name + '/hist_cl_target/'\n",
    "# else:\n",
    "#     path = '/data/ibougoudis/MOAD/files/results/' + name + '/hist_cl_drivers/'\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# with lzma.open(path + 'regr_all.xz', 'wb') as f:\n",
    "    \n",
    "#     dill.dump(regr_all, f)\n",
    "\n",
    "# with open(path + 'metrics.pkl', 'wb') as f:\n",
    "#     dill.dump([r_train,rms_train,slope_train,dates_season,season], f)\n",
    "\n",
    "# file_creation(path, targets_all, 'Targets')\n",
    "# file_creation(path, predictions_all, 'Predictions')\n",
    "# file_creation(path, (targets_all-predictions_all), 'Targets - Predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
