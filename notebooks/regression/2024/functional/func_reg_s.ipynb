{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diatom concentration with functional regression (spatial means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.regression import HistoricalLinearRegression, KernelRegression\n",
    "\n",
    "from skfda.misc.hat_matrix import NadarayaWatsonHatMatrix, LocalLinearRegressionHatMatrix, KNeighborsHatMatrix\n",
    "from skfda.preprocessing.smoothing import KernelSmoother\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n",
    "\n",
    "np.warnings.filterwarnings('ignore') # For the nan mean warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, name, inputs_names):\n",
    "    \n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "\n",
    "    targets = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "\n",
    "    inputs = []\n",
    "    for i in inputs_names:\n",
    "        inputs.append(dataset2[i].to_numpy().reshape(*dataset2[i].to_numpy().shape[:1],-1))\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    # Deleting 29 of February\n",
    "    inputs = np.delete(inputs,indx,axis=1)\n",
    "    targets = np.delete(targets,indx,axis=0)\n",
    "\n",
    "    # Splitting in years\n",
    "    inputs = np.array(np.split(inputs,len(np.unique(dataset.time_counter.dt.year)),axis=1))\n",
    "    targets = np.array(np.split(targets,len(np.unique(dataset.time_counter.dt.year)),axis=0))\n",
    "\n",
    "    # Transposing\n",
    "    inputs = np.transpose(inputs, (1,2,0,3))\n",
    "    targets = np.transpose(targets, (1,0,2))\n",
    "\n",
    "    x =  np.tile(dataset2.x, len(dataset2.y))\n",
    "    y =  np.tile(np.repeat(dataset2.y, len(dataset2.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(targets[0]).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,:,:,indx[0]]\n",
    "    targets = targets[:,:,indx[0]]\n",
    "\n",
    "    inputs = np.nanmean(inputs,axis=3)\n",
    "    targets = np.nanmean(targets,axis=2)\n",
    "\n",
    "    return(inputs, targets, indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_creation(path, variable, name):\n",
    "\n",
    "    temp = variable.to_dataset(name=name)\n",
    "    temp.to_netcdf(path = path + 'targets_predictions.nc', mode='a', encoding={name:{\"zlib\": True, \"complevel\": 9}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, lag):\n",
    "\n",
    "    # Printing of the correlation coefficients\n",
    "    temp_inputs = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]), order='F')\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    temp_targets = np.reshape(targets, (targets.shape[0]*targets.shape[1]), order='F')\n",
    "\n",
    "    r_inputs = np.round(r_regression(temp_inputs,temp_targets),2)\n",
    "\n",
    "    # Scaling the inputs\n",
    "    scaler_inputs = make_column_transformer((StandardScaler(), np.arange(0,len(inputs))))\n",
    "    temp_inputs = scaler_inputs.fit_transform(temp_inputs)\n",
    "    temp_inputs = temp_inputs.transpose()\n",
    "    inputs = np.reshape(temp_inputs,(len(inputs),inputs.shape[1],inputs.shape[2]), order='F')   \n",
    "    \n",
    "    # Scaling the targets\n",
    "    scaler_targets = StandardScaler()\n",
    "    temp_targets = np.expand_dims(temp_targets,-1)\n",
    "    temp_targets = scaler_targets.fit_transform(temp_targets)\n",
    "    targets = temp_targets.reshape(targets.shape, order='F')\n",
    "\n",
    "    # Final transformations\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    targets = targets.transpose()\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets[0])))\n",
    "    targets = FDataGrid(data_matrix=targets, grid_points=np.arange(0,len(targets[0])))\n",
    "\n",
    "    # Smoothing\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10))\n",
    "    kernel_estimator = LocalLinearRegressionHatMatrix(bandwidth=1)\n",
    "    smoother = KernelSmoother(kernel_estimator=kernel_estimator)\n",
    "    inputs = smoother.fit_transform(inputs)\n",
    "\n",
    "    model = HistoricalLinearRegression(n_intervals=3, lag=lag)\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    return(regr,scaler_inputs,scaler_targets,smoother,r_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(regr,inputs,scaler_inputs,targets,scaler_targets,smoother):\n",
    "\n",
    "    # targets = targets.to_basis(FourierBasis(n_basis=10)\n",
    "\n",
    "    # # Scaling the inputs\n",
    "    temp = np.reshape(inputs,(len(inputs),inputs.shape[1]*inputs.shape[2]))\n",
    "    temp = temp.transpose()\n",
    "    temp = scaler_inputs.transform(temp)\n",
    "    temp = temp.transpose()        \n",
    "    inputs = np.reshape(temp,(len(inputs),inputs.shape[1],inputs.shape[2]))\n",
    "        \n",
    "    inputs = np.transpose(inputs,axes=(2,1,0))\n",
    "    inputs = FDataGrid(data_matrix=inputs, grid_points=np.arange(0,len(targets)))\n",
    "\n",
    "    inputs = smoother.transform(inputs)\n",
    "\n",
    "    predictions = regr.predict(inputs)\n",
    "\n",
    "    # Post-processing of predictions\n",
    "    predictions = np.array(predictions.to_grid(np.arange(0,len(targets))).data_matrix)\n",
    "    predictions = np.squeeze(predictions,2)\n",
    "\n",
    "    # # Scaling the predictions\n",
    "    temp = np.ravel(predictions)\n",
    "    temp = np.expand_dims(temp,axis=-1)\n",
    "    temp = scaler_targets.inverse_transform(temp)\n",
    "    predictions = temp.reshape(predictions.shape)\n",
    "    predictions = predictions.transpose()\n",
    "\n",
    "    return(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(dates, targets, predictions, name):\n",
    "\n",
    "    indx = pd.DatetimeIndex(dates[0:75]) # From the first year\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, predictions, deg=1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter = ax.scatter(targets,predictions, s = 10, c= indx.month)\n",
    "\n",
    "    lims = [np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "        np.max([ax.get_xlim(), ax.get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax.axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax.set_xlabel('targets')\n",
    "    ax.set_ylabel('predictions')\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=['February','March','April'])\n",
    "\n",
    "    ax.plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates,mean_targets,mean_predictions,category,units,region):\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "    ticks = [0]\n",
    "    \n",
    "    fig, _ = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(mean_targets)\n",
    "    mean_predictions = np.ma.array(mean_predictions)\n",
    "\n",
    "    for year in years[:-1]:\n",
    "        ticks.append((np.where(dates.year==year)[0][-1]+1))\n",
    "        mean_targets[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "        mean_predictions[(np.where(dates.year==year)[0][-1]+1)] = np.ma.masked\n",
    "\n",
    "    plt.plot(mean_targets, label = 'targets')\n",
    "    plt.plot(mean_predictions, label = 'predictions')\n",
    "    plt.xlabel('Years')\n",
    "    plt.xticks(ticks,years)\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(id,dates,years,targets,predictions,name):\n",
    "\n",
    "    # For every year\n",
    "    r_years = np.array([])\n",
    "    rms_years = np.array([])\n",
    "    slope_years = np.array([])\n",
    "\n",
    "    for i in range (0,len(years)):\n",
    "\n",
    "        r_year = np.round(np.corrcoef(np.ravel(targets[:,i]), np.ravel(predictions[:,i]))[0][1],3)\n",
    "        rms_year = np.round(rmse(np.ravel(targets[:,i]), np.ravel(predictions[:,i])),5)\n",
    "        m,_ = np.polyfit(np.ravel(targets[:,i]), np.ravel(predictions[:,i]), deg=1)\n",
    "        slope_year = np.round(m,3)\n",
    "\n",
    "        r_years = np.append(r_years,r_year)\n",
    "        rms_years = np.append(rms_years,rms_year)\n",
    "        slope_years = np.append(slope_years,slope_year)\n",
    "\n",
    "        if id == 0:\n",
    "            _ = scatter_plot(dates, targets[:,i], predictions[:,i], name + ' for '+ str(years[i])) \n",
    "\n",
    "    return r_years,rms_years,slope_years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Flagellate'\n",
    "units = '[mmol m-2]'\n",
    "category = 'Concentrations'\n",
    "\n",
    "if name == 'Diatom':\n",
    "    inputs_names = ['Summation_of_solar_radiation','Mean_wind_speed','Mean_air_temperature']\n",
    "    lags = 49.3 # n_intervals=3\n",
    "    # lags = 55.5 # n_intervals=4\n",
    "\n",
    "else:\n",
    "    inputs_names = ['Summation_of_solar_radiation','Mean_air_temperature','Mean_pressure', 'Mean_precipitation', 'Mean_specific_humidity']\n",
    "    lags = 24.6 # n_intervals=3\n",
    "    # lags = 18.5 # n_intervals=4\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "# Low resolution\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "#     x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "inputs,targets,indx = datasets_preparation(dataset,dataset2,name,inputs_names)\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "indx2 = ~((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "dates = dates[indx2]\n",
    "\n",
    "regr,scaler_inputs,scaler_targets,smoother,r_inputs = regressor(inputs, targets, lags)\n",
    "\n",
    "print('Metrics between input features and '+name)\n",
    "temp = pd.DataFrame(r_inputs, index=inputs_names)\n",
    "display(temp)\n",
    "\n",
    "predictions = scaling(regr,inputs,scaler_inputs,targets,scaler_targets,smoother)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,len(inputs_names), figsize = (15,6), layout='constrained')\n",
    "\n",
    "for i in range(0,len(inputs_names)):\n",
    "\n",
    "    temp = regr.coef_\n",
    "    coeff = temp.data_matrix\n",
    "    coeff = np.where(coeff==0,np.nan,coeff)\n",
    "\n",
    "    vmin = np.nanmin(coeff[0,:,:,i])\n",
    "    vmax = np.nanmax(coeff[0,:,:,i])\n",
    "\n",
    "    h = axs[i].imshow(coeff[0,:,:,i], cmap='bwr',aspect='auto', vmin=-np.maximum(np.abs(vmin),vmax), vmax=np.maximum(np.abs(vmin),vmax))\n",
    "\n",
    "    cbar = fig.colorbar(h)\n",
    "    axs[i].set_ylim(axs[i].get_ylim()[::-1])\n",
    "    axs[i].set_xlabel('Day')\n",
    "    axs[i].set_ylabel('Day')\n",
    "    fig.suptitle(inputs_names[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = np.round(np.corrcoef(np.ravel(targets),np.ravel(predictions))[0][1],3)\n",
    "rms_train = rmse(np.ravel(targets),np.ravel(predictions))\n",
    "m,_ = np.polyfit(np.ravel(targets), np.ravel(predictions), deg=1)\n",
    "slope_train = np.round(m,3)\n",
    "\n",
    "print ('The correlation coefficient during training is: ' + str(r_train))\n",
    "print ('The rmse during training is: ' + str(rms_train))\n",
    "print('The slope of the best fitting line during training is: '+str(slope_train))\n",
    "print('\\n')\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "indx2 = ~((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "dates = dates[indx2]\n",
    "\n",
    "plotting_mean_values(dates, np.reshape(targets,targets.shape[0]*targets.shape[1],order='F'), np.reshape(predictions,predictions.shape[0]*predictions.shape[1],order='F'), units, category, 'Salish Sea')\n",
    "\n",
    "season = np.mean(targets,axis=1)\n",
    "plt.plot(season)\n",
    "plt.suptitle('Long-term seasonality (2007-2020)')\n",
    "plt.show()\n",
    "\n",
    "season_train = np.tile(season,targets.shape[1]) # Broadcasting season to all training years\n",
    "season_train = np.reshape(season_train,(targets.shape[0],targets.shape[1]),order='F')\n",
    "\n",
    "season_test = np.tile(season,len(np.unique(ds.sel(time_counter = slice('2021', '2024')).time_counter.dt.year))) # Broadcasting season to all testing years\n",
    "season_test = np.reshape(season_test,(targets.shape[0],len(np.unique(ds.sel(time_counter = slice('2021', '2024')).time_counter.dt.year))),order='F')\n",
    "\n",
    "r_train_season = np.round(np.corrcoef(np.ravel(targets-season_train),np.ravel(predictions-season_train))[0][1],3)\n",
    "rms_train_season = rmse(np.ravel(targets-season_train),np.ravel(predictions-season_train))\n",
    "m,_ = np.polyfit(np.ravel(targets-season_train), np.ravel(predictions-season_train), deg=1)\n",
    "slope_train_season = np.round(m,3)\n",
    "\n",
    "print ('The correlation coefficient during training is: ' + str(r_train_season))\n",
    "print ('The rmse during training is: ' + str(rms_train_season))\n",
    "print('The slope of the best fitting line during training is: '+str(slope_train_season))\n",
    "    \n",
    "plotting_mean_values(dates, np.reshape(targets-season_train,targets.shape[0]*targets.shape[1],order='F'), np.reshape(predictions-season_train,predictions.shape[0]*predictions.shape[1],order='F'), units, category, 'Salish Sea')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2021', '2024'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "indx = ~((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "dates = dates[indx]\n",
    "\n",
    "inputs, targets, indx = datasets_preparation(dataset, dataset2, name, inputs_names)\n",
    "\n",
    "predictions = scaling(regr,inputs,scaler_inputs,targets,scaler_targets,smoother)\n",
    "\n",
    "r_test,rms_test,slope_test = evaluation(0,dates,years,targets,predictions,name)\n",
    "\n",
    "r_test = np.round(np.corrcoef(np.ravel(targets),np.ravel(predictions))[0][1],3)\n",
    "rms_test = rmse(np.ravel(targets),np.ravel(predictions))\n",
    "m,_ = np.polyfit(np.ravel(targets), np.ravel(predictions), deg=1)\n",
    "slope_test = np.round(m,3)\n",
    "\n",
    "print ('The correlation coefficient during testing is: ' + str(r_test))\n",
    "print ('The rmse during testing is: ' + str(rms_test))\n",
    "print ('The slope of the best fitting line during testing is: ' + str(slope_test))\n",
    "print ('\\n')\n",
    "\n",
    "plotting_mean_values(dates, np.reshape(targets,targets.shape[0]*targets.shape[1],order='F'), np.reshape(predictions,predictions.shape[0]*predictions.shape[1],order='F'), units, category, 'Salish Sea')\n",
    "\n",
    "r_test_season = np.round(np.corrcoef(np.ravel(targets-season_test),np.ravel(predictions-season_test))[0][1],3)\n",
    "rms_test_season = rmse(np.ravel(targets-season_test),np.ravel(predictions-season_test))\n",
    "m,_ = np.polyfit(np.ravel(targets-season_test),np.ravel(predictions-season_test), deg=1)\n",
    "slope_test_season = np.round(m,3)\n",
    "\n",
    "print ('The correlation coefficient during testing is: ' + str(r_test_season))\n",
    "print ('The rmse during testing is: ' + str(rms_test_season))\n",
    "print ('The slope of the best fitting line during testing is: ' + str(slope_test_season))\n",
    "print ('\\n')\n",
    "\n",
    "r_test_season,rms_test_season,slope_test_season = evaluation(1,dates,years,targets-season_test,predictions-season_test,name)\n",
    "plotting_mean_values(dates, np.reshape(targets-season_test,targets.shape[0]*targets.shape[1],order='F'), np.reshape(predictions-season_test,predictions.shape[0]*predictions.shape[1],order='F'), units, category, 'Salish Sea (removed seasonality)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/data/ibougoudis/MOAD/files/results/' + name + '/func_reg_s/'\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# with lzma.open(path + 'regr_all.xz', 'wb') as f:\n",
    "    \n",
    "#     dill.dump(regr, f)\n",
    "\n",
    "# with open(path + 'metrics.pkl', 'wb') as f:\n",
    "#     dill.dump([r_train,rms_train,slope_train,r_test,rms_test,slope_test,r_test_season,rms_test_season,slope_test_season], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
