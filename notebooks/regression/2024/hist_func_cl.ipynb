{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diatom concentration with functional clustering and a Histogram-based Gradient Boosting Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.ml.clustering import KMeans\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "import random\n",
    "\n",
    "import salishsea_tools.viz_tools as sa_vi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, dataset2, clusters, name):\n",
    "    \n",
    "    inputs = np.stack([\n",
    "        np.ravel(dataset2['Summation_of_solar_radiation']),\n",
    "        np.ravel(dataset2['Mean_wind_speed']),\n",
    "        np.ravel(dataset2['Mean_air_temperature']),\n",
    "        np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y))\n",
    "        ])\n",
    "    \n",
    "    x = np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter))\n",
    "\n",
    "    targets = np.ravel(dataset[name])\n",
    "    \n",
    "    indx = np.where(np.isfinite(targets) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs = inputs[:,indx[0]]\n",
    "    targets = targets[indx[0]]\n",
    "\n",
    "    clusters = np.tile(np.ravel(clusters), len(dataset.time_counter))\n",
    "    clusters = clusters[indx[0]]\n",
    "\n",
    "    inputs = inputs.transpose()\n",
    "\n",
    "    return(inputs, targets, indx, clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_target(dataset, name):\n",
    "\n",
    "    id = 0 # To know which type of clustering was used\n",
    "\n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "\n",
    "    input = dataset[name].to_numpy().reshape(*dataset[name].to_numpy().shape[:1],-1)\n",
    "    input  = np.delete(input, indx,axis=0)\n",
    "    input  = np.split(input, len(np.unique(dataset.time_counter.dt.year)),axis=0)\n",
    "\n",
    "    input  = np.nanmean(input,axis=0)\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(input).any(axis=0))) \n",
    "    input = input[:, indx[0]]\n",
    "\n",
    "    input = input.transpose()\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    input2 = FDataGrid(input)\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(input2)\n",
    "    clusters = kmeans.predict(input2)\n",
    "\n",
    "    unique, _ = np.unique(clusters, return_counts=True)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    clusters2 = xr.DataArray(clusters,\n",
    "        coords = {'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['y','x'],\n",
    "        attrs=dict(description=\"Clusters of the performed functional analysis algorithm\",\n",
    "        long_name =\"Cluster\",\n",
    "        units=\"count\"))\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize =(5,9))\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "    cmap.set_bad('gray')\n",
    "    clus = clusters2.plot(ax=ax, cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "\n",
    "    cbar = fig.colorbar(clus, ticks = unique+0.5) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    cbar.set_label('Clusters [count]')\n",
    "    ax.set_title('Functional Clustering for '+ name + ' (2007-2020)')\n",
    "\n",
    "    sa_vi.set_aspect(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(clusters,id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (Drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_drivers(dataset, dataset2,name):\n",
    "\n",
    "    id = 1 # To know which type of clustering was used\n",
    "\n",
    "    indx = np.where((dataset.time_counter.dt.month==2) & (dataset.time_counter.dt.day==29))\n",
    "\n",
    "    inputs = np.stack([\n",
    "        np.reshape(np.ravel(dataset2['Summation_of_solar_radiation']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset2['Mean_wind_speed']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset2['Mean_air_temperature']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        ])\n",
    "    \n",
    "    target= np.reshape(np.ravel(dataset[name]), (len(dataset.time_counter), len(dataset.y) * len(dataset.x)))\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(target).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    inputs =inputs[:,:,indx[0]]\n",
    "\n",
    "    # Transforming each variable individually\n",
    "    for j in range (0, len(inputs)):\n",
    "\n",
    "        temp = scale(np.ravel(inputs[j]))\n",
    "        inputs[j] = temp.reshape(inputs[j].shape)\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    inputs = np.transpose(inputs,axes=(2,1,0)) # this is the right shape for converting it to a functional variable\n",
    "    input2 = FDataGrid(inputs, np.arange(0,len(inputs[0])))\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(input2)\n",
    "    clusters = kmeans.predict(input2)\n",
    "\n",
    "    unique, _ = np.unique(clusters, return_counts=True)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    clusters2 = xr.DataArray(clusters,\n",
    "        coords = {'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['y','x'],\n",
    "        attrs=dict(description=\"Clusters of the performed functional analysis algorithm\",\n",
    "        long_name =\"Cluster\",\n",
    "        units=\"count\"))\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize =(5,9))\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', unique.max()+1)\n",
    "    cmap.set_bad('gray')\n",
    "    clus = clusters2.plot(ax=ax, cmap=cmap, vmin = unique.min(), vmax = unique.max()+1, add_colorbar=False)\n",
    "\n",
    "    cbar = fig.colorbar(clus, ticks = unique+0.5) \n",
    "    cbar.set_ticklabels(unique+1)\n",
    "    cbar.set_label('Clusters [count]')\n",
    "    ax.set_title('Functional Clustering for inputs (2007-2020)')\n",
    "\n",
    "    sa_vi.set_aspect(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(clusters, id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the data arrays\n",
    "def datasets_preparation2(variable, name, units, dataset):\n",
    "\n",
    "    # Obtaining the daily indexes\n",
    "    temp = np.reshape(np.ravel(dataset['Temperature_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x)))\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(temp).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "\n",
    "    variable_all = np.full((len(dataset.time_counter), len(dataset.y) * len(dataset.x)),np.nan)\n",
    "    variable_all[:,indx[0]] = variable\n",
    "    variable_all = np.reshape(variable_all,(len(dataset.time_counter),len(dataset.y),len(dataset.x)))\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    array = xr.DataArray(variable_all,\n",
    "        coords = {'time_counter': dataset.time_counter,'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['time_counter','y','x'],\n",
    "        attrs=dict(description= name,\n",
    "        units=units))\n",
    "        \n",
    "    return (array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_creation(path, variable, name):\n",
    "\n",
    "    temp = variable.to_dataset(name=name)\n",
    "    temp.to_netcdf(path = path + 'targets_predictions.nc', mode='a', encoding={name:{\"zlib\": True, \"complevel\": 9}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, clusters):\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "        transformers=[('drivers', StandardScaler(), [0,1,2])],remainder='passthrough'),\n",
    "        HistGradientBoostingRegressor(learning_rate=0.5, categorical_features=[3])),\n",
    "        transformer=StandardScaler())\n",
    "    \n",
    "    regr_all = []\n",
    "    r = []\n",
    "    rms = []\n",
    "    slope = []\n",
    "    \n",
    "    for i in range (0,len(np.unique(clusters))):\n",
    "        indx2 = np.where(clusters==i) # indexes of the i cluster\n",
    "        inputs2 = inputs[indx2[0]] # inputs of the i cluster\n",
    "        targets2 = targets[indx2[0]]\n",
    "\n",
    "        regr = BaggingRegressor(model, n_estimators=12, n_jobs=4).fit(inputs2,targets2)\n",
    "        \n",
    "        r.append(np.corrcoef(regr.predict(inputs2),targets2)[0][1])\n",
    "        rms.append(rmse(regr.predict(inputs2),targets2))\n",
    "        m,_ = np.polyfit(regr.predict(inputs2), targets2, deg=1)\n",
    "        slope.append(m)\n",
    "\n",
    "        regr_all.append(regr)\n",
    "\n",
    "    print ('The mean correlation coefficient during training is: ' + str(np.round(np.mean(r),3)))\n",
    "    print ('The mean rmse during training is: ' + str(np.mean(rms)))\n",
    "    print('The mean slope of the best fitting line during training is: '+str(np.round(np.mean(m),3)))\n",
    "\n",
    "    return(regr_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(targets, predictions, name):\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, predictions, deg=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(5,10), layout='constrained')\n",
    "\n",
    "    ax[0].scatter(targets,predictions, alpha = 0.2, s = 10)\n",
    "\n",
    "    lims = [np.min([ax[0].get_xlim(), ax[0].get_ylim()]),\n",
    "        np.max([ax[0].get_xlim(), ax[0].get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[0].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[0].set_xlabel('targets')\n",
    "    ax[0].set_ylabel('predictions')\n",
    "    ax[0].set_xlim(lims)\n",
    "    ax[0].set_ylim(lims)\n",
    "    ax[0].set_aspect('equal')\n",
    "\n",
    "    ax[0].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    h = ax[1].hist2d(targets,predictions, bins=100, cmap='jet', \n",
    "        range=[lims,lims], cmin=0.1, norm='log')\n",
    "    \n",
    "    ax[1].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[1].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[1].set_xlabel('targets')\n",
    "    ax[1].set_ylabel('predictions')\n",
    "    ax[1].set_aspect('equal')\n",
    "\n",
    "    fig.colorbar(h[3],ax=ax[1], location='bottom')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_criteria(dates, variable, year_variable, title):\n",
    "    \n",
    "    indx = pd.DatetimeIndex(dates)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter= ax.scatter(dates,variable, marker='.', c=indx.month)\n",
    "    plt.xticks(rotation=70)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=['February','March','April'])\n",
    "    ax.plot(dates[(indx.month == 3) & (indx.day == 15)], year_variable,color='red',marker='*')\n",
    "    fig.suptitle(title + ' (15 Feb - 30 Apr)')\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates, mean_targets, mean_predictions, units, category):\n",
    "\n",
    "    fig, _ = plt.subplots(figsize=(9,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(mean_targets)\n",
    "    mean_targets[75]=np.ma.masked\n",
    "    mean_targets[150]=np.ma.masked\n",
    "    mean_targets[225]=np.ma.masked\n",
    "\n",
    "    mean_predictions = np.ma.array(mean_predictions)\n",
    "    mean_predictions[75]=np.ma.masked\n",
    "    mean_predictions[150]=np.ma.masked\n",
    "    mean_predictions[225]=np.ma.masked\n",
    "\n",
    "    plt.plot(mean_targets, label = 'targets')\n",
    "    plt.plot(mean_predictions, label = 'predictions')\n",
    "    plt.xlabel('Days')\n",
    "    # plt.xticks(rotation=70)\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr)')\n",
    "    plt.legend()\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_maps(targets, predictions, name, units):\n",
    "\n",
    "    fig, ax = plt.subplots(2,2, figsize = (10,15), layout='tight')\n",
    "\n",
    "    cmap = plt.get_cmap('cubehelix')\n",
    "    cmap.set_bad('gray')\n",
    "\n",
    "    targets.plot(ax=ax[0,0], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    predictions.plot(ax=ax[0,1], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    (targets-predictions).plot(ax=ax[1,0], cmap=cmap, cbar_kwargs={'label': name + ' ' + units})\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "        bottom=0.1, \n",
    "        right=0.95, \n",
    "        top=0.95, \n",
    "        wspace=0.35, \n",
    "        hspace=0.35)\n",
    "\n",
    "    sa_vi.set_aspect(ax[0,0])\n",
    "    sa_vi.set_aspect(ax[0,1])\n",
    "    sa_vi.set_aspect(ax[1,0])\n",
    "\n",
    "    ax[0,0].title.set_text('Targets')\n",
    "    ax[0,1].title.set_text('Predictions')\n",
    "    ax[1,0].title.set_text('Targets-Predictions')\n",
    "    ax[1,1].axis('off')\n",
    "\n",
    "    fig.suptitle(name + ' '+ str(targets.time_counter.dt.date.values))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (regr_all, clusters0, ds, ds2, name, units):\n",
    "\n",
    "    years = np.unique(ds.time_counter.dt.year)\n",
    "\n",
    "    # For every year\n",
    "    r_years = np.array([])\n",
    "    rms_years = np.array([])\n",
    "    slope_years = np.array([])\n",
    "\n",
    "    # The data arrays \n",
    "    targets_all = []\n",
    "    predictions_all = []\n",
    "\n",
    "    for year in (years):\n",
    "\n",
    "        dataset = ds.sel(time_counter=str(year))\n",
    "        dataset2 = ds2.sel(time_counter=str(year))\n",
    "\n",
    "        inputs, targets, indx, clusters  = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "        # Predictions for each regressor\n",
    "\n",
    "        predictions = np.full(len(targets),np.nan) # size of a year without nans\n",
    "        for i in range (0,len(np.unique(clusters))):\n",
    "            indx2 = np.where(clusters==i) # indexes of the j cluster\n",
    "            inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "            predictions[indx2[0]] = regr_all[i].predict(inputs2) # putting them in the right place\n",
    "\n",
    "        # Calculating the annual time-series\n",
    "        m_year = scatter_plot(targets, predictions, name + ' for '+ str(year)) \n",
    "        r_year = np.round(np.corrcoef(targets, predictions)[0][1],3)\n",
    "        rms_year = rmse(targets, predictions)\n",
    "        \n",
    "        r_years = np.append(r_years,r_year)\n",
    "        rms_years = np.append(rms_years,rms_year)\n",
    "        slope_years = np.append(slope_years,m_year)\n",
    "\n",
    "        # Daily arrays\n",
    "        targets = np.reshape(targets,(len(dataset.time_counter), int(len(indx[0]) / len(dataset.time_counter))))\n",
    "        predictions = np.reshape(predictions,(len(dataset.time_counter), int(len(indx[0]) / len(dataset.time_counter))))\n",
    "        targets_all.append (datasets_preparation2(targets, name + ' _targets', units, dataset))\n",
    "        predictions_all.append(datasets_preparation2(predictions, name + ' _predictions', units, dataset))   \n",
    "\n",
    "    # Daily arrays\n",
    "    targets_all = xr.concat(targets_all, dim='time_counter')\n",
    "    predictions_all = xr.concat(predictions_all, dim='time_counter')\n",
    "    \n",
    "    return(r_years, rms_years, slope_years, targets_all, predictions_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Diatom_Production_Rate'\n",
    "units = '[mmol N m-2 s-1]'\n",
    "category = 'production rates'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# ds2 = ds2.isel(y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "#     x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "# Selecting the clustering input (drivers or target)\n",
    "# clusters0, id = func_clust_target(dataset, name)\n",
    "clusters0, id = func_clust_drivers(dataset,dataset2, name)\n",
    "\n",
    "inputs, targets, _, clusters = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "regr_all = regressor(inputs, targets, clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(time_counter = slice('2021', '2024'))\n",
    "ds2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "dates = pd.DatetimeIndex(ds['time_counter'].values)\n",
    "\n",
    "r_years, rms_years, slope_years, targets_all, predictions_all = evaluation(regr_all, clusters0, ds,ds2, name, units)\n",
    "\n",
    "r_days = xr.corr(targets_all,predictions_all, dim=['x','y'])\n",
    "rms_days = xs.rmse(targets_all,predictions_all, dim=['x','y'], skipna=True)\n",
    "slope_days = xs.linslope(targets_all,predictions_all, dim=['x','y'], skipna=True)\n",
    "\n",
    "mean_targets = targets_all.mean(dim=['x','y'], skipna=True)\n",
    "mean_predictions = predictions_all.mean(dim=['x','y'], skipna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_criteria(dates, r_days, r_years, 'Correlation Coefficients')\n",
    "plotting_criteria(dates, rms_days, rms_years, 'Root Mean Square Errors')\n",
    "plotting_criteria(dates, slope_days, slope_years, 'Slopes of the best fitting line')\n",
    "\n",
    "plotting_mean_values(dates, mean_targets, mean_predictions, units, category)\n",
    "\n",
    "# Daily maps\n",
    "maps = random.sample(sorted(np.arange(0,len(targets_all.time_counter))),10)\n",
    "for i in maps:\n",
    "\n",
    "    idx = np.isfinite(np.ravel(targets_all[i]))\n",
    "    scatter_plot(np.ravel(targets_all[i])[idx], np.ravel(predictions_all[i])[idx], name + ' '+ str(targets_all[i].time_counter.dt.date.values))\n",
    "\n",
    "    plotting_maps(targets_all[i], predictions_all[i], name, units)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if id == 0:\n",
    "    path = '/data/ibougoudis/MOAD/files/results/' + name + '/func_cl_target_ext_new/'\n",
    "else:\n",
    "    path = '/data/ibougoudis/MOAD/files/results/' + name + '/func_cl_drivers_ext_old/'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "with lzma.open(path + 'regr.xz', 'wb') as f:\n",
    "    dill.dump(regr_all, f)\n",
    "\n",
    "file_creation(path, targets_all, 'Targets')\n",
    "file_creation(path, predictions_all, 'Predictions')\n",
    "file_creation(path, (targets_all-predictions_all), 'Targets - Predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
