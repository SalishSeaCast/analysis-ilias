{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for collecting data, training and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import salishsea_tools.viz_tools as sa_vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the file\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(time_counter = (np.arange(0, len(ds.time_counter),2)), \n",
    "    y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(time_counter = (np.arange(0, len(ds2.time_counter),2)), \n",
    "    y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "# Selecting the first 2 years\n",
    "dataset = ds.sel(time_counter = slice('2007-2-15', '2009-4-30'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007-2-15', '2009-4-30'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am packing all of them in one variable, named drivers\n",
    "\n",
    "drivers = np.stack([np.ravel(dataset['Temperature_(0m-15m)']),\n",
    "        np.ravel(dataset['Temperature_(15m-100m)']), \n",
    "        np.ravel(dataset['Salinity_(0m-15m)']),\n",
    "        np.ravel(dataset['Salinity_(15m-100m)']),\n",
    "        np.ravel(dataset2['Summation_of_solar_radiation']),\n",
    "        np.ravel(dataset2['Mean_wind_speed']),\n",
    "        np.ravel(dataset2['Mean_air_temperature']),\n",
    "        np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter)),\n",
    "        np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y)),\n",
    "        np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y))\n",
    "        ])\n",
    "\n",
    "# Removing of nans\n",
    "indx = np.where(~np.isnan(drivers).any(axis=0))\n",
    "drivers = drivers[:,indx[0]]\n",
    "\n",
    "diat = np.ravel(dataset['Diatom'])\n",
    "diat = diat[indx]\n",
    "\n",
    "# Transpose to bring it to the format (samples, features)\n",
    "drivers = drivers.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = ds.sel(time_counter = slice('2021-2-15', '2024-4-30'))\n",
    "dataset02 = ds2.sel(time_counter = slice('2021-2-15', '2024-4-30'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am packing all of them in one variable, named drivers\n",
    "\n",
    "drivers2 = np.stack([np.ravel(dataset0['Temperature_(0m-15m)']),\n",
    "        np.ravel(dataset0['Temperature_(15m-100m)']), \n",
    "        np.ravel(dataset0['Salinity_(0m-15m)']),\n",
    "        np.ravel(dataset0['Salinity_(15m-100m)']),\n",
    "        np.ravel(dataset02['Summation_of_solar_radiation']),\n",
    "        np.ravel(dataset02['Mean_wind_speed']),\n",
    "        np.ravel(dataset02['Mean_air_temperature']),\n",
    "        np.tile(np.repeat(dataset0.y, len(dataset0.x)), len(dataset0.time_counter)),\n",
    "        np.tile(dataset0.x, len(dataset0.time_counter)*len(dataset0.y)),\n",
    "        np.repeat(dataset0.time_counter.dt.dayofyear, len(dataset0.x)*len(dataset0.y))\n",
    "        ])\n",
    "\n",
    "# Removing of nans\n",
    "indx = np.where(~np.isnan(drivers2).any(axis=0))\n",
    "drivers2 = drivers2[:,indx[0]]\n",
    "\n",
    "diat2 = np.ravel(dataset0['Diatom'])\n",
    "diat2 = diat2[indx]\n",
    "\n",
    "# Transpose to bring it to the format (samples, features)\n",
    "drivers2 = drivers2.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "\n",
    "model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "    transformers=[],remainder=StandardScaler()),\n",
    "    Nystroem(kernel=\"poly\", degree=2, n_components=100),\n",
    "    LinearSVR(max_iter=2000)),\n",
    "    transformer=MinMaxScaler())\n",
    "\n",
    "regr = BaggingRegressor(model, n_estimators=12, n_jobs=-1).fit(drivers,diat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regr.predict(drivers2)\n",
    "print(np.corrcoef(diat2,predictions)[0][1])\n",
    "print(rmse(diat2,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
