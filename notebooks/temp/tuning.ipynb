{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to tune the hyperparameters of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_preparation(dataset, dataset2):\n",
    "    \n",
    "    drivers = np.stack([np.ravel(dataset['Temperature_(0m-15m)']),\n",
    "        np.ravel(dataset['Temperature_(15m-100m)']), \n",
    "        np.ravel(dataset['Salinity_(0m-15m)']),\n",
    "        np.ravel(dataset['Salinity_(15m-100m)']),\n",
    "        np.ravel(dataset2['Summation_of_solar_radiation']),\n",
    "        np.ravel(dataset2['Mean_wind_speed']),\n",
    "        np.ravel(dataset2['Mean_air_temperature']),\n",
    "        np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter)),\n",
    "        np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y)),\n",
    "        np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y))\n",
    "        ])\n",
    "\n",
    "    indx = np.where(~np.isnan(drivers).any(axis=0) & (drivers[8]>10) & ((drivers[8]>100) | (drivers[7]<880)))\n",
    "    drivers = drivers[:,indx[0]]\n",
    "\n",
    "    diat = np.ravel(dataset['Diatom'])\n",
    "    diat = diat[indx[0]]\n",
    "\n",
    "    return(drivers, diat, indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets):\n",
    "\n",
    "    # Tuning of parameters      \n",
    "    params = {'eta':[0.1,0.3], 'max_depth':[10,20,30,50], 'subsample': [0.9], 'colsample_bynode': [1]}                                        \n",
    "    scale = preprocessing.MinMaxScaler()                                                                               \n",
    "\n",
    "    inputs = inputs.transpose()\n",
    "    X_train, _, y_train, _ = train_test_split(inputs, targets, train_size=0.20)\n",
    "\n",
    "    inputs = scale.fit_transform(inputs)\n",
    "   \n",
    "    model = xgb.XGBRegressor()\n",
    "\n",
    "    random_search = GridSearchCV(estimator=model, param_grid=params, scoring='r2',\n",
    "        cv=3, n_jobs=-1, verbose=3, pre_dispatch='2*n_jobs', return_train_score=True)\n",
    "\n",
    "    random_search.fit(X_train,y_train)\n",
    "\n",
    "    print('\\n', 'The best parameters are', random_search.best_params_)\n",
    "\n",
    "    return (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.1, max_depth=10, subsample=0.9;, score=(train=0.915, test=0.840) total time=   7.6s\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.3, max_depth=10, subsample=0.9;, score=(train=0.969, test=0.862) total time=   7.8s\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.3, max_depth=10, subsample=0.9;, score=(train=0.969, test=0.858) total time=   9.9s\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.3, max_depth=10, subsample=0.9;, score=(train=0.969, test=0.861) total time=  10.3s\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.1, max_depth=10, subsample=0.9;, score=(train=0.913, test=0.841) total time=  10.5s\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.1, max_depth=10, subsample=0.9;, score=(train=0.913, test=0.840) total time=  11.8s\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.3, max_depth=20, subsample=0.9;, score=(train=1.000, test=0.865) total time=  49.2s\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.3, max_depth=20, subsample=0.9;, score=(train=1.000, test=0.864) total time=  50.9s\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.3, max_depth=20, subsample=0.9;, score=(train=1.000, test=0.858) total time=  51.9s\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.3, max_depth=50, subsample=0.9;, score=(train=1.000, test=0.855) total time= 1.0min\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.3, max_depth=30, subsample=0.9;, score=(train=1.000, test=0.854) total time= 1.1min\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.3, max_depth=30, subsample=0.9;, score=(train=1.000, test=0.861) total time= 1.1min\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.3, max_depth=30, subsample=0.9;, score=(train=1.000, test=0.861) total time= 1.2min\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.3, max_depth=50, subsample=0.9;, score=(train=1.000, test=0.861) total time= 1.3min\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.3, max_depth=50, subsample=0.9;, score=(train=1.000, test=0.860) total time= 1.3min\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.1, max_depth=20, subsample=0.9;, score=(train=1.000, test=0.883) total time= 1.9min\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.1, max_depth=20, subsample=0.9;, score=(train=1.000, test=0.879) total time= 1.9min\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.1, max_depth=20, subsample=0.9;, score=(train=1.000, test=0.885) total time= 1.9min\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.1, max_depth=30, subsample=0.9;, score=(train=1.000, test=0.877) total time= 2.4min\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.1, max_depth=50, subsample=0.9;, score=(train=1.000, test=0.879) total time= 2.4min\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.1, max_depth=50, subsample=0.9;, score=(train=1.000, test=0.882) total time= 2.4min\n",
      "[CV 2/3] END colsample_bynode=1, eta=0.1, max_depth=50, subsample=0.9;, score=(train=1.000, test=0.876) total time= 2.4min\n",
      "[CV 3/3] END colsample_bynode=1, eta=0.1, max_depth=30, subsample=0.9;, score=(train=1.000, test=0.882) total time= 2.4min\n",
      "[CV 1/3] END colsample_bynode=1, eta=0.1, max_depth=30, subsample=0.9;, score=(train=1.000, test=0.879) total time= 2.6min\n",
      "\n",
      " The best parameters are {'colsample_bynode': 1, 'eta': 0.1, 'max_depth': 20, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(time_counter = (np.arange(0, len(ds.time_counter),2)), \n",
    "    y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(time_counter = (np.arange(0, len(ds2.time_counter),2)), \n",
    "    y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "drivers, diat, _ = datasets_preparation(dataset, dataset2)\n",
    "\n",
    "regr = regressor(drivers, diat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters for each case (Diatom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPRegressor(alpha=0.001, learning_rate='invscaling', tol=1e-06, epsilon=1e-07, power_t=1)\n",
    "# model = ExtraTreesRegressor(max_features='sqrt')\n",
    "# model = GradientBoostingRegressor(criterion='squared_error',learning_rate=0.5,subsample=0.5,min_samples_split=5,min_samples_leaf=6,max_depth=8,max_features='log2')\n",
    "# model = HistGradientBoostingRegressor(learning_rate=0.5, max_iter=400,max_leaf_nodes=None,min_samples_leaf=200,max_bins=100)\n",
    "# model = DecisionTreeRegressor(min_samples_leaf=15,min_samples_split=10)\n",
    "# model = KNeighborsRegressor(leaf_size=10, metric='cityblock', n_neighbors=3, p=1, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
