{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary script for predicting Flagellate production rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from skfda.representation.grid import FDataGrid\n",
    "\n",
    "from skfda.ml.clustering import KMeans\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dill\n",
    "import random\n",
    "\n",
    "import salishsea_tools.viz_tools as sa_vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_preparation(dataset, dataset2, clusters, name):\n",
    "    \n",
    "    drivers = np.stack([np.ravel(dataset['Temperature_(0m-15m)']),\n",
    "        np.ravel(dataset['Temperature_(15m-100m)']), \n",
    "        np.ravel(dataset['Salinity_(0m-15m)']),\n",
    "        np.ravel(dataset['Salinity_(15m-100m)']),\n",
    "        np.ravel(dataset2['Summation_of_solar_radiation']),\n",
    "        np.ravel(dataset2['Mean_wind_speed']),\n",
    "        np.ravel(dataset2['Mean_air_temperature']),\n",
    "        np.repeat(dataset.time_counter.dt.dayofyear, len(dataset.x)*len(dataset.y))\n",
    "        ])\n",
    "    \n",
    "    x = np.tile(dataset.x, len(dataset.time_counter)*len(dataset.y))\n",
    "    y = np.tile(np.repeat(dataset.y, len(dataset.x)), len(dataset.time_counter))\n",
    "\n",
    "    indx = np.where(~np.isnan(drivers).any(axis=0) & (x>10) & ((x>100) | (y<880)))\n",
    "    drivers = drivers[:,indx[0]]\n",
    "\n",
    "    phyto = np.ravel(dataset[name])\n",
    "    phyto = phyto[indx[0]]\n",
    "\n",
    "    clusters = np.tile(np.ravel(clusters), len(dataset.time_counter))\n",
    "    clusters = clusters[indx[0]]\n",
    "\n",
    "    drivers = drivers.transpose()\n",
    "\n",
    "    return(drivers, phyto, indx, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_target(dataset, name):\n",
    "\n",
    "    input = np.reshape(np.ravel(dataset[name]), (len(dataset.time_counter), len(dataset.y) * len(dataset.x)))\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(input).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "    input = input[:, indx[0]]\n",
    "\n",
    "    input = input.transpose()\n",
    "    input = minmax_scale(input)\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    input2 = FDataGrid(input)\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(input2)\n",
    "    clusters = kmeans.predict(input2)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    clusters2 = xr.DataArray(clusters,\n",
    "        coords = {'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['y','x'],\n",
    "        attrs=dict(description=\"Clusters of the performed functional analysis algorithm\",\n",
    "        long_name =\"Cluster\",\n",
    "        units=\"count\"),)\n",
    "    \n",
    "    # Plotting\n",
    "    clusters2.plot()\n",
    "    plt.title('Functional Clustering for '+ name)\n",
    "\n",
    "    return(clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Clustering (drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_clust_drivers(dataset, dataset2, name):\n",
    "\n",
    "    input = np.stack([np.reshape(np.ravel(dataset['Temperature_(0m-15m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset['Temperature_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset['Salinity_(0m-15m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset['Salinity_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset2['Summation_of_solar_radiation']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset2['Mean_wind_speed']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        np.reshape(np.ravel(dataset2['Mean_air_temperature']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "        ])\n",
    "\n",
    "    x =  np.tile(dataset.x, len(dataset.y))\n",
    "    y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "    indx = np.where((~np.isnan(input[1]).any(axis=0)) & (x>10) & ((x>100) | (y<880))) # input[1] because this variable is down to 100m\n",
    "    input = input[:,:,indx[0]]\n",
    "\n",
    "    input = np.transpose(input,axes=(0,2,1)) # this is the right shape for preprocessing the data\n",
    "\n",
    "    # Transforming each variable individually\n",
    "    input[0] = minmax_scale(input[0])\n",
    "    input[1] = minmax_scale(input[1])\n",
    "    input[2] = minmax_scale(input[2])\n",
    "    input[3] = minmax_scale(input[3])\n",
    "    input[4] = minmax_scale(input[4])\n",
    "    input[5] = minmax_scale(input[5])\n",
    "    input[6] = minmax_scale(input[6])\n",
    "\n",
    "    # Converting it to an appropriate format for functional clustering\n",
    "    input = np.transpose(input,axes=(1,2,0)) # this is the right shape for converting it to a functional variable\n",
    "    input2 = FDataGrid(input, np.arange(0,len(dataset.time_counter)))\n",
    "\n",
    "    # Training\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(input2)\n",
    "    clusters = kmeans.predict(input2)\n",
    "\n",
    "    # Creating the map\n",
    "    indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "    indx2[indx[0]] = clusters\n",
    "    clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    clusters2 = xr.DataArray(clusters,\n",
    "        coords = {'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['y','x'],\n",
    "        attrs=dict(description=\"Clusters of the performed functional analysis algorithm\",\n",
    "        long_name =\"Cluster\",\n",
    "        units=\"count\"),)\n",
    "    \n",
    "    # Plotting\n",
    "    clusters2.plot()\n",
    "    plt.title('Functional Clustering for drivers')\n",
    "\n",
    "    return(clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, clusters):\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "        transformers=[('temporal', OneHotEncoder(), [7])],remainder=MinMaxScaler()),\n",
    "        xgb.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)),\n",
    "        transformer=MinMaxScaler())\n",
    "    \n",
    "    regr_all = []\n",
    "    \n",
    "    for i in range (0,len(np.unique(clusters))):\n",
    "        indx2 = np.where(clusters==i) # indexes of the j cluster\n",
    "        inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "        targets2 = targets[indx2]\n",
    "\n",
    "        regr = BaggingRegressor(model, n_estimators=12, n_jobs=4).fit(inputs2,targets2)\n",
    "        \n",
    "        regr_all.append(regr)\n",
    "\n",
    "    return(regr_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Other Years (Anually))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor2 (inputs, targets, clusters):\n",
    "\n",
    "    # Outputs for each regressor\n",
    "    outputs = []\n",
    "    outputs_all = np.full(len(targets),np.nan) # size of a year without nans\n",
    "    \n",
    "    for i in range (0,len(np.unique(clusters))):\n",
    "\n",
    "        indx2 = np.where(clusters==i) # indexes of the j cluster\n",
    "        inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "        outputs.append(regr_all[i].predict(inputs2))\n",
    "\n",
    "        outputs_all[indx2] = outputs[i] # putting them in the right place\n",
    "      \n",
    "    m, _ = np.polyfit(targets, outputs_all, deg=1)\n",
    "    r = np.round(np.corrcoef(targets, outputs_all)[0][1],3)\n",
    "    rms = rmse(targets, outputs_all)\n",
    "\n",
    "    return (r, rms, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Other Years (Daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor3 (inputs, targets, clusters):\n",
    "\n",
    "    # Outputs for each regressor\n",
    "    outputs = []\n",
    "    outputs_all = np.full(len(targets),np.nan) # size of a day without nans\n",
    "\n",
    "    for j in range (0,len(np.unique(clusters))):\n",
    "\n",
    "        indx2 = np.where(clusters==j) # indexes of the j cluster\n",
    "        inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "        outputs.append(regr_all[j].predict(inputs2))\n",
    "        \n",
    "        outputs_all[indx2] = outputs[j] # putting them in the right place\n",
    "       \n",
    "    ave_model = np.mean(outputs_all)\n",
    "\n",
    "    m, _ = np.polyfit(targets, outputs_all, deg=1)\n",
    "    r = np.round(np.corrcoef(targets, outputs_all)[0][1],3)\n",
    "    rms = rmse(targets, outputs_all)\n",
    "\n",
    "    return (r, rms, m, ave_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor (Daily Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor4 (inputs, targets, indx, targets_i, clusters, name):\n",
    "    \n",
    "    # Outputs for each regressor\n",
    "    outputs = []\n",
    "    outputs_all = np.full(len(targets),np.nan) # size of a day without nans\n",
    "\n",
    "    for j in range (0,len(np.unique(clusters))):\n",
    "\n",
    "        indx2 = np.where(clusters==j) # indexes of the j cluster\n",
    "        inputs2 = inputs[indx2[0]] # inputs of the j cluster\n",
    "        outputs.append(regr_all[j].predict(inputs2))\n",
    "        \n",
    "        outputs_all[indx2] = outputs[j] # putting them in the right place\n",
    "        \n",
    "    m = scatter_plot(targets, outputs_all, name +' ' + str(i.dt.date.values)) \n",
    "\n",
    "    # Creating a variable with nans\n",
    "    model = np.full((len(targets_i.y)*len(targets_i.x)),np.nan) # size of a day without nans\n",
    "    model [indx[0]]= outputs_all # putting them in the right place\n",
    "    model = np.reshape(model,(len(targets_i.y),len(targets_i.x))) \n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    model = xr.DataArray(model,\n",
    "        coords = {'y': targets_i.y, 'x': targets_i.x},\n",
    "        dims = ['y','x'],\n",
    "        attrs=dict( long_name = name + \"Concentration\",\n",
    "        units=\"mmol m-2\"),)\n",
    "                        \n",
    "    plotting3(targets, model, targets_i, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing (targets, outputs, m):\n",
    "\n",
    "    print ('The amount of data points is', outputs.size)\n",
    "    print ('The slope of the best fitting line is ', np.round(m,3))\n",
    "    print ('The correlation coefficient is:', np.round(np.corrcoef(targets, outputs)[0][1],3))\n",
    "    print ('The root mean square error is:', rmse(targets,outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(targets, outputs, variable_name):\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, outputs, deg=1)\n",
    "\n",
    "    printing(targets, outputs, m)\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(5,10), layout='constrained')\n",
    "\n",
    "    ax[0].scatter(targets,outputs, alpha = 0.2, s = 10)\n",
    "\n",
    "    lims = [np.min([ax[0].get_xlim(), ax[0].get_ylim()]),\n",
    "        np.max([ax[0].get_xlim(), ax[0].get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[0].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[0].set_xlabel('targets')\n",
    "    ax[0].set_ylabel('outputs')\n",
    "    ax[0].set_xlim(lims)\n",
    "    ax[0].set_ylim(lims)\n",
    "    ax[0].set_aspect('equal')\n",
    "\n",
    "    ax[0].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    h = ax[1].hist2d(targets,outputs, bins=100, cmap='jet', \n",
    "        range=[lims,lims], cmin=0.1, norm='log')\n",
    "    \n",
    "    ax[1].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[1].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[1].set_xlabel('targets')\n",
    "    ax[1].set_ylabel('outputs')\n",
    "    ax[1].set_aspect('equal')\n",
    "\n",
    "    fig.colorbar(h[3],ax=ax[1], location='bottom')\n",
    "\n",
    "    fig.suptitle(variable_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return (m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Other Years (Annually))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(variable, name):\n",
    "\n",
    "    plt.plot(years,variable, marker = '.', linestyle = '')\n",
    "    plt.xlabel('Years')\n",
    "    plt.ylabel(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Other Years (Daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting2(variable,title):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter= ax.scatter(dates,variable, marker='.', c=pd.DatetimeIndex(dates).month)\n",
    "\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=['February','March','April'])\n",
    "    fig.suptitle('Daily ' + title + ' (15 Feb - 30 Apr)')\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Daily Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting3(targets, model, targets_i, name):\n",
    "\n",
    "    fig, ax = plt.subplots(2,2, figsize = (10,15))\n",
    "\n",
    "    cmap = plt.get_cmap('cubehelix')\n",
    "    cmap.set_bad('gray')\n",
    "\n",
    "    targets_i.plot(ax=ax[0,0], cmap=cmap, vmin = targets.min(), vmax =targets.max(), cbar_kwargs={'label': name + ' Concentration  [mmol m-2]'})\n",
    "    model.plot(ax=ax[0,1], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' Concentration  [mmol m-2]'})\n",
    "    ((targets_i-model) / targets_i * 100).plot(ax=ax[1,0], cmap=cmap, cbar_kwargs={'label': name + ' Concentration  [percentage]'})\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "        bottom=0.1, \n",
    "        right=0.95, \n",
    "        top=0.95, \n",
    "        wspace=0.35, \n",
    "        hspace=0.35)\n",
    "\n",
    "    sa_vi.set_aspect(ax[0,0])\n",
    "    sa_vi.set_aspect(ax[0,1])\n",
    "    sa_vi.set_aspect(ax[1,0])\n",
    "\n",
    "\n",
    "    ax[0,0].title.set_text(name + ' (targets)')\n",
    "    ax[0,1].title.set_text(name + ' (outputs)')\n",
    "    ax[1,0].title.set_text('targets - outputs')\n",
    "    ax[1,1].axis('off')\n",
    "\n",
    "    fig.suptitle(str(i.dt.date.values))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Flagellate'\n",
    "\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "# ds = ds.isel(time_counter = (np.arange(0, len(ds.time_counter),2)), \n",
    "#     y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# ds2 = ds2.isel(time_counter = (np.arange(0, len(ds2.time_counter),2)), \n",
    "#     y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "#     x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "# Can choose the inputs of the functional clustering map - drivers or target\n",
    "clusters0 = func_clust_drivers(dataset, dataset2, name)\n",
    "\n",
    "inputs, targets, _, clusters = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "regr_all = regressor(inputs, targets, clusters)\n",
    "\n",
    "ds = ds.sel(time_counter = slice('2021', '2024'))\n",
    "ds2 = ds2.sel(time_counter = slice('2021', '2024'))\n",
    "\n",
    "dates = pd.DatetimeIndex(ds['time_counter'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years (Anually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range (2021,2025)\n",
    "\n",
    "r_all = np.array([])\n",
    "rms_all = np.array([])\n",
    "slope_all = np.array([])\n",
    "\n",
    "for i in tqdm(years):\n",
    "    \n",
    "    dataset = ds.sel(time_counter=str(i))\n",
    "    dataset2 = ds2.sel(time_counter=str(i))\n",
    "\n",
    "    inputs, targets, _, clusters = datasets_preparation(dataset,dataset2, clusters0, name)\n",
    "\n",
    "    r, rms, m = regressor2(inputs, targets, clusters)\n",
    "\n",
    "    r_all = np.append(r_all,r)\n",
    "    rms_all = np.append(rms_all,rms)\n",
    "    slope_all = np.append(slope_all,m)\n",
    "\n",
    "plotting(r_all, name)\n",
    "plotting(rms_all, name)\n",
    "plotting(slope_all, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Years (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all = np.array([])\n",
    "rms_all = np.array([])\n",
    "slope_all = np.array([])\n",
    "\n",
    "mean_meas = np.array([])\n",
    "mean_model = np.array([])\n",
    "\n",
    "for i in tqdm(ds.time_counter):\n",
    "\n",
    "    dataset = ds.sel(time_counter=slice(i,i))\n",
    "    dataset2 = ds2.sel(time_counter=slice(i,i))\n",
    "\n",
    "    inputs, targets, indx, clusters = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "    ave_meas = np.mean(targets)\n",
    "\n",
    "    r, rms, m, ave_model = regressor3(inputs, targets, clusters)\n",
    "\n",
    "    r_all = np.append(r_all,r)\n",
    "    rms_all = np.append(rms_all,rms)\n",
    "    slope_all = np.append(slope_all,m)\n",
    "\n",
    "    mean_meas = np.append(mean_meas,ave_meas)\n",
    "    mean_model = np.append(mean_model,ave_model)\n",
    "\n",
    "plotting2(r_all, 'Correlation Coefficients')\n",
    "plotting2(rms_all, 'Root Mean Square Errors')\n",
    "plotting2(slope_all, 'Slope of the best fitting line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = random.sample(sorted(ds.time_counter),10)\n",
    "\n",
    "for i in tqdm(maps):\n",
    "\n",
    "    dataset = ds.sel(time_counter=slice(i,i))\n",
    "    dataset2 = ds2.sel(time_counter=slice(i,i))\n",
    "    inputs, targets, indx, clusters = datasets_preparation(dataset, dataset2, clusters0, name)\n",
    "\n",
    "    targets_i = dataset[name]\n",
    "\n",
    "    regressor4(inputs, targets, indx, targets_i, clusters, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates,(mean_meas), marker = '.', linestyle = '', label = 'targets')\n",
    "plt.plot(dates,(mean_model), marker = '.', linestyle = '', label = 'outputs')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel(name + ' Concentration [mmol m-2]')\n",
    "plt.suptitle('Daily Time-series')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
