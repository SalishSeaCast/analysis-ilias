{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test notebook for functional clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skfda.ml.clustering import KMeans\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from skfda.representation.grid import FDataGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the file\n",
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "\n",
    "ds = ds.isel(time_counter = (np.arange(0, len(ds.time_counter),2)), \n",
    "    y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# Selecting the first 2 years\n",
    "dataset = ds.sel(time_counter = slice('2007-2-15', '2020-4-30'))\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.stack([np.reshape(np.ravel(dataset['Temperature_(0m-15m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset['Temperature_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset['Salinity_(0m-15m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset['Salinity_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset2['Summation_of_solar_radiation']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset2['Mean_wind_speed']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset2['Mean_air_temperature']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    ])\n",
    "\n",
    "x =  np.tile(dataset.x, len(dataset.y))\n",
    "y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "indx = np.where((~np.isnan(test[1]).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "test = test[:,:,indx[0]]\n",
    "\n",
    "test = np.transpose(test,axes=(0,2,1))\n",
    "\n",
    "# test[0] = minmax_scale(test[0])\n",
    "# test[1] = minmax_scale(test[1])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.reshape(np.ravel(dataset['Flagellate']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x)))\n",
    "test2 = np.reshape(np.ravel(dataset['Diatom']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x)))\n",
    "\n",
    "indx = np.where((~np.isnan(test2).any(axis=0)) & (x>10) & ((x>100) | (y<880)))\n",
    "test1 = test1[:,indx[0]]\n",
    "test2 = test2[:,indx[0]]\n",
    "\n",
    "test1 = test1.transpose()\n",
    "test2 = test2.transpose()\n",
    "\n",
    "test1 = minmax_scale(test1)\n",
    "test2 = minmax_scale(test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.stack([test1,test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = np.transpose(test,axes=(1,2,0))\n",
    "test.shape\n",
    "test = FDataGrid(test, np.arange(0,len(dataset.time_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(test)\n",
    "clusters = kmeans.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx2 = np.full(len(dataset.y) * len(dataset.x),np.nan)\n",
    "indx2[indx[0]] = clusters\n",
    "clusters = np.reshape(indx2,(len(dataset.y),len(dataset.x))) \n",
    "\n",
    "# Preparation of the dataarray \n",
    "clusters= xr.DataArray(clusters,\n",
    "    coords = {'y': dataset.y, 'x': dataset.x},\n",
    "    dims = ['y','x'],\n",
    "    attrs=dict(description=\"Clusters of the performed self organizing map algorithm\",\n",
    "    long_name =\"Cluster\",\n",
    "    units=\"count\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/data/ibougoudis/MOAD/files/integrated_original.nc')\n",
    "ds2 = xr.open_dataset('/data/ibougoudis/MOAD/files/external_inputs.nc')\n",
    "\n",
    "ds = ds.isel(time_counter = (np.arange(0, len(ds.time_counter),2)), \n",
    "    y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "    x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "ds2 = ds2.isel(time_counter = (np.arange(0, len(ds2.time_counter),2)), \n",
    "    y=(np.arange(ds2.y[0], ds2.y[-1], 5)), \n",
    "    x=(np.arange(ds2.x[0], ds2.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "dataset2 = ds2.sel(time_counter = slice('2007', '2020'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.stack([np.reshape(np.ravel(dataset['Temperature_(0m-15m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    np.reshape(np.ravel(dataset['Temperature_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    # np.reshape(np.ravel(dataset['Salinity_(0m-15m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    # np.reshape(np.ravel(dataset['Salinity_(15m-100m)']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    # np.reshape(np.ravel(dataset2['Summation_of_solar_radiation']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    # np.reshape(np.ravel(dataset2['Mean_wind_speed']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    # np.reshape(np.ravel(dataset2['Mean_air_temperature']), (len(dataset.time_counter), len(dataset.y) * len(dataset.x))),\n",
    "    ])\n",
    "\n",
    "x =  np.tile(dataset.x, len(dataset.y))\n",
    "y =  np.tile(np.repeat(dataset.y, len(dataset.x)),1)\n",
    "\n",
    "indx = np.where((~np.isnan(input[1]).any(axis=0)) & (x>10) & ((x>100) | (y<880))) # input[1] because this variable is down to 100m\n",
    "input =input[:,:,indx[0]]\n",
    "\n",
    "input = np.transpose(input,axes=(0,2,1)) # this is the right shape for preprocessing the data\n",
    "\n",
    "# Transforming each variable individually\n",
    "input[0] = minmax_scale(input[0])\n",
    "input[1] = minmax_scale(input[1])\n",
    "# input[2] = minmax_scale(input[2])\n",
    "# input[3] = minmax_scale(input[3])\n",
    "# input[4] = minmax_scale(input[4])\n",
    "# input[5] = minmax_scale(input[5])\n",
    "# input[6] = minmax_scale(input[6])\n",
    "\n",
    "# Converting it to an appropriate format for functional clustering\n",
    "input = np.transpose(input,axes=(1,2,0)) # this is the right shape for converting it to a functional variable\n",
    "input2 = FDataGrid(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(input2)\n",
    "clusters = kmeans.predict(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
