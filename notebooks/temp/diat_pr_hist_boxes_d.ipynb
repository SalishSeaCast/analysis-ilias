{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diatom production rate with a Histogram-based Gradient Boosting Regression Tree based on the oceanographic boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "import os\n",
    "import lzma\n",
    "import dill\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import cmocean.cm as cm\n",
    "import salishsea_tools.viz_tools as sa_vi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the training - testing datasets\n",
    "def datasets_preparation(dataset, name, inputs_names, regions):\n",
    "\n",
    "    dayofyear = np.tile(np.arange(0,len(dataset.time_counter)//len(np.unique(dataset.time_counter.dt.year))), len(np.unique(dataset.time_counter.dt.year)))\n",
    "\n",
    "    inputs = []\n",
    "    \n",
    "    if 'Day_of_year' in inputs_names:   \n",
    "        for i in inputs_names[0:inputs_names.index('Day_of_year')]:\n",
    "            inputs.append(dataset[i].to_numpy().flatten())  \n",
    "\n",
    "        inputs.append(np.repeat(dayofyear, len(dataset.x)*len(dataset.y)))\n",
    "\n",
    "        for i in inputs_names[inputs_names.index('Day_of_year')+1:]:\n",
    "            inputs.append(dataset[i].to_numpy().flatten()) \n",
    "\n",
    "    else:        \n",
    "        for i in inputs_names:\n",
    "            inputs.append(dataset[i].to_numpy().flatten())\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.ravel(dataset[name])\n",
    "\n",
    "    regions = np.tile(np.ravel(regions), len(np.unique(dataset.time_counter)))\n",
    "\n",
    "    indx = np.where(np.isfinite(targets) & ~np.isnan(regions))\n",
    "    inputs = inputs[:,indx[0]]\n",
    "    targets = targets[indx[0]]\n",
    "\n",
    "    regions = regions[indx[0]]\n",
    "\n",
    "    inputs = inputs.transpose()\n",
    "\n",
    "    return(inputs, targets, indx, regions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preparation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the data arrays\n",
    "def datasets_preparation2(variable, name, units, dataset, indx):\n",
    "\n",
    "    # Obtaining the daily indexes\n",
    "    variable_all = np.full((len(dataset.time_counter) * len(dataset.y) * len(dataset.x)),np.nan)\n",
    "    variable_all[indx[0]] = variable\n",
    "    variable_all = np.reshape(variable_all,(len(dataset.time_counter),len(dataset.y),len(dataset.x)))\n",
    "\n",
    "    # Preparation of the dataarray \n",
    "    array = xr.DataArray(variable_all,\n",
    "        coords = {'time_counter': dataset.time_counter,'y': dataset.y, 'x': dataset.x},\n",
    "        dims = ['time_counter','y','x'],\n",
    "        attrs=dict(description= name,\n",
    "        units=units))\n",
    "        \n",
    "    return (array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_creation(path, variable, name):\n",
    "\n",
    "    temp = variable.to_dataset(name=name)\n",
    "    temp.to_netcdf(path = path + 'targets_predictions.nc', mode='a', encoding={name:{\"zlib\": True, \"complevel\": 9}})\n",
    "    temp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor (inputs, targets, inputs_names, n_bins, i, r_inputs, indexes):\n",
    "\n",
    "    spatial = []\n",
    "    if 'Latitude' in inputs_names:\n",
    "        spatial.append('Latitude')\n",
    "    if 'Longitude' in inputs_names:\n",
    "        spatial.append('Longitude')\n",
    "\n",
    "    if spatial == []:\n",
    "        if 'Day_of_year' not in inputs_names:\n",
    "\n",
    "            model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "            transformers=[('drivers', StandardScaler(), np.arange(0,len(inputs_names)))]),\n",
    "            HistGradientBoostingRegressor()),\n",
    "            transformer=StandardScaler())\n",
    "\n",
    "        else:\n",
    "\n",
    "            model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "                transformers=[('drivers', StandardScaler(), np.arange(0,len(inputs_names)-1))], remainder='passthrough'),\n",
    "                HistGradientBoostingRegressor(categorical_features=[len(inputs_names)-1])),\n",
    "                transformer=StandardScaler())\n",
    "        \n",
    "    else:\n",
    "\n",
    "        model = TransformedTargetRegressor(regressor=make_pipeline(ColumnTransformer(\n",
    "        transformers=[('drivers', StandardScaler(), np.arange(0,inputs_names.index(spatial[0]))),\n",
    "            ('spatial', KBinsDiscretizer(n_bins=n_bins,encode='ordinal',strategy='kmeans'), np.arange(inputs_names.index(spatial[0]),inputs_names.index(spatial[-1])+1))],\n",
    "            remainder='passthrough'),\n",
    "        HistGradientBoostingRegressor(categorical_features=np.arange(inputs_names.index(spatial[0]), len(inputs_names)))),\n",
    "        transformer=StandardScaler())\n",
    "\n",
    "    regr = model.fit(inputs,targets)\n",
    "\n",
    "    r_inputs[i,indexes] = np.round(r_regression(inputs,targets),2)\n",
    "\n",
    "    return(regr, r_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(targets, predictions, name):\n",
    "\n",
    "    # compute slope m and intercept b\n",
    "    m, b = np.polyfit(targets, predictions, deg=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(5,10), layout='constrained')\n",
    "\n",
    "    ax[0].scatter(targets,predictions, alpha = 0.2, s = 10)\n",
    "\n",
    "    lims = [np.min([ax[0].get_xlim(), ax[0].get_ylim()]),\n",
    "        np.max([ax[0].get_xlim(), ax[0].get_ylim()])]\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[0].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[0].set_xlabel('targets')\n",
    "    ax[0].set_ylabel('predictions')\n",
    "    ax[0].set_xlim(lims)\n",
    "    ax[0].set_ylim(lims)\n",
    "    ax[0].set_aspect('equal')\n",
    "\n",
    "    ax[0].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    h = ax[1].hist2d(targets,predictions, bins=100, cmap='jet', \n",
    "        range=[lims,lims], cmin=0.1, norm='log')\n",
    "    \n",
    "    ax[1].plot(lims, lims,linestyle = '--',color = 'k')\n",
    "\n",
    "    # plot fitted y = m*x + b\n",
    "    ax[1].axline(xy1=(0, b), slope=m, color='r')\n",
    "\n",
    "    ax[1].set_xlabel('targets')\n",
    "    ax[1].set_ylabel('predictions')\n",
    "    ax[1].set_aspect('equal')\n",
    "\n",
    "    fig.colorbar(h[3],ax=ax[1], location='bottom')\n",
    "\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_criteria(dates, variable, year_variable, months, period, title):\n",
    "    \n",
    "    indx = pd.DatetimeIndex(dates)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scatter= ax.scatter(dates,variable, marker='.', c=indx.month)\n",
    "    plt.xticks(rotation=70)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=months)\n",
    "    ax.plot(dates[(indx.month == np.unique(indx.month)[1]) & (indx.day == len(np.unique(dates.day)) // 2)], year_variable,color='red',marker='*')\n",
    "    fig.suptitle(title + ' ' + period)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_values(dates, targets, predictions, mean, units, category, region, period, labels):\n",
    "\n",
    "    r = np.round(np.corrcoef(predictions,targets)[0][1],3)\n",
    "    rms = rmse(predictions,targets)  / mean * 100\n",
    "    m,_ = np.polyfit(targets, predictions, deg=1)\n",
    "    slope = np.round(m,3)\n",
    "\n",
    "    temp = pd.DataFrame(np.vstack((r,rms,slope)).transpose(),columns=['r','rms [%]','slope'])\n",
    "    display(temp)\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    mean_targets = np.ma.array(targets)\n",
    "    mean_predictions = np.ma.array(predictions)\n",
    "\n",
    "    for year in years:\n",
    "        mean_targets[(np.where(dates.year==year)[0][-1])] = np.ma.masked\n",
    "        mean_predictions[(np.where(dates.year==year)[0][-1])] = np.ma.masked\n",
    "        \n",
    "    ax.plot(mean_targets, label = 'targets')\n",
    "    ax.plot(mean_predictions, label = 'predictions')\n",
    "\n",
    "    ticks = np.arange(0,len(years)*len(labels),len(labels)/2)\n",
    "    ticks = np.int16(ticks)\n",
    "    labels2=np.tile(labels,len(years))\n",
    "\n",
    "    ax.set_xticks(ticks, labels2[ticks])\n",
    "\n",
    "    ax2 = ax.secondary_xaxis('bottom')\n",
    "    ax2.set_xticks(ticks=np.arange(0,len(years)*len(labels),len(labels)), labels=years)\n",
    "    \n",
    "    ax2.tick_params(length=0, pad=30)\n",
    "\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' ' + period + ' ' + region)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return(r,rms,slope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_maps(targets, predictions, name, units):\n",
    "\n",
    "    fig, ax = plt.subplots(2,2, figsize = (10,15), layout='tight')\n",
    "\n",
    "    cmap = plt.get_cmap('cubehelix')\n",
    "    cmap.set_bad('gray')\n",
    "\n",
    "    targets.plot(ax=ax[0,0], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    predictions.plot(ax=ax[0,1], cmap=cmap, vmin = targets.min(), vmax = targets.max(), cbar_kwargs={'label': name + ' ' + units})\n",
    "    (targets-predictions).plot(ax=ax[1,0], cmap=cmap, cbar_kwargs={'label': name + ' ' + units})\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "        bottom=0.1, \n",
    "        right=0.95, \n",
    "        top=0.95, \n",
    "        wspace=0.35, \n",
    "        hspace=0.35)\n",
    "\n",
    "    sa_vi.set_aspect(ax[0,0])\n",
    "    sa_vi.set_aspect(ax[0,1])\n",
    "    sa_vi.set_aspect(ax[1,0])\n",
    "\n",
    "    ax[0,0].title.set_text('Targets')\n",
    "    ax[0,1].title.set_text('Predictions')\n",
    "    ax[1,0].title.set_text('Targets-Predictions')\n",
    "    ax[1,1].axis('off')\n",
    "\n",
    "    fig.suptitle(name + ' '+ str(targets.time_counter.dt.date.values))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(ax, corn, colour):\n",
    "\n",
    "    ax.plot([corn[2], corn[3], corn[3], corn[2], corn[2]], \n",
    "    [corn[0], corn[0], corn[1], corn[1], corn[0]], '-', color=colour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Mean Peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_mean_peaks(dates,targets_mean,predictions_mean,category,units,region,boxname,labels):\n",
    "\n",
    "    years = np.unique(dates.year)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(19,5))\n",
    "    \n",
    "    targets_mean = np.ma.array(targets_mean)\n",
    "    predictions_mean = np.ma.array(predictions_mean)\n",
    "\n",
    "    for year in years:\n",
    "      \n",
    "        targets_mean[(np.where(dates.year==year)[0][-1])] = np.ma.masked\n",
    "        predictions_mean[(np.where(dates.year==year)[0][-1])] = np.ma.masked\n",
    "\n",
    "    ax.plot(targets_mean, label = 'targets')\n",
    "    ax.plot(predictions_mean, label = 'predictions')\n",
    "\n",
    "    ax.set_xticks(ticks=np.arange(0,len(years)*len(labels),len(labels)//len(years)+1), labels=np.tile(labels[np.arange(0,len(labels),len(labels)//len(years)+1)],len(years)))\n",
    "    \n",
    "    ax2 = ax.secondary_xaxis('bottom')\n",
    "    ax2.set_xticks(ticks=np.arange(0,len(years)*len(labels),len(labels)+1), labels=years)\n",
    "    \n",
    "    ax2.tick_params(length=0, pad=30)\n",
    "\n",
    "    plt.suptitle('Mean '+category + ' ' +units + ' (15 Feb - 30 Apr) ' + region + ' ' + boxname)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (Regional analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_regional(metric,box,years,category):\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    for i in range (0,len(box)):\n",
    "        ax.plot(years,metric[:,i],marker= '*', label=box[i])\n",
    "    plt.suptitle(category+ ' (Regional analysis)')\n",
    "    plt.legend()\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(dates,dataset,targets,predictions,units,category,period,labels,boxname):\n",
    "\n",
    "    targets_mean = np.reshape(targets,(len(dataset.time_counter), len(targets) // len(dataset.time_counter)))\n",
    "    predictions_mean = np.reshape(predictions,(len(dataset.time_counter), len(targets) // len(dataset.time_counter)))\n",
    "\n",
    "    targets_mean = np.mean(targets_mean,axis=1)\n",
    "    predictions_mean = np.mean(predictions_mean,axis=1)\n",
    "\n",
    "    r,rms,slope = plotting_mean_values(dates, targets_mean, predictions_mean, np.mean(targets_mean), units, category, period, boxname, labels)\n",
    "\n",
    "    season = np.array(np.split(targets_mean,len(np.unique(dates.year)),axis=0))\n",
    "    season = np.mean(season, axis=0)\n",
    "\n",
    "    season_train = np.tile(season,len(np.unique(dates.year))) # Broadcasting season to all training years\n",
    "\n",
    "    r_train_season,_,slope_train_season = plotting_mean_values(dates, targets_mean-season_train, predictions_mean-season_train, np.mean(targets_mean),\n",
    "    units, category, period, boxname + ' (removed seasonality)', labels)\n",
    "\n",
    "    return(r, rms, slope, season, r_train_season, slope_train_season)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (dataset, dates, name, units, regr_all, regions0, boxes, unique_inputs_names, indexes):\n",
    "\n",
    "    # For every year\n",
    "    r_years = np.array([])\n",
    "    rms_years = np.array([])\n",
    "    slope_years = np.array([])\n",
    "\n",
    "    inputs, targets, indx, regions  = datasets_preparation(dataset, name, unique_inputs_names, regions0)\n",
    "\n",
    "    predictions = np.full(targets.shape,np.nan) # size of targets without nans\n",
    "\n",
    "    std_targets_test = np.zeros(len(boxes))\n",
    "    std_predictions_test = np.zeros(len(boxes))\n",
    "\n",
    "    for i in range (0, len(boxes)):\n",
    "\n",
    "        indx2 = np.where(regions==i)\n",
    "        inputs2 = inputs[indx2[0],:][:,indexes[i]]\n",
    "        targets2 = targets[indx2[0]]\n",
    "\n",
    "        predictions[indx2[0]] = regr_all[i].predict(inputs2) \n",
    "\n",
    "        std_targets_test[i] = np.nanstd(targets2)\n",
    "        std_predictions_test[i] = np.nanstd(predictions)\n",
    "\n",
    "    targets_temp = np.split(targets,len(np.unique(dates.year)),axis=0)\n",
    "    predictions_temp = np.split(predictions,len(np.unique(dates.year)),axis=0)\n",
    "\n",
    "    for i in range (0, len(np.unique(dates.year))):\n",
    "\n",
    "        # Calculating the annual time-series\n",
    "        m_year = scatter_plot(targets_temp[i], predictions_temp[i], name + ' for ' + str(np.unique(dates.year)[i]))\n",
    "        r_year = np.corrcoef(targets_temp[i], predictions_temp[i])[0][1]\n",
    "        rms_year = rmse(targets_temp[i], predictions_temp[i]) / np.mean(targets_temp[i]) * 100\n",
    "\n",
    "        r_years = np.append(r_years,r_year)\n",
    "        rms_years = np.append(rms_years,rms_year)\n",
    "        slope_years = np.append(slope_years,m_year)\n",
    "\n",
    "    targets_all = datasets_preparation2(targets, name, units, dataset, indx)\n",
    "    predictions_all = datasets_preparation2(predictions, name, units, dataset, indx)\n",
    "\n",
    "    return(r_years, rms_years, slope_years, targets_all, predictions_all, targets, predictions, regions, std_targets_test, std_predictions_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Diatom_Production_Rate'\n",
    "units = '[mmol N m-2 s-1]'\n",
    "category = 'Production rates'\n",
    "\n",
    "filename = '/data/ibougoudis/MOAD/files/inputs/jan_mar.nc'\n",
    "\n",
    "inputs_names = [['Summation_of_solar_radiation', 'Mean_wind_speed', 'Mean_precipitation', 'Latitude', 'Longitude', 'Day_of_year'], \n",
    "    ['Summation_of_solar_radiation', 'Mean_wind_speed', 'Mean_air_temperature', 'Latitude', 'Day_of_year'],\n",
    "    ['Summation_of_solar_radiation', 'Mean_wind_speed', 'Mean_air_temperature', 'Latitude', 'Day_of_year'], \n",
    "    ['Summation_of_solar_radiation', 'Mean_wind_speed', 'Mean_air_temperature', 'Latitude', 'Longitude', 'Day_of_year'],\n",
    "    ['Summation_of_solar_radiation', 'Latitude', 'Longitude'], \n",
    "    ['Summation_of_solar_radiation', 'Mean_precipitation', 'Latitude', 'Longitude', 'Day_of_year'], \n",
    "    ['Summation_of_solar_radiation', 'Latitude', 'Longitude'],\n",
    "    ['Summation_of_solar_radiation', 'Mean_wind_speed', 'Mean_air_temperature', 'Mean_precipitation', 'Latitude', 'Longitude', 'Day_of_year'], \n",
    "    ['Summation_of_solar_radiation', 'Mean_wind_speed', 'Mean_air_temperature', 'Mean_precipitation', 'Latitude', 'Longitude', 'Day_of_year']]\n",
    "\n",
    "unique_inputs_names = []\n",
    "for i in inputs_names:\n",
    "    unique_inputs_names.extend(i)\n",
    "unique_inputs_names = list(set(unique_inputs_names))\n",
    "\n",
    "n_bins = 155\n",
    "\n",
    "if filename[35:42] == 'jan_mar': # 75 days, 1st period\n",
    "    period = '(16 Jan - 31 Mar)'\n",
    "    id = '1'\n",
    "    months = ['January', 'February', 'March']\n",
    "\n",
    "elif filename[35:42] == 'jan_apr': # 120 days, 2nd period\n",
    "    period = '(01 Jan - 30 Apr)'\n",
    "    id = '2'\n",
    "    months = ['January', 'February', 'March', 'April']\n",
    "\n",
    "elif filename[35:42] == 'feb_apr': # 75 days, 3rd period\n",
    "    period = '(15 Feb - 30 Apr)'\n",
    "    id = '3'\n",
    "    months = ['February', 'March', 'April']\n",
    "\n",
    "elif filename[35:42] == 'apr_jun': # 76 days, 4th period\n",
    "    period = '(16 Apr - 30 Jun)'\n",
    "    id = '4'\n",
    "    months = ['April', 'May', 'June']\n",
    "\n",
    "elif filename[35:42] == 'may_sep': # 153 days, 5th period\n",
    "    period = '(01 May - 30 Sep)'\n",
    "    id = '5'\n",
    "    months = ['May', 'June', 'July', 'August', 'September']\n",
    "   \n",
    "ds = xr.open_dataset(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy = xr.open_dataset('/home/sallen/MEOPAR/grid/bathymetry_202108.nc')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 9))\n",
    "mycmap = cm.deep\n",
    "mycmap.set_bad('grey')\n",
    "ax.pcolormesh(ds[name][0], cmap=mycmap)\n",
    "sa_vi.set_aspect(ax)\n",
    "\n",
    "SoG_north = [650, 730, 100, 200]\n",
    "plot_box(ax, SoG_north, 'g')\n",
    "SoG_center = [450, 550, 200, 300]\n",
    "plot_box(ax, SoG_center, 'b')\n",
    "Fraser_plume = [380, 460, 260, 330]\n",
    "plot_box(ax, Fraser_plume, 'm')\n",
    "SoG_south = [320, 380, 280, 350]\n",
    "plot_box(ax, SoG_south, 'k')\n",
    "Haro_Boundary = [290, 350, 210, 280]\n",
    "plot_box(ax, Haro_Boundary, 'm')\n",
    "JdF_west = [250, 425, 25, 125]\n",
    "plot_box(ax, JdF_west, 'c')\n",
    "JdF_east = [200, 290, 150, 260]\n",
    "plot_box(ax, JdF_east, 'w')\n",
    "PS_all = [0, 200, 80, 320]\n",
    "plot_box(ax, PS_all, 'm')\n",
    "PS_main = [20, 150, 200, 280]\n",
    "plot_box(ax, PS_main, 'r')\n",
    "\n",
    "boxnames = ['SoG_north','SoG_center','Fraser_plume','SoG_south', 'Haro_Boundary', 'JdF_west', 'JdF_east', 'PS_all', 'PS_main']\n",
    "fig.legend(boxnames)\n",
    "\n",
    "boxes = [SoG_north,SoG_center,Fraser_plume,SoG_south,Haro_Boundary,JdF_west,JdF_east,PS_all,PS_main]\n",
    "\n",
    "regions0 = np.full((len(ds.y),len(ds.x)),np.nan)\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "    regions0[boxes[i][0]:boxes[i][1], boxes[i][2]:boxes[i][3]] = i\n",
    "\n",
    "regions0 = xr.DataArray(regions0,dims = ['y','x'])\n",
    "\n",
    "# Low resolution\n",
    "\n",
    "# temp = []\n",
    "\n",
    "# for i in boxes:\n",
    "#     temp.append([x//5 for x in i])\n",
    "\n",
    "# boxes = temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low resolution\n",
    "\n",
    "# ds = ds.isel(y=(np.arange(ds.y[0], ds.y[-1], 5)), \n",
    "#     x=(np.arange(ds.x[0], ds.x[-1], 5)))\n",
    "\n",
    "# regions0 = regions0.isel(y=(np.arange(regions0.y[0], regions0.y[-1], 5)), \n",
    "#     x=(np.arange(regions0.x[0], regions0.x[-1], 5)))\n",
    "\n",
    "dataset = ds.sel(time_counter = slice('2007', '2020'))\n",
    "\n",
    "labels = np.unique(dataset.time_counter.dt.strftime('%d %b'))\n",
    "indx_labels = np.argsort(pd.to_datetime(labels, format='%d %b'))\n",
    "labels = labels[indx_labels]\n",
    "\n",
    "indexes = []\n",
    "temp = []\n",
    "for i in inputs_names:\n",
    "    for j in i:\n",
    "        temp.append(unique_inputs_names.index(j))\n",
    "    indexes.append(temp)\n",
    "    temp = []\n",
    "\n",
    "inputs, targets, indx, regions = datasets_preparation(dataset, name, unique_inputs_names, regions0)\n",
    "\n",
    "r_inputs = np.full((len(boxnames), len(unique_inputs_names)), np.nan)\n",
    "regr_all = []\n",
    "predictions = np.full(targets.shape,np.nan) # size of targets without nans\n",
    "\n",
    "for i in range(0, len(boxes)):\n",
    "\n",
    "    indx2 = np.where(regions==i)\n",
    "    inputs2 = inputs[indx2[0]][:,indexes[i]]\n",
    "    targets2 = targets[indx2[0]]\n",
    "    \n",
    "    regr, r_inputs = regressor(inputs2, targets2, inputs_names[i], n_bins, i, r_inputs, indexes[i])\n",
    "\n",
    "    regr_all.append(regr)\n",
    "    \n",
    "    predictions[indx2[0]] = regr.predict(inputs2)\n",
    "\n",
    "print('Metrics between input features and '+name)\n",
    "temp = pd.DataFrame(r_inputs, index=boxnames, columns=unique_inputs_names)\n",
    "display(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "\n",
    "r_train = np.zeros(len(boxes))\n",
    "rms_train = np.zeros(len(boxes))\n",
    "slope_train = np.zeros(len(boxes))\n",
    "\n",
    "season = np.zeros((len(boxes),len(labels)))\n",
    "\n",
    "r_train_season = np.zeros(len(boxes))\n",
    "slope_train_season = np.zeros(len(boxes))\n",
    "\n",
    "peak = np.zeros(len(boxes))\n",
    "std_targets = np.zeros(len(boxes))\n",
    "std_season = np.zeros(len(boxes))\n",
    "std_predictions = np.zeros(len(boxes))\n",
    "\n",
    "for i in range(0, len(boxes)):\n",
    "\n",
    "    indx2 = np.where(regions==i)\n",
    "    targets2 = targets[indx2[0]]\n",
    "    predictions2 = predictions[indx2[0]]\n",
    "\n",
    "    r_train[i], rms_train[i], slope_train[i], season[i], r_train_season[i], slope_train_season[i] = post_processing(\n",
    "        dates,dataset,targets2,predictions2,units,category,period,labels,boxnames[i])\n",
    "\n",
    "    mean = np.mean(targets2)\n",
    "    std_targets[i] = np.std(targets2)\n",
    "    peak[i] = mean + 0*std_targets[i]\n",
    "    \n",
    "    std_season[i] = np.std(season[i])\n",
    "    std_predictions[i] = np.std(predictions2)\n",
    "\n",
    "plt.plot(season.transpose())\n",
    "plt.legend(boxnames)\n",
    "plt.suptitle('Long-term seasonalities (2007-2020)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.sel(time_counter = slice('2021', '2025'))\n",
    "\n",
    "dates = pd.DatetimeIndex(dataset['time_counter'].values)\n",
    "years = np.unique(dataset.time_counter.dt.year)\n",
    "\n",
    "r_years, rms_years, slope_years, targets_all, predictions_all, targets, predictions, regions, std_targets_test, std_predictions_test = evaluation(\n",
    "    dataset, dates, name, units,regr_all, regions0, boxes, unique_inputs_names, indexes)\n",
    "\n",
    "r_days = xr.corr(targets_all,predictions_all, dim=['x','y'])\n",
    "rms_days = xs.rmse(targets_all,predictions_all, dim=['x','y'], skipna=True) / np.mean(targets) * 100\n",
    "slope_days = xs.linslope(targets_all,predictions_all, dim=['x','y'], skipna=True)\n",
    "\n",
    "plotting_criteria(dates, r_days, r_years, months, period, 'Correlation Coefficients')\n",
    "plotting_criteria(dates, rms_days, rms_years, months, period, 'Root Mean Square Errors')\n",
    "plotting_criteria(dates, slope_days, slope_years, months, period, 'Slopes of the best fitting line')\n",
    "\n",
    "# Daily maps\n",
    "\n",
    "maps = random.sample(sorted(np.arange(0,len(targets_all.time_counter))),10)\n",
    "for i in maps:\n",
    "\n",
    "    idx = np.isfinite(np.ravel(targets_all[i]))\n",
    "    scatter_plot(np.ravel(targets_all[i])[idx], np.ravel(predictions_all[i])[idx], name + ' '+ str(targets_all[i].time_counter.dt.date.values))\n",
    "\n",
    "    plotting_maps(targets_all[i], predictions_all[i], name, units)\n",
    "\n",
    "season_test = np.tile(season,len(np.unique(dates.year))) # Broadcasting season to all testing years\n",
    "\n",
    "peak = np.reshape(np.tile(peak, len(np.unique(dates.year))), (len(boxes),len(np.unique(dates.year))), order='F') # Broadcasting peak to all testing years\n",
    "peak = xr.DataArray(peak, coords = {'box': boxnames, 'year': years}, dims = ['box','year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(std_targets)\n",
    "plt.plot(std_season)\n",
    "plt.plot(std_targets_test)\n",
    "plt.plot(std_targets-std_season)\n",
    "plt.plot(std_targets_test-std_season)\n",
    "\n",
    "plt.plot(std_predictions)\n",
    "plt.plot(std_predictions_test)\n",
    "plt.plot(std_predictions-std_season)\n",
    "plt.plot(std_predictions_test-std_season)\n",
    "\n",
    "plt.xticks(ticks=np.arange(0,len(boxes)), labels=boxnames)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(('targets','season','targets_test','targets-season','targets_test-season','predictions','predictions_test','predictions-season','predictions_test-season'))\n",
    "plt.suptitle('Standard Deviations for training-seasonanlity-testing')\n",
    "plt.show()\n",
    "\n",
    "plt.plot((std_targets_test-std_season)*100/std_targets_test)\n",
    "plt.xticks(ticks=np.arange(0,len(boxes)), labels=boxnames)\n",
    "plt.xticks(rotation=45)\n",
    "plt.suptitle('Percentage of difference between testing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test, rms_test, slope_test = np.zeros(len(boxes)),  np.zeros(len(boxes)), np.zeros(len(boxes))\n",
    "\n",
    "r_test_season, slope_test_season = np.zeros(len(boxes)), np.zeros(len(boxes))\n",
    "\n",
    "targets_sum, predictions_sum  = np.zeros((len(boxes),len(np.unique(dates.year)))), np.zeros((len(boxes),len(np.unique(dates.year))))\n",
    "\n",
    "targets_mean, predictions_mean = np.zeros((len(boxes),len(np.unique(dates.year)))), np.zeros((len(boxes),len(np.unique(dates.year))))\n",
    "\n",
    "targets_diff, predictions_diff = np.zeros((len(boxes),len(season[0])*len(np.unique(dates.year)))), np.zeros((len(boxes),len(season[0])*len(np.unique(dates.year))))\n",
    "\n",
    "rss = np.zeros(len(boxes))\n",
    "\n",
    "for i in range (0, len(boxes)):\n",
    "\n",
    "    targets = targets_all.where(regions0==i).mean(['y','x'])\n",
    "    predictions = predictions_all.where(regions0==i).mean(['y','x'])\n",
    "\n",
    "    r_test[i] = xr.corr(targets,predictions)\n",
    "    rms_test[i] = xs.rmse(targets,predictions,skipna=True) / np.mean(targets) * 100\n",
    "    slope_test[i] = xs.linslope(targets,predictions,skipna=True)\n",
    "\n",
    "    targets_sum[i] = (targets-season_test[i]).groupby(targets.time_counter.dt.year).sum().values\n",
    "    predictions_sum[i] =  (predictions-season_test[i]).groupby(predictions.time_counter.dt.year).sum().values\n",
    "\n",
    "    targets_mean[i] = (targets-season_test[i]).groupby(targets.time_counter.dt.year).mean().values\n",
    "    predictions_mean[i] =  (predictions-season_test[i]).groupby(predictions.time_counter.dt.year).mean().values\n",
    "\n",
    "    rss[i] = ((targets-predictions)**2).sum().values # Similar to rms, is not affected by the seasonality\n",
    "\n",
    "    r_test_season[i], _, slope_test_season[i] = plotting_mean_values(dates, targets-season_test[i], predictions-season_test[i], np.mean(targets),\n",
    "        units, category, boxnames[i] +' (removed seasonality)', period, labels)\n",
    "    \n",
    "    targets_diff[i] = (targets).groupby(targets.time_counter.dt.year).where((targets).groupby(targets.time_counter.dt.year)>peak[i])\n",
    "    predictions_diff[i] = (predictions).groupby(predictions.time_counter.dt.year).where((predictions).groupby(predictions.time_counter.dt.year)>peak[i])\n",
    "\n",
    "    # plotting_mean_peaks(dates,targets_diff[i],predictions_diff[i],category,units,'Peaks',boxnames[i],labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/data/ibougoudis/MOAD/files/results/' + name + '/single_runs/' + name[0:4].lower() + '_pr_hist' + id + '_boxes_4/'\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# with lzma.open(path + 'regr_all.xz', 'wb') as f:   \n",
    "#     dill.dump(regr, f)\n",
    "\n",
    "# with open(path + 'r_inputs.pkl', 'wb') as f:\n",
    "#     dill.dump(r_inputs, f)\n",
    "\n",
    "# with open(path + 'train_metrics.pkl', 'wb') as f:\n",
    "#     dill.dump([r_train,rms_train,slope_train,r_train_season,slope_train_season,season.transpose()], f)\n",
    "\n",
    "# with open(path + 'test_metrics.pkl', 'wb') as f:\n",
    "#     dill.dump([r_test,rms_test,slope_test,r_test_season,slope_test_season,targets_sum,predictions_sum,targets_mean,predictions_mean,targets_diff,predictions_diff,rss], f)\n",
    "\n",
    "# file_creation(path, targets_all, 'Targets')\n",
    "# file_creation(path, predictions_all, 'Predictions')\n",
    "# file_creation(path, (targets_all-predictions_all), 'Targets - Predictions')\n",
    "\n",
    "# with open(path + 'readme.txt', 'w') as f:\n",
    "#     f.write ('name: ' + name)\n",
    "#     f.write('\\n')\n",
    "#     f.write('period: ' + filename[35:42])\n",
    "#     f.write ('\\n')\n",
    "#     f.write ('input_features: ')\n",
    "#     f.write (str([i for i in inputs_names]))\n",
    "#     f.write ('\\n')\n",
    "#     f.write('n_bins: ' + str(n_bins))\n",
    "#     f.write ('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-ilias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
